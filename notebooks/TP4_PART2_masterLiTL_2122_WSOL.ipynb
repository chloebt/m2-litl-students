{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP4_PART2_masterLiTL_2122_WSOL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCHhtzOXQ2po"
      },
      "source": [
        "# TP 4: Training a Feedforward neural network (Part 2)\n",
        "Master LiTL - 2021-2022\n",
        "\n",
        "## Requirements\n",
        "In this part, we will use continuous representations of words, namely Continuous Bag of Words that is randomly initialized embeddings, and then pretrained embeddings.\n",
        "In order to create the representation of a document, we will take all the embeddings of the words that appear in the document, and sum them together (or take their average).\n",
        "So instead of having an input vector of size 5000, we now have an input vector of size e.g. 50, that represents the ‘average’, combined meaning of all the words in the document taken together. \n",
        "\n",
        "Crucially,  the  neural  network  will  also  learn  the  embeddings  during  training :  the  embeddings  of  the network are also parameters that are optimized according to the loss function.\n",
        "\n",
        "The dataset remains the French set of reviews labeled with sentiment.\n",
        "\n",
        "We will compare our model to the scores obtained previously with bag of word representations.\n",
        "\n",
        "Once you've read and understood the code below to read the data, you can run your model and make some experiments, varying the hyper-parameters: \n",
        "* size of the embeddings, \n",
        "* size of the hidden layer, \n",
        "* activation function, \n",
        "* number of iterations.\n",
        "\n",
        "Try to save your results and print some vizualizations: \n",
        "+ Plot the cost function during training with different values for the learning rate\n",
        "+ Plot the accuracy wrt the size of the embedding layer\n",
        "+ Plot the accuracy wrt to the number of training examples "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nT7n30VpCOzF",
        "outputId": "b3c350f1-b1e2-46af-9481-ee44a546ea9e"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If you’re using Colab, allocate a GPU by going to Edit > Notebook Settings.\n",
        "# We move our tensor to the GPU if available\n",
        "if torch.cuda.is_available():\n",
        "  print(f\"GPU ok\")\n",
        "else:\n",
        "  print(\"no gpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdSyhJqpVczO"
      },
      "source": [
        "\n",
        "## 1. Read the data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "train_path = \"allocine_train.tsv\"\n",
        "dev_path = \"allocine_dev.tsv\"\n",
        "test_path = \"allocine_test.tsv\"\n",
        "\n",
        "# Load train set\n",
        "train_df = pd.read_csv(train_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "train_iter = []\n",
        "for i in train_df.index:\n",
        "    #print( train_df[\"sentiment\"][i], train_df[\"review\"][i])\n",
        "    train_iter.append( tuple( [train_df[\"sentiment\"][i], train_df[\"review\"][i]] ) )\n",
        "\n",
        "print( '\\n'.join( [ str(train_iter[i][0])+'\\t'+train_iter[i][1] for i in range(0,10) ] ) )\n",
        "\n",
        "\n",
        "dev_df = pd.read_csv(dev_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "test_df = pd.read_csv(test_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "dev_iter, test_iter = [], []\n",
        "for i in dev_df.index:\n",
        "    dev_iter.append( tuple( [dev_df[\"sentiment\"][i], dev_df[\"review\"][i]] ) )\n",
        "\n",
        "for i in test_df.index:\n",
        "    test_iter.append( tuple( [test_df[\"sentiment\"][i], test_df[\"review\"][i]] ) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNpGiuaRxlvA",
        "outputId": "4a7aaf98-c32d-4ed8-e28d-f98beb0761c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tStephen King doit bien ricaner en constatant cette navrante histoire de disparus, les scénaristes semblent s'être inspirés de ses oeuvres mais ont bien moins son talent que celui du business. Quel perte de temps que de regarder ces personnages perdus au centre d'une histoire sans fin et sans intérêt, où 2 ou 3 épisodes suffisent pour décrocher, à l'inverse d'une série comme Desperate housewives dont les dialogues, les scénarii et les personnages contribuent sans cesse à relancer l'intérêt et le plaisir au fil des épisodes. Pourtant mes goûts initiaux m'auraient porté davantage du côté de la série fantastique. Il ne faut préjuger de rien! A bon entendeur...\n",
            "1\tExcellentissime! Une série à l'apparence toute calme et lisse, qui se révèle être un véritable noeud de problèmes, de secrets, de mensonges... Les actrices sont vraiment toutes très bonnes dans leurs rôles, avec une petite préférence pour Bree, qui pète complètement un câble à la fin de la saison 2!\n",
            "0\tVoir de pareilles évaluations me conforte dans l'idée de ne pas expliquer comment utiliser internet a ma grand mère !! Les francais devrait arreter de tenter de faire des séries TV !!!\n",
            "0\tQuand je pense qu'il ya des séries géniales qui sont arrètées car fautes d'audiences (séries us bien sur). Et la on maintien ce truc pour une quatrième saison... D'ailleurs la preuve que la série est nulle: a-t-elle rencontré le succès aux US?\n",
            "0\tCette série est nul tout les médecins couche entre eux. Il n'y a que du tout le monde couche avec tout le monde et rien d'autre. Ce n'est pas la réalité et c'est ennuyeux. On c'est meme pas qui couche avec qui.\n",
            "0\tLes Feux de L'Amour est un chef d'oeuvre à côté tant par les dialogues que par les scénarios. Attention, tu fumes une et une seule cigarette à 17 ans, tu vas pourrir en enfer. Tu bois un seul verre d'alcool à 21 ans, tu es un alcoolique. Dans Les Feux de l'Amour au moins les personnages ne sont pas lisses. Ici, tout le monde est beau, tout le monde est gentil. Si vous avez des tendances suicidaires, ne regardez pas la série! Vous risquez de passer à l'acte.\n",
            "0\tUne des pires séries que j'ai eu l'occasion de voir, les scénarii sont d'un plat, tellement prévisibles que c'en est affligeant, les dialogues sont surréalistes, de même que les actions, à 16 ans toutes les filles tombent enceintes, boivent, couchent avec tous le monde sans s'en souvenir le lendemain, sans parler des garçons, je ne sais pas combien de fois on nous répéte dans la série à quel point Lucas est \"le bon garçon\", il couche avec les filles sans protection et certaines fois sans les connaître, boit et se tatoue... humm quel excellent exemple vraiment, la série de l'année.\n",
            "1\tImpressionant tant cette série est captivante.Lost c'est l'art de faire monter l'adrénaline d'un coup.De plus les acteurs sont vraiments bons.Parfois on rigole aussi.Bref une série incroyable qui tient en haleine !\n",
            "1\tCette serie est une drogue, apres avoir vu un ou deux épisodes comme ça sans trop d'interêt on veut absolument connaitre la suite afin de savoir pourquoi Marie Alice est morte. Chaque épisode est incroyablement bien pensé et différent, tout les acteurs sont parfait dans leurs roles. Attention de ne pas manquer d'épisodes sous peine d'etre perdu^^ Je tire enfin mon chapeau a Danny Elfman qui nous offre encore un superbe thème musical dont on ne se lasse pas de l'écouter et de le réécouter.\n",
            "0\tOriginale ? Non. L'histoire : des naufragés jouent a Ko Lantha sur une ile ou il se passe des choses etranges. C'est filmé comme du Alias : sans style (je regrette homicide ou Oz qui étaient des series originale autant par les scénars, le traitement et la manière de filmer.). Les personnages sont nuls : Le docteur charismatique et heroique, La tête de mule au grand coeur, La fille gentille mais qui est poursuivie par la justice etc... David Lynch je t'en prie, reviens !!!!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time, we don't directly use the bag of word representation built by scikit.\n",
        "\n",
        "We need to tokenize our data, and build the corresponding vocabulary (on the train set)."
      ],
      "metadata": {
        "id": "xlXairmKu8Xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# splits the string sentence by space.\n",
        "tokenizer = get_tokenizer( None ) \n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "ZBctRAcZsQSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vocabulary\n",
        "\n",
        "Here the vocabulary is a specific object in Pytorch, check the existing functions to use it here: https://pytorch.org/text/stable/vocab.html\n",
        "\n",
        "For example, the vocabulary directly converts a list of tokens into integers."
      ],
      "metadata": {
        "id": "Tus9Kedas5dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab(['Avant', 'cette', 'série', ','])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tb6TYA9Is5v6",
        "outputId": "8bfc234c-b9fe-411a-8e27-2fde43bb348a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2910, 18, 7, 144]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶▶ **Now, try to retrieve the indice of the word 'mauvais'.** "
      ],
      "metadata": {
        "id": "3aAwvzFavjIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print( vocab.lookup_indices( ['mauvais'] ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9cKqyj3vjT8",
        "outputId": "5987d40b-c666-4500-ebfc-b241d773fb8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[246]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Text and label pipelines\n",
        "\n",
        "The text pipeline converts a text string into a list of integers based on the lookup table defined in the vocabulary. \n",
        "\n",
        "The label pipeline converts the label into integers. "
      ],
      "metadata": {
        "id": "_4zDQHrstB12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) #simple mapping to self"
      ],
      "metadata": {
        "id": "eUNfcXNUtCBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_pipeline('Avant cette série, je ne connaissais que Urgence')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArT6VMS23I0i",
        "outputId": "fee42615-76d8-40c1-8602-ecdad1645b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2910, 18, 89, 16, 17, 6120, 8, 10529]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_pipeline('0')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LySlUUzY3FEE",
        "outputId": "fa9a026e-db06-43a3-d7e5-00884ac5bc3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate data batch and iterator\n",
        "\n",
        "Here we also use *torch.utils.data.DataLoader* with an iterable dataset, here a simple list of labels and text reviews, as saved in *train_iter*.\n",
        "\n",
        "Before sending to the model, we apply a function, *collate_fn*, to our input data:\n",
        "* The input to *collate_fn* is a batch of data with the batch size in *DataLoader*, \n",
        "* *collate_fn* processes them according to the data processing pipelines declared previously. \n",
        "\n",
        "In 'collate_batch', we define how we want to pre-process our data.\n",
        "\n",
        "The function is directly called within *DataLoader*:\n",
        "```\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
        "```\n",
        "\n",
        "Below: \n",
        "* the text entries in the original data batch input are packed into a list and concatenated as a single tensor. \n",
        "* the offset is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor\n",
        "* Label is a tensor saving the labels of individual text entries."
      ],
      "metadata": {
        "id": "Nj21QrBotJrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label.to(device), text_list.to(device), offsets.to(device)\n",
        "\n",
        "#train_iter = train_list \n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "GE82N6mmuxYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define the model\n",
        "\n",
        "### Bag of embeddings\n",
        "\n",
        "The model is composed of the nn.EmbeddingBag layer: https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html\n",
        "\n",
        "* mode (string, optional) – \"sum\", \"mean\" or \"max\". Default=mean.\n",
        "\n",
        "### Weight initialization\n",
        "\n",
        "Weight initialization is done by default uniformly. You can also specify the initialization as done below, and choose among varied options: https://pytorch.org/docs/stable/nn.init.html. Further info there https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch or there https://discuss.pytorch.org/t/clarity-on-default-initialization-in-pytorch/84696/3"
      ],
      "metadata": {
        "id": "B6jAI7orupfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
        "        # just always do it in an nn.Module\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "\n",
        "        # Define the parameters that you will need. \n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True) # <----\n",
        "\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "\n",
        "        # Non-linearity\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "\n",
        "        print( 'embed_dim', embed_dim, 'hidden_dim', hidden_dim)\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc2.bias.data.fill_(0.01)\n",
        "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc2.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "\n",
        "        embedded = self.embedding(text, offsets)\n",
        "\n",
        "        # Linear function  # LINEAR\n",
        "        out = self.fc1(embedded)\n",
        "\n",
        "        # Non-linearity  # NON-LINEAR\n",
        "        out = self.sigmoid(out) \n",
        "\n",
        "        # Linear function (readout)  # LINEAR\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "2GxkAeRQuqfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and evaluation functions\n",
        "\n",
        "Other functions for training and evaluating, but roughly doing the same as previously."
      ],
      "metadata": {
        "id": "Bq7EqLhGy_es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predicted_label = model(text, offsets)\n",
        "        loss = criterion(predicted_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predicted_label = model(text, offsets)\n",
        "            loss = criterion(predicted_label, label)\n",
        "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "metadata": {
        "id": "4WgmkNZWurC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3. Running an experiment \n",
        "\n",
        "### Adjusting the learning rate\n",
        "\n",
        "The code below uses a *scheduler*: *torch.optim.lr_scheduler* provides several methods to adjust the learning rate based on the number of epochs.\n",
        "\n",
        "Learning rate scheduling should be applied after optimizer’s update.\n",
        "\n",
        "* torch.optim.lr_scheduler.StepLR: Decays the learning rate of each parameter group by gamma every step_size epochs.\n",
        "\n",
        "https://pytorch.org/docs/stable/optim.html"
      ],
      "metadata": {
        "id": "_6zahomkrWMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emb_dim = 3000\n",
        "hid_dim = 64\n",
        "model = FeedforwardNeuralNetModel(vocab_size, emb_dim, hid_dim, num_class).to(device)"
      ],
      "metadata": {
        "id": "A7le5MIHuq7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "561cc327-54b5-4ea2-aa32-e794d776d32a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embed_dim 3000 hidden_dim 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1) # <----\n",
        "total_accu = None\n",
        "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(dev_iter, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_iter, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val: # <----\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Oo69gIdv4Sr",
        "outputId": "0512f243-6ddb-49b5-9115-fd7107d70839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time:  0.52s | valid accuracy    0.628 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time:  0.49s | valid accuracy    0.665 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time:  0.48s | valid accuracy    0.789 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time:  0.48s | valid accuracy    0.794 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time:  0.47s | valid accuracy    0.738 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time:  0.50s | valid accuracy    0.809 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time:  0.48s | valid accuracy    0.796 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time:  0.46s | valid accuracy    0.809 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time:  0.47s | valid accuracy    0.801 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time:  0.49s | valid accuracy    0.803 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZq7hmXOv4t-",
        "outputId": "917f7211-bef0-4ed8-fd66-238434e37724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Using pretrained embeddings\n",
        "\n",
        "Upload the file *cc.fr.300.10000.vec': the first 10000 lines of the FastText embeddings for French, https://fasttext.cc/docs/en/crawl-vectors.html."
      ],
      "metadata": {
        "id": "UDlM7OZq56HO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the vectors\n",
        "\n",
        "▶▶ **Write a function that load the vectors**, i.e. a dictionary mapping a word to its vector, as defined in the fasttext file. \n",
        "\n",
        "▶▶ **Print the vocabulary and the size of the embeddings.**"
      ],
      "metadata": {
        "id": "RX2DkAqws1gU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "\n",
        "def load_vectors(fname):\n",
        "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    n, d = map(int, fin.readline().split())\n",
        "    print(n,d) #here in fact only 10000 words\n",
        "    data = {}\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split(' ')\n",
        "        data[tokens[0]] = [float(t) for t in tokens[1:]]\n",
        "    return data\n",
        "\n",
        "embed_file='cc.fr.300.10000.vec'\n",
        "vectors = load_vectors( embed_file )\n",
        "print(vectors.keys() )\n",
        "print( vectors['de'] )"
      ],
      "metadata": {
        "id": "yd2EEjECv4vk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c030ba12-fe87-4c60-dc78-6088eca9e33b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000000 300\n",
            "dict_keys([',', 'de', '.', '</s>', 'la', 'et', ':', 'à', 'le', '\"', 'en', '’', 'les', 'des', ')', '(', 'du', 'est', 'un', \"l'\", \"d'\", 'une', 'pour', '/', '|', 'dans', 'sur', 'que', 'par', 'au', 'a', 'l', 'qui', '-', 'd', 'il', 'pas', '!', 'avec', '_', 'plus', \"'\", 'Le', 'ce', 'ou', 'La', 'ne', 'se', '»', '...', '?', 'vous', 'sont', 'son', '«', 'je', 'Les', 'Il', 'aux', '1', ';', 'mais', \"qu'\", 'on', \"n'\", 'comme', '2', 'sa', 'cette', 'y', 'nous', 'été', 'tout', 'fait', 'En', \"s'\", 'bien', 'ses', 'très', 'ont', 's', 'être', 'votre', 'ai', 'elle', 'n', '3', 'même', \"L'\", 'deux', 'faire', \"c'\", 'aussi', '>', 'leur', '%', 'si', 'entre', 'qu', '€', '&', '4', 'sans', 'Je', \"j'\", 'était', '10', 'autres', 'tous', 'peut', 'France', 'ces', '…', '5', 'lui', 'me', ']', '[', 'où', 'ans', '6', '#', 'après', '+', 'ils', 'dont', 'Pour', '°', '–', 'temps', '*', 'sous', 'Un', 'avoir', 'L', 'A', '}', 'site', 'peu', 'mon', 'encore', '12', 'depuis', '0', 'ça', 'fois', '2017', 'ainsi', 'alors', 'donc', 'notre', 'Ce', '20', '11', 'autre', 'monde', 'non', 'Paris', 'avant', 'Une', 'Elle', '15', 'également', 'Re', 'contre', 'Vous', 'c', 'moins', 'tu', 'suis', '7', 'ville', 'avait', 'vos', 'vers', 'premier', 'vie', 'Et', '2016', '2014', 'jour', '00', '2013', 'leurs', 'Dans', 'soit', '2012', 'toutes', 'nom', '2015', '14', 'De', 'On', '8', 'prix', '18', \"C'\", 'Mais', 'partie', '•', 'nos', 'voir', 'article', '16', 'Plus', '13', 'of', 'chez', 'inscription', 'première', 'quelques', 'toujours', '17', 'Nous', 'plusieurs', 'mai', 'place', 'français', '2011', 'cas', 'puis', 'Cette', 'année', 'ma', 'toute', '2010', 'the', '30', 'suite', 'pays', 'The', 'années', 'lors', 'fin', 'bon', '19', 'À', '21', 'dit', 'trois', 'grand', 'quand', 'partir', 'car', 'sera', '22', 'cet', 'jours', 'C', '2009', 'petit', '=', \"J'\", 'Si', 'maison', 'fut', 'ligne', 'faut', '9', 'nouveau', 'moi', 'lieu', 'mois', '23', 'cours', 'personnes', 'va', 'déjà', 'cela', '2008', 'beaucoup', 'juin', 'groupe', 'mars', 'travail', 'nouvelle', 'compte', '24', 'page', 'messages', '25', 'and', 'janvier', 'hui', 'film', 'commune', 'j', 'grande', 'ici', 'Au', 'avril', \"m'\", 'histoire', '2007', 'détail', 'famille', 'savoir', 'doit', 'avis', 'chaque', 'trop', 'enfants', 'eau', 'm', 'part', \"jusqu'\", 'septembre', 'mes', 'homme', 'rien', 'avons', 'octobre', 'décembre', 'forum', 'jeu', 'produits', 'trouve', 'juillet', 'produit', 'équipe', 'CEST', 'politique', 'là', 'novembre', 'permet', 'in', 'titre', 'pendant', 'notamment', 'recherche', 'nombre', '·', 'dire', 'http', 'service', 'pouvez', 'février', 'point', 'dernier', '05', 'moment', 'selon', 'mort', 'droit', '2006', 'DE', 'afin', 'jamais', 'effet', 'mise', 'Des', '—', '26', 'région', 'projet', '\\\\', 'saison', 'août', 'niveau', '28', 'reste', 'bonne', 'ensemble', '27', 'peuvent', 'exemple', 'Voir', '01', 'série', 'souvent', 'centre', 'Après', 'écrit', 'pouvoir', '--', 'mettre', 'km', 'général', 'Page', 'forme', 'début', '09', 'ceux', 'personne', 'eu', 'française', 'vraiment', 'services', 'demande', '29', 'question', 'Par', 'près', 'Merci', 'celui', 'qualité', 'vue', 'tant', 'petite', 'système', '©', 'Ils', 'ailleurs', 'Europe', 'avez', 'mieux', 'société', '^', 'informations', 'données', 'prendre', 'elles', 'guerre', 'surtout', 'to', 'Jean', 'né', 'CET', '08', 'certains', '06', 'village', 'membres', 'rapport', 'an', 'face', 'étaient', 'mot', 'femme', 'possible', '50', 'seul', '@', 'Prix', '04', 'rue', '07', 'te', 'celle', 'mal', 'articles', 'aide', 'nombreux', 'base', 'ayant', '<', '03', '2005', 'entreprise', 'Catégorie', '..', 'ni', 'liste', '02', 'livre', 'passe', 'https', 'mis', 'seulement', 'côté', 'public', 'utilisation', 'ton', 'développement', '31', 'vu', '100', 'D', 'chose', 'dès', 'quatre', 'situé', 'Ces', 'devant', 'photos', 'hommes', 'trouver', 'Son', 'image', '\\xad', 'fr', 'plan', 'étant', 'type', 'tour', '$', 'grâce', 'cadre', 'juste', 'musique', 'président', 'version', 'aime', 'points', 'simple', 'Avec', 'formation', 'jeune', 'assez', 'quoi', 'offre', 'origine', 'sens', 'serait', 'gratuit', 'Pierre', 'heures', 'Nombre', 'corps', 'salle', 'tête', 'sujet', 'adresse', 'carte', 'minutes', 'date', 'font', 'fils', 'création', 'donne', 'e', 'choix', 'album', 'dernière', 'agit', 'loi', 'passé', 'propre', 'coup', 'propose', 'environ', 'chambre', 'accès', 'devient', '....', \"D'\", 'semaine', 'sécurité', 'parce', 'vidéo', 'ensuite', 'porte', 'h', 'lien', 'haut', 'comment', 'femmes', 'façon', 'nationale', 'état', 'présente', 'long', 'nouvelles', 'tard', 'besoin', 'raison', 'club', 'gouvernement', 'retour', 'genre', 'problème', 'x', 'ancien', 'époque', 'séjour', 'Sur', 'Forum', 'passer', 'information', '40', 'auteur', 'belle', '�', 'autour', 'eux', 'rôle', 'bois', '2004', 'meilleur', 'jeux', 'marché', 'deuxième', 'population', 'État', 'manière', 'santé', 'photo', 'J', 'particulier', 'semble', 'pense', 'merci', 'proche', 'N', 'air', 'Tous', 'aurait', 'fonction', 'Tout', 'différents', 'Mar', 'entreprises', 'statistiques', 'plutôt', 'nuit', 'accueil', 'située', 'ordre', 'aller', '--Les', 'êtes', 'école', 'père', 'droits', 'as', 'petits', 'utiliser', 'édition', \"aujourd'\", 'occasion', 'maintenant', 'États-Unis', 'période', 'Grand', 'Saint', 'donner', 'fille', 'Lire', 'jeunes', 'millions', 'activités', 'sommes', 'aucun', 'enfant', 'seule', 'production', '000', 'autant', 'M.', 'II', 'anglais', 'hôtel', 'œuvre', 'habitants', 'espace', '“', 'art', 'nouveaux', 'Ajouter', 'réseau', 'gestion', 'modèle', 'but', 'prend', '2000', 'parfois', 'I', 'département', 'national', 'marque', 'New', 'veut', 'activité', 'quelque', 'église', 'avais', 'propos', '”', 'gauche', 'cause', 'texte', 'idée', 'pris', 'nombreuses', 'chef', 'existe', 'mots', 'main', 'scène', 'grands', 'route', 'gens', 'style', 'sites', 'durant', 'programme', 'pu', 'études', 'mesure', 'calme', 'Comment', 'conditions', 'ministre', 'seront', 'terme', 'laquelle', 'vient', 'mode', 'or', 'Comme', 'jardin', 'www.insee.fr', 'situation', 'travaux', 'vacances', 'journée', 'vrai', 'membre', 'plein', 'code', 'sein', 'web', 'rencontre', 'lire', 'mer', 'Du', 'numéro', 'pages', 'action', 'euros', 'Mai', 'loin', 'lorsque', 'sais', 'agréable', 'domaine', '2003', 'pourrait', 'nature', 'travers', 'Conseil', 'disponible', 'expérience', 'fond', 'François', 'roi', 'siècle', 'oui', 'sud', 'etc.', 'choses', 'heure', 'LA', 'Accueil', 'milieu', 'cuisine', 'pratique', 'terre', 'grandes', 'blog', 'américain', '~', 'questions', 'vente', 'construction', 'pourquoi', 'peux', 'différentes', 'toi', 'répondre', 'jusqu', 'Mon', 'emploi', 'abord', 'sortie', 'intérieur', 'droite', 'bas', 'cinq', 'Louis', 'aucune', 'plaisir', 'premiers', 'message', 'pièces', 'suivant', 'donné', 'enfin', 'proximité', 'logement', 'Alors', 'prise', 'voiture', 'objet', 'Nord', 'accord', 'section', 'âge', 'gros', 'nord', 'découvrir', 'technique', 'présent', 'République', 'soir', 'Depuis', 'créer', 'S', 'concernant', 'jouer', 'Paul', 'important', '2002', '1er', 'succès', 'appartement', 'Jeu', 'chambres', 'met', 'campagne', 'discuter', 'peut-être', 'territoire', 'Bonjour', 'certaines', 'argent', 'langue', 'rapide', 'parmi', 'geo', 'Internet', 'John', 'vais', 'Charles', 'résultats', 'Dieu', 'direction', 'moyen', '²', 'Français', 'Canada', 'couleur', 'Jeux', 'rendre', 'poste', 'fort', 'Sa', 'auprès', 'départ', 'armée', 'Michel', 'Centre', 'entrée', 'valeur', '2001', 'avaient', 'charge', 'zone', 'min', 'cœur', 'mère', 'match', 'taille', 'Allemagne', 'amour', 'noir', \"t'\", 'Sud', 'clients', 'aura', 'naissance', 'annonce', 'quartier', 'Québec', 'économique', 'frais', 'Afrique', 'mm', 'voyage', 'Pas', 'Selon', 'réponse', 'pied', 'Maison', 'international', 'culture', 'troisième', 'Mer', 'beau', 'connu', 'affaires', 'blanc', 'voix', 'doivent', 'directement', 'plupart', 'rouge', 'compris', 'amis', 'conseil', 'classe', 'Université', 'sujets', 't', 'Jacques', 'presse', 'protection', 'parti', 'arrivée', '35', 'rapidement', 'obtenir', 'application', 'parler', 'p.', 'association', 'doute', 'Sujets', 'mondiale', 'château', 'communauté', 'appel', 'images', 'panier', 'lequel', 'projets', 'étude', 'football', '60', 'générale', 'vite', 'libre', 'commentaire', 'arrive', 'Cet', 'ta', 'matière', 'aider', 'contrôle', 'risque', 'cm', 'commande', 'trouvé', '45', 'quel', 'unique', 'politiques', 'voit', 'Quand', 'intérêt', 'source', 'communes', 'contenu', 'internet', '1999', 'organisation', 'Date', 'utilisé', 'Robert', 'secteur', 'for', 'présence', 'B', 'mouvement', 'référence', 'is', 'villes', 'double', 'catégorie', 'techniques', 'force', 'lettres', 'ancienne', 'simplement', 'yeux', 'éléments', 'île', 'carrière', 'Coupe', 'vont', 'joueur', 'livres', 'passage', 'ET', 'historique', 'commence', 'petites', 'Italie', 'Cela', 'presque', 'Sam', 'sociale', 'parle', '32', 'moyenne', 'épisode', 'réalisé', 'particulièrement', 'problèmes', 'environnement', 'terrain', 'taux', 'films', 'tel', 'roman', 'David', 'chacun', '80', 'Bien', 'téléphone', 'pièce', 'Messages', 'actuellement', 'Tu', 'divers', 'super', 'dernières', 'Recherche', 'Histoire', 'similaires', 'second', 'couleurs', 'publié', 'parc', 'esprit', 'Votre', '33', 'derniers', 'énergie', 'publique', 'créé', 'cinéma', 'Union', 'lit', 'moteur', 'seconde', 'York', 'aujourd', 'disponibles', 'Philippe', 'sûr', 'US', 'Posté', 'TV', 'es', 'Non', 'facile', 'social', 'large', 'Google', 'siège', 'Lun', 'longtemps', 'communication', 'nécessaire', 'bord', 'Site', 'Ainsi', 'permis', 'liens', 'matin', 'directeur', 'mètres', 'Belgique', 'durée', 'vivre', 'Oui', 'Dim', 'table', 'Que', 'principal', 'solution', 'joue', 'devrait', 'idées', 'suivre', 'dimanche', 'personnel', 'ouverture', 'total', 'sait', 'envie', 'meilleure', 'six', 'fais', 'fil', 'collection', 'Liste', 'Marie', 'premières', 'semaines', 'groupes', 'désormais', 'parents', 'malgré', 'hôtels', '1998', 'Espagne', 'Guerre', 'Tour', '1990', 'ami', 'manque', 'lettre', 'position', 'hors', 'finale', 'via', 'cependant', 'nommé', 'conseils', 'haute', 'laisser', 'Notre', 'lieux', 'professionnels', 'difficile', 'militaire', 'venir', 'celles', 'bout', 'visite', 'Ven', 'évolution', 'coeur', 'internationale', 'veux', 'comprendre', 'université', 'voie', 'Rechercher', 'permettant', 'contrat', 'LE', 'Société', 'cher', 'Club', 'économie', 'soleil', 'partager', 'professionnel', 'chemin', 'devenir', 'permettre', 'Chine', 'bar', 'commentaires', 'établissement', 'traitement', 'réalité', 'utilise', 'retrouve', 'sélection', 'train', 'élèves', 'usage', 'port', 'tels', 'Bon', 'Etat', 'tes', 'européenne', 'Wikipédia', 'objectif', 'espèce', '{', 'faisant', 'concours', 'feu', 'lecture', 'location', 'suivi', 'certain', 'ca', '200', 'joueurs', 'vendredi', 'mariage', 'écran', 'propriété', '36', 'endroit', 'résultat', 'possède', 'samedi', 'disposition', 'décision', 'Facebook', 'analyse', 'mission', 'Très', 'etc', 'marche', '1997', 'from', '│', 'Lyon', 'Toutes', '34', 'soient', 'bâtiment', 'DU', 'moyens', 'province', 'Art', 'suivante', 'compagnie', 'longue', 'Fichier', 'américaine', 'puisque', 'inscrit', 'sorti', 'at', 'lundi', 'publics', 'pourtant', 'éviter', 'Suisse', 'finalement', 'Cependant', 'achat', 'personnage', 'parcours', 'Nouveau', 'enseignement', 'Commentaires', 'reçu', 'animaux', 'meilleurs', 'complet', 'parties', 'sources', '1996', '70', 'musée', 'chanson', 'Article', 'montre', 'Nos', 'Image', 'devez', 'importe', 'contact', 'officiel', 'outils', '1995', 'lui-même', 'DES', 'actions', 'peine', 'Juin', 'allemand', 'note', 'affaire', 'Église', 'bureau', 'processus', 'sol', 'matériel', 'Qui', 'changer', 'ait', '38', 'Nicolas', 'pratiques', 'importante', 'ouvrage', 'Pays', 'document', 'San', 'comprend', 'parfait', 'bain', 'furent', 'attention', 'liberté', 'possibilité', 'uniquement', 'Jan', 'M', 'by', 'X', 'sort', 'théâtre', 'frère', 'équipes', 'Ses', 'championnat', 'relation', 'police', 'mémoire', 'est-ce', \"S'\", 'Enfin', 'salon', 'Musée', 'laisse', 'commerce', 'armes', '44', 'personnages', '48', 'Henri', 'soutien', 'client', 'quelle', 'vitesse', 'Articles', 'lumière', 'extérieur', 'utilisateur', 'victoire', 'hôtes', 'Lors', 'course', '42', 'réaliser', 'choisir', 'objets', 'III', 'administration', 'véritable', 'bons', 'éducation', 'ouest', 'derrière', 'Ligue', 'tandis', 'généralement', 'Deux', 'annonces', 'peuple', 'acheter', 'règles', 'titres', '39', 'besoins', 'gamme', 'combat', 'huile', 'Wish', 'sociaux', 'honneur', 'critique', 'sorte', 'gare', 'continue', 'crise', 'papier', 'hiver', 'bataille', 'piscine', 'réseaux', 'sport', 'Japon', 'commun', 'retrouver', '41', '1994', 'permettent', 'puissance', 'modèles', 'thème', 'sciences', '37', 'mêmes', 'appelle', 'moderne', 'Ne', 'with', 'responsable', 'exposition', 'neuf', 'anciens', 'ajouter', 'court', 'classique', 'Petit', 'principe', 'ouvert', 'ouvre', 'forte', 'crois', 'précédent', 'sauf', 'stock', 'Publié', 'principale', '43', 'professeur', 'dispose', 'navigation', 'Londres', 'Amérique', 'régime', 'forces', '55', '90', 'garde', 'rend', 'buts', 'Elles', 'vol', 'appelé', '500', 'couple', 'livraison', 'celui-ci', 'Association', 'demander', 'avance', 'Accessoires', 'électrique', 'mains', '1992', 'connaître', 'transport', 'telle', 'connaissance', 'ressources', '--Le', 'changement', 'peau', 'dix', 'filles', 'i', 'offres', 'Chambre', 'Informations', 'Institut', 'Noël', 'impression', 'Voici', 'Angleterre', 'étape', 'magnifique', 'physique', '1980', 'vois', 'textes', 'cookies', 'numérique', 'présentation', 'utilisateurs', 'œuvres', 'Signaler', 'documents', 'majorité', 'fer', 'télévision', 'étudiants', 'artistes', 'recevoir', '52', 'relations', 'étais', 'élections', 'professionnelle', 'Russie', 'Festival', 'bande', 'poids', 'privée', 'principalement', 'St', 'émission', 'bonnes', '1993', 'familles', 'Ma', 'britannique', 'lignes', 'caractère', 'assurer', 'Thomas', 'participe', 'découverte', 'E', 'proposer', 'cartes', 'souhaite', 'Or', 'Salle', 'recherches', 'partenaires', 'chance', 'maire', 'peur', 'rivière', 'vignette', 'Ville', 'acteur', 'explique', 'univers', 'direct', '49', 'surface', 'soirée', 'confiance', 'journal', 'facilement', 'Windows', 'bientôt', 'Est', 'dessus', 'Mes', 'arrière', 'lac', 'Présentation', 'nouvel', 'Montréal', 'logiciel', 'abus', 'Répondre', 'devenu', 'installation', 'courant', 'faible', 'travailler', 'Service', 'Rome', 'profiter', 'langues', 'capacité', 'André', 'systèmes', 'auteurs', 'maisons', 'née', 'attaque', 'accessoires', 'Martin', 'actuelle', 'soins', 'Hotel', 'capitale', 'tableau', '1991', 'pieds', 'artiste', 'fête', 'fonds', 'concerne', 'est-à-dire', '46', 'classement', 'hauteur', 'plage', 'original', 'sept', 'mesures', 'coin', 'centrale', 'diverses', 'faite', 'dossier', 'cité', 'Pourquoi', 'ci-dessous', 'marques', 'EN', 'justice', 'Bernard', 'gratuitement', 'types', 'station', 'Sans', 'formes', 'célèbre', 'Joseph', 'éditions', 'fonctions', 'adore', 'dis', 'jeudi', 'paix', 'limite', 'sortir', 'LES', 'vote', 'Web', 'pleine', 'Championnat', 'mercredi', 'acteurs', 'principaux', 'plans', 'exploitation', 'épouse', 'supérieur', 'James', 'Georges', 'Parti', 'médias', 'confort', 'Claude', 'cherche', 'format', 'signe', 'propres', 'composé', 'R', 'mardi', 'V', '47', 'communale', 'naturel', 'FC', 'faites', 'di', 'lutte', '51', 'entièrement', 'peinture', 'actuel', 'écrire', 'structure', 'vieux', 'Premier', 'Vue', 'situe', 'pose', 'Monde', 'quotidien', 'espère', '´', 'Avr', 'espèces', '1970', 'risques', 'o', '59', 'populaire', 'prochain', 'infos', 'distance', 'étoiles', 'proches', 'latitude', 'moitié', 'détails', 'termes', 'Richard', 'immédiatement', 'solutions', 'contraire', 'sociales', 'importance', 'idéal', 'effets', 'représente', 'parfaitement', '¤', '56', 'Alain', 'locaux', 'entretien', '54', 'partout', 'penser', '1989', 'chaîne', 'top', 'pierre', 'patrimoine', 'voire', '57', 'choisi', 'réponses', 'g', 'rester', 'informatique', 'maladie', 'totalement', 'e-mail', 'Michael', 'discussion', 'complète', 'Guide', 'studio', 'pourra', 'Location', 'saint', 'Avis', 'défense', 'Carte', 'vidéos', 'réalisation', 'scientifique', 'Plan', 'Grande', 'rendez-vous', 'erreur', 'pourrez', 'Contact', 'décidé', 'gaz', 'opération', 'industrie', 'raisons', 'générales', 'H', 'Sujet', 'longitude', 'assurance', 'contacter', 'privé', 'améliorer', 'align', 'Hôtel', 'belles', 'valeurs', 'enquête', 'atteint', 'croissance', 'perdu', 'avenir', 'traduction', 'suffit', 'bébé', 'faits', 'participation', 'russe', 'régulièrement', 'zones', 'Président', 'appareil', 'goût', 'Groupe', 'El', 'terrasse', 'p', 'maximum', 'tellement', 'formé', 'Lycée', 'local', 'fonctionnement', 'Terre', 'dos', 'remporte', 'portes', '53', 'canton', 'ordinateur', 'gratuite', 'restaurant', 'machine', 'sexe', 'utilisant', 'fichier', 'construit', 'cour', 'division', 'mobile', 'approche', 'Bruxelles', 'recette', 'F', 'absence', 'écoles', 'vis', 'pouvait', \"lorsqu'\", 'unité', 'dû', 'sert', 'voyageurs', 'actualité', 'Rue', 'noms', 'volonté', 'existence', 'expression', 'ministère', 'méthode', 'italien', 'propriétaire', 'sociétés', 'Code', 'partage', 'cheveux', 'tient', 'décide', 'Marseille', '1988', 'International', 'Nov', 'bleu', 'consommation', 'entraide', 'élu', 'Autres', 'matchs', 'confortable', 'revient', 'européen', 'mondial', 'National', 'électronique', 'participer', 'régions', 'identité', 'Daniel', 'DH', 'Pendant', 'PC', 'minimum', 'Fév', 'faisait', 'ventes', 'quant', 'trouverez', 'apos', 'revue', 'probablement', 'Donc', 'apparaît', 'Oct', 'Chaque', 'fenêtre', 'voici', 'Aucun', 'demandes', 'recommande', 'ferme', 'outre', 'futur', 'morts', 'pression', 'maître', 'événements', 'réserve', 'attendre', '58', 'équipements', 'William', 'acte', 'viens', 'L.', 'regard', 'vert', 'publication', 'belge', 'différence', 'magasin', 'vent', 'kilomètres', 'étranger', 'reçoit', 'A.', 'manger', 'présenter', 'champ', '→', 'contexte', 'suivants', 'déjeuner', '300', 'École', 'chien', 'compétition', 'Services', 'Première', 'outil', 'commencé', 'porter', 'P', 'coupe', 'Bordeaux', 'statut', 'George', '́', 'montagne', 'chercher', 'responsabilité', 'Santé', 'description', 'Même', 'écriture', 'Empire', \"Aujourd'\", 'Partager', '1986', 'signifie', 'repas', 'immobilier', 'gagner', 'conception', 'travaille', 'bras', 'Bretagne', 'Avant', 'Dès', 'visage', 'fera', 'guide', 'efficace', 'rendu', 'puisse', 'id', 'épisodes', 'créée', '1987', 'hier', 'longueur', '†', 'Livraison', 'revenir', 'sinon', 'Titre', 'écrivain', 'correspond', 'élection', 'obtient', '1960', 'emplacement', 'design', 'celle-ci', 'tendance', 'Laurent', \"quelqu'\", 'voilà', 'Sciences', 'développer', 'réduction', 'domicile', 'applications', 'décès', '--La', 'jeunesse', 'réel', 'fit', '1982', 'retraite', 'contient', 'places', 'devait', 'radio', 'peintre', 'littérature', 'Déc', 'mises', 'Moi', 'forêt', 'figure', 'toutefois', 'beauté', 'clair', 'Commission', 'prochaine', 'Contenu', 'largement', '1984', 'cul', 'change', 'constitue', 'Sep', 'économiques', 'entier', 'ouverte', 'respect', 'disque', 'payer', 'Lorsque', 'vin', 'Connexion', '1985', 'Toulouse', 'café', 'milliards', 'bonheur', 'rejoint', 'programmes', 'vaut', 'médecin', 'oeuvre', 'pont', 'représentant', 'del', 'intérêts', 'Sélectionner', 'visiter', 'Entre', 'ciel', 'Retour', 'pétrole', 'verre', 'plantes', 'véhicule', 'preuve', 'chargé', 'suivantes', 'Patrick', 'Marc', 'joué', 'Homme', 'anniversaire', 'modifier', 'conseiller', 'Aoû', 'fruits', 'discours', 'débat', 'atteindre', 'altitude', 'phase', 'instant', 'historiques', 'Bonne', 'prévu', 'b', 'agence', '‘', 'vit', 'passant', 'Juil', 'regarder', 'Culture', 'final', 'certaine', 'Loire', 'inscrire', 'architecture', 'Nom', 'atelier', 'J.', 'critères', 'Maroc', 'issue', 'Disponible', 'vision', 'fleurs', 'spectacle', 'évaluation', 'huit', 'basse', 'prêt', 'complètement', 'louer', 'centres', 'volume', 'utilisés', 'sympa', 'Air', 'essayer', 'température', 'opérations', 'collaboration', 'fiche', 'souhaitez', '75', 'offrir', 'Ouest', 'demandé', 'Puis', 'dollars', 'distribution', 'Cliquez', 'tres', 'DVD', 'lu', 'supérieure', 'liés', 'montant', 'intervention', 'boutique', 'influence', 'Monsieur', 'diffusion', 'Conditions', 'troupes', 'sang', 'nécessaires', 'utilisée', 'Éditions', 'rejoindre', 'tenu', 'lance', 'véhicules', 'compter', 'objectifs', 'arrêt', 'Découvrez', 'Assemblée', 'construire', 'apprendre', \"N'\", 'présenté', 'Super', 'élevé', 'Mme', 'Certains', 'scolaire', 'publiques', 'compétences', 'éditeur', 'connecté', 'cliquez', 'Anne', 'excellent', 'écoute', 'budget', 'françaises', 'opposition', 'concept', 'étage', '150', 'équipé', 'événement', 'Tags', '1983', 'test', 'niveaux', 'commencer', 'avion', 'échange', 'caractéristiques', 'servir', 'envoyer', 'T', 'voulez', 'Château', 'tenue', 'fichiers', 'City', 'Sport', 'côtés', 'totale', 'poser', 'stade', 'eaux', 'entendu', 'Théâtre', 'conscience', 'humain', 'vallée', 'militaires', 'Christian', 'no', 'réussi', 'humaine', 'coordonnées', 'mauvais', 'touche', 'riche', 'Musique', 'associations', 'Twitter', 'suit', 'protéger', 'Top', 'Quelques', 'ouvrages', 'mari', 'portant', '×', 'remise', 'soi', 'candidat', 'Guillaume', 'Age', 'comte', 'utile', 'dur', 'aéroport', 'meilleures', 'IV', 'stratégie', 'hésitez', 'Algérie', 'promotion', 'Afficher', 'Créer', 'vide', '1975', 'autorités', 'Vie', '1981', 'telles', 'you', 'préparation', 'élève', 'technologie', 'théorie', 'Total', 'arrêté', '1978', 'Peter', 'paiement', 'journaliste', 'prises', 'tente', 'indique', 'locale', 'ouvrir', 'principales', 'Ben', 'traité', 'festival', 'espaces', 'von', 'loisirs', 'naturelle', 'défaut', 'support', 'baisse', 'Israël', 'phpBB', 'rencontres', 'O', 'Cour', '1968', 'Résultats', 'découvert', 'comptes', 'plat', 'Antoine', 'jolie', 'crée', 'Modèle', 'annoncé', 'victimes', 'avions', 'recettes', 'installer', 'lait', 'dehors', 'biens', 'légales', 'impossible', 'croire', 'email', 'Alexandre', 'municipalité', 'établissements', 'Asie', 'domaines', 'tombe', 'week-end', 'intéressant', 'noire', 'arts', 'conférence', 'Car', 'considéré', 'allez', 'champion', 'magazine', 'clubs', 'Olivier', 'coups', 'Parc', 'arriver', 'Parmi', 'commercial', 'pouvant', 'World', 'post', 'Disney', 'Académie', 'salles', 'fortement', 'résidence', 'artistique', 'champs', 'tourisme', 'proposé', 'In', 'CD', 'davantage', 'lancer', 'conflit', 'aventure', 'séries', 'serveur', 'rêve', 'civile', 'faveur', 'enregistrer', 'connue', '1979', 'Ça', 'tenir', 'japonais', 'perte', 'fonctionne', 'Albert', 'mairie', 'termine', 'espagnol', 'lesquels', 'garder', 'Jouer', 'allemande', 'précise', 'montrer', 'déclaré', 'exercice', 'quatrième', 'vérité', 'basée', 'scientifiques', 'trouvent', 'importants', 'right', 'capable', 'prison', 'villages', 'catégories', 'Maurice', 'soin', 'actes', 'aurais', 'métier', 'C.', 'veulent', 'foi', 'quantité', 'chinois', 'masse', 'expédition', 'récemment', 'charme', 'revanche', 'stage', 'concert', 'complexe', 'milliers', 'was', 'accéder', 'tôt', 'van', 'pop', 'pensée', '1976', 'Comité', 'secrétaire', 'der', 'superbe', 'clé', 'particuliers', 'fini', 'printemps', 'demain', 'commission', 'originale', 'camp', 'Permalien', 'dessin', 'marchés', 'envers', 'réception', 'lois', 'Dr', 'religion', 'chansons', 'lycée', 'ambiance', 'Mars', 'Quel', 'dois', 'vivant', 'engagement', '›', 'juridique', 'mur', 'noter', 'ski', 'consulter', 'central', 'option', '1977', 'juge', 'Sous', 'absolument', 'entrer', 'viennent', 'meme', 'Type', 'jaune', 'élément', 'r', 'chaleureux', 'catholique', 'Note', 'vendre', 'invite', 'menu', 'rose', 'essentiel', '1950', 'tarifs', 'couverture', 'Archives', 'Saison', 'Se', 'évêque', 'Pologne', 'Livre', 'Berlin', 'difficultés', 'blanche', '400', 'chapelle', 'olympiques', 'organisé', '1972', 'procédure', 'présents', 'frères', 'performance', 'perdre', '1973', 'Message', 'notes', 'génération', 'Journal', 'voitures', 'au-dessus', 'médecine', 'bâtiments', 'condition', '®', 'Photo', '1974', 'rappelle', 'importantes', 'glace', 'cheval', 'durable', 'connaît', 'effectuer', 'quitte', 'contenant', 'pro', 'continuer', 'tradition', 'candidats', 'beaux', 'lancé', 'automobile', 'Trois', 'Malgré', 'coût', 'réunion', 'Ca', 'primaire', 'réduire', 'chat', 'obtenu', 'définition', 'Produits', 'résumé', 'chasse', 'apporter', 'Jésus', 'sucre', 'Espace', 'Général', 'Vincent', 'laissé', 'vérifier', 'lendemain', 'député', 'Salon', 'traduit', 'froid', 'actrice', 'clés', 'terres', 'reprises', 'reprise', 'chiffres', 'résistance', 'publiée', 'surprise', 'chocolat', 'alimentation', 'Nuit', 'Nouvelle', 'échelle', 'autorité', 'ceci', 'Fiche', 'capital', 'Etats-Unis', 'chaud', 'comité', 'Plusieurs', 'M2', 'licence', 'échanges', 'interne', 'épreuve', 'collège', 'joli', 'liées', 'âme', 'tiers', 'critiques', 'enfance', 'vélo', 'Arts', 'télévisée', 'envoyé', 'pourraient', 'côte', 'Royaume-Uni', 'murs', '65', 'bus', 'fabrication', 'Black', 'réalisateur', 'demeure', 'prince', 'piste', 'conduit', 'soldats', 'lecteur', 'États', 'hôte', 'Fédération', 'douche', 'batterie', 'salariés', 'cadeau', 'Gestion', 'aspect', 'home', 'sommet', 'connaissances', 'Alpes', 'Projet', 'essentiellement', 'oublier', 'Politique', 'philosophie', 'René', 'seuls', 'district', 'Grâce', 'religieux', 'sac', 'IP', 'occupe', 'Nantes', 'locales', 'duc', 'documentaire', 'Toutefois', 'chute', 'méthodes', 'scénario', 'planète', 'parking', 'sympathique', 'héros', '2007Sujet', 'garantie', 'label', 'pêche', 'comportement', 'renseignements', 'cycle', 'humaines', 'crédit', 'mélange', 'consiste', 'précédente', 'accueille', 'logiciels', 'ajouté', 'Me', '.....', 'visiteurs', 'boîte', 'forcément', 'Van', 'grave', 'Australie', 'tournée', 'exception', 'multiples', 'chiffre', 'Film', 'connexion', 'logique', 'restauration', 'somme', '64', 'préparer', 'mail', 'comté', 'équipée', '1962', 'eut', 'édité', 'moindre', 'réflexion', 'portail', 'accessible', 'Actualités', 'vraie', 'anciennes', 'proposition', 'Mise', 'Inde', 'technologies', 'Leur', '1940', 'Bienvenue', 'financement', 'mouvements', 'modification', 'royaume', 'évidemment', 'acceptez', 'spécialiste', 'crème', 'Seconde', '1945', 'Voilà', 'File', 'etre', 'gérer', 'îles', 'découvre', 'affiche', 'Formation', 'équipement', 'Vos', '1967', 'augmentation', 'banque', 'règle', 'feuilles', 'agriculture', 'langage', 'Los', 'automatiquement', '3D', 'secret', 'simples', 'impact', 'Star', 'Ou', 'Description', 'till', 'certainement', 'régional', 'citer', 'info', 'rapports', 'portée', 'démarche', \"Qu'\", 'arrondissement', 'profil', 'hôpital', '1971', 'accueillir', 'suisse', 'expliquer', 'officielle', 'appareils', 'révolution', 'restaurants', 'violence', 'secondes', 'a-t-il', 'Durant', 'néanmoins', 'voulu', 'Pro', 'Brésil', 'veille', 'normal', 'animation', 'connais', 'Frédéric', \"--L'\", 'Roger', 'comporte', 'danse', '1969', 'inclus', 'Marine', 'apparition', 'bibliothèque', 'record', 'G', 'décrit', 'Strasbourg', 'score', 'Catherine', 'Bref', 'indépendance', 'archives', 'Henry', 'destination', 'Auteur', 'Genève', 'this', 'humains', 'composée', 'revenu', 'clairement', 'moments', 'f', 'VF', 'Dominique', 'faux', 'apprentissage', 'Aide', 'donnée', 'passion', 'achats', 'mauvaise', 'Attention', 'devoir', 'Royal', 'pilote', 'tome', 'Femme', 'chapitre', 'chaleur', 'faudra', 'permettra', 'USA', 'fournir', 'féminin', 'assure', 'reprend', 'thèmes', 'Radio', 'superficie', 'élus', 'séance', 'PS', 'investissement', 'commerces', 'producteur', 'citoyens', 'financière', 'Direction', 'indiqué', 'connecter', 'exactement', '1944', 'architecte', 'capitaine', 'Appartement', 'fondée', 'pire', 'publie', 'effectivement', 'science', 'meurt', 'heureux', 'initiative', 'météo', 'Maria', 'Révolution', 'conforme', 'entendre', 'arrivé', 'réforme', 'saisons', 'actif', 'accident', 'réalisée', 'matières', 'dessous', 'adultes', 'placé', 'rock', 'guitare', 'faudrait', 'truc', 'Place', 'text', 'Nice', 'bouche', 'nucléaire', 'réalise', 'hommage', 'acheté', 'essai', 'aimé', 'urgence', 'présidentielle', 'cuir', 'utiles', 'Collection', 'Var', 'reprendre', 'appartient', 'voyages', 'fondé', 'partenaire', 'tournoi', 'appelée', 'grosse', 'Banque', '1000', 'culturel', 'chômage', 'délai', 'principes', 'Quelle', 'pâte', 'eacute', 'piano', 'Sécurité', 'tours', 'décoration', '2008Sujet', 'WP', 'Y', 'frontière', 'difficulté', 'développé', 'étrangers', 'catalogue', 'faute', 'matériaux', 'spécial', 'missions', 'arabe', 'Anglais', 'circuit', 'four', 'Victor', 'permanent', 'réservation', 'étrangères', 'yoga', 'douce', 'auraient', 'Christophe', 'Jack', 'avantage', 'palais', 'responsables', 'Médecine', 'kg', 'classé', 'Park', 'pointe', 'supplémentaires', '\\ufeff', 'rares', 'bassin', 'Lille', 'Cours', 'sexy', 'avocat', 'pain', 'prennent', 'vêtements', '120', 'victime', 'pouvons', 'précis', 'One', 'Christ', 'profit', 'vouloir', 'disponibilité', 'parole', 'Eric', 'sent', 'marketing', 'arrêter', 'légèrement', 'signé', 'hausse', 'participé', 'Aux', 'robe', 'aurez', 'neige', 'Source', 'Catégories', 'Gérard', 'dynamique', 'transports', 'composition', 'classes', 'Guy', 'Générale', 'lesquelles', 'essais', 'produire', 'reconnaissance', 'devenue', 'propriétaires', 'visites', 'net', 'références', 'bains', 'vaste', 'spécifique', '1965', 'auquel', 'américains', 'conséquences', 'légende', 'convient', 'marine', 'elle-même', 'attend', 'restent', 'abbaye', 'Côte', 'empereur', '1966', '™', 'adapté', 'dessins', 'Max', 'Jules', 'efficacité', 'very', 'Pascal', 'entraîneur', 'environs', '250', 'institutions', 'parfaite', 'bref', 'décret', 'déclaration', 'conduite', 'Agence', 'machines', 'Sinon', '1964', 'rubrique', 'monuments', 'seraient', 'performances', 'single', 'au-delà', 'mandat', 'italienne', 'indépendant', 'You', 'partenariat', 'offert', 'portable', 'R.', 'souvenir', 'rédaction', 'Ceci', 'siècles', 'spécifiques', 'changements', 'porté', 'center', 'relativement', 'Simon', 'extension', 'organisations', 'électricité', 'limites', 'Microsoft', '1930', 'Manuel', 'Vidéo', 'collectif', 'Toute', 'représentation', 'plateau', 'possibles', 'réduit', 'recours', 'participants', 'centaines', 'dispositions', 'finir', 'Adresse', 'majeur', 'populations', 'agent', 'habitude', 'géographique', 'automatique', 'Ah', 'graphique', 'animal', 'sportif', 'Tourisme', 'emplois', 'Parlement', 'arbre', 'chefs', 'donnant', 'Tom', 'revenus', 'opinion', 'S.', 'fondateur', 'bilan', 'unités', 'Madame', 'estime', 'axe', 'Grèce', 'formule', 'littéraire', 'dépend', 'collections', 'dispositif', 'venu', 'bénéficier', 'fixe', 'phénomène', 'bouton', 'Blog', 'pape', 'rencontrer', 'rythme', 'préféré', 'notion', 'amoureux', 'répond', 'chant', 'dite', 'rare', 'gris', 'P.', 'remplacer', 'réellement', 'Accès', 'basé', 'fêtes', 'formations', 'und', 'usine', 'Ordre', 'bases', 'adaptation', 'financiers', 're', 'expériences', 'oiseaux', 'européens', 'pouvoirs', 'Marcel', 'procès', 'Est-ce', 'agents', 'apporte', 'étapes', 'Mentions', 'toile', 'rang', 'voies', 'ajoute', 'consacré', 'donnent', 'moto', 'tenter', 'finit', 'chacune', 'Région', 'longues', 'arbres', 'vues', 'versions', 'su', 'règne', 'Quant', 'liésPortail', 'copie', 'reconnu', '─', 'ex', 'actifs', 'Docteur', 'clip', 'circulation', 'remettre', 'iPhone', 'intitulé', 'fallait', 'Cuisine', 'courte', 'est-il', 'rues', 'dirigeants', 'métiers', 'Ensuite', 'climat', 'particulière', 'chiens', 'efforts', 'officiellement', 'récit', 'supprimer', 'bel', 'standard', 'experts', 'automne', 'courage', 'relative', '1963', 'attente', 'Pays-Bas', 'internautes', 'monter', 'Pièces', 'marqué', '1958', 'publicité', 'changé', 'partis', 'Stade', 'Jean-Pierre', 'Al', 'extrêmement', 'croix', 'Photos', 'bruit', 'Envoyer', 'tomber', 'conçu', 'Portail', 'commandes', 'latin', 'sable', '72', 'minute', 'left', 'former', 'télécharger', 'Thierry', 'enregistré', 'perd', 'Chambres', 'options', 'transfert', 'Palais', 'bateau', 'internationales', 'Bureau', 'justement', 'Blanc', 'pluie', 'territoires', 'titulaire', 'avancée', 'Office', 'obligatoire', 'angle', 'réservés', 'Love', 'horaires', 'structures', '1961', 'Petite', 'Afin', 'oublié', 'spécialisé', 'ci-dessus', 'examen', 'Divers', 'Galerie', 'pourront', 'personnelle', 'chanteur', \"puisqu'\", 'contenus', 'correspondant', 'Création', 'compositeur', 'interdit', 'histoires', 'sœur', 'Hollande', 'municipal', 'Communauté', 'Livres', 'sérieux', 'réussite', 'lecteurs', 'agricole', 'Bois', 'normes', 'mille', 'FAQ', 'Julien', 'quitter', 'avantages', 'Directeur', 'phrase', 'F.', 'augmenter', 'lorsqu', 'Rien', 'upright', '1920', 'logements', 'Washington', 'José', 'cabinet', 'enregistrement', 'Loi', 'B.', 'acier', 'Discussion', 'installé', 'précision', 'représentants', 'Dernière', 'Roi', 'intéresse', 'Bruno', 'classiques', 'exécution', 'listes', 'aménagement', 'œil', 'soumis', 'My', 'définir', 'billet', 'Lien', 'organisme', 'vigueur', 'logo', 'espoir', 'Autriche', 'raconte', 'utilisent', 'danger', 'v', 'prestations', 'Évaluation', 'paroles', 'Champion', 'Tunisie', 'Portugal', 'associé', 'attendant', 'avenue', 'allant', 'combien', 'Encore', 'organiser', 'Emmanuel', 'Faire', 'destiné', 'rayon', 'Vienne', 'Yves', 'vendu', 'démocratie', 'Chez', '♥', 'plastique', 'patients', 'impose', 'réaction', 'bronze', 'Âge', 'spéciale', 'extrait', 'potentiel', 'Montpellier', 'Apple', 'anglaise', 'vainqueur', 'constitué', 'allait', 'Ici', 'Table', 'canal', 'sel', 'midi', '1954', '95', 'Revue', 'Aucune', 'interprétation', 'doux', 'seigneur', 'limitée', 'camping', 'Système', 'TTC', 'dédié', 'chevaux', 'surveillance', 'intégration', 'maladies', 'appartements', 'pierres', 'issus', 'lancement', '85', 'session', 'légumes', 'hockey', 'supplémentaire', 'personnelles', 'Bibliothèque', 'Parce', 'musicale', 'individus', '1936', 'différent', 'organise', 'financier', 'ateliers', 'Affaires', 'Nationale', 'Nations', 'Jardin', 'Moscou', 'quels', 'Noir', 'montage', 'construite', 'rouges', 'numéros', 'Où', 'défaite', 'front', 'Père', 'culturelle', 'auront', 'armées', 'auto', 'commerciale', 'POUR', '1956', '1946', '1959', 'humour', 'postes', 'accepte', 'reine', 'autorisation', 'métro', 'remplacé', 'charges', 'Cannes', 'No', 'agir', 'métal', 'arme', 'Droit', 'règlement', 'poète', 'Entreprises', 'sports', 'Suite', 'Gilles', 'Pourtant', 'innovation', 'barre', 'vise', 'sorties', 'débuts', '62', 'Android', 'exemplaires', 'identifier', 'Big', 'Mr', 'alimentaire', 'garçon', 'hébergement', 'Normandie', 'rire', '600', 'employés', 'dates', 'Travaux', 'établir', 'écrits', 'vivement', 'pistes', 'flux', 'Série', 'socialiste', 'secondaire', 'Protection', 'apprend', 'dimension', 'égard', 'poissons', 'présentes', 'solo', 'don', 'tirer', 'vols', 'mécanique', 'poursuit', 'tourne', 'amélioration', 'annuaire', 'gagne', 'ceinture', 'électriques', 'Rennes', 'Californie', 'équilibre', 'secteurs', 'nuits', 'allons', 'nécessité', 'Infos', 'chercheurs', 'Belle', 'paysage', 'active', 'blancs', 'médaille', 'concurrence', 'Durée', 'Aller', 'erreurs', 'bac', 'joie', 'USB', 'ben', 'repris', 'travailleurs', 'préfère', 'royale', 'invités', 'respecter', 'Madrid', 'Demande', 'appeler', '1939', '2006Sujet', 'indispensable', '\\u200b', 'suppression', 'orchestre', 'Réponse', 'émissions', 'morceaux', 'luxe', 'précisément', 'trafic', 'léger', 'alentours', 'prénom', 'tué', 'Fondation', 'tennis', 'solaire', 'Denis', 'voulait', 'travaillé', 'Sports', 'cadeaux', 'Partagez', 'intention', 'naturelles', 'Famille', 'considère', 'Red', 'modernes', 'favoris', 'engage', 'hasard', 'vécu', 'sentiment', 'courses', 'Arthur', 'poursuivant', 'cesse', 'auparavant', 'Z', 'Vers', 'grec', 'Détails', 'urbaine', 'extrême', 'voulais', 'bio', 'live', 'rupture', 'soutenir', 'Programme', 'humanité', 'photographie', 'calcul', 'modifications', 'visant', 'faciliter', 'Santa', 'signature', 'plages', 'maintenir', 'déco', 'tissu', 'amie', 'affirme', 'Retrouvez', 'title', 'universitaire', 'Angeles', 'cathédrale', 'vingt', 'demi', '78', 'mettant', 'familiale', '77', 'Produit', 'personnalité', 'Offres', 'menace', 'mention', 'Mode', 'apprécié', 'Salut', 'scolaires', 'U', 'aimerais', 'écrite', 'quartiers', 'Sarkozy', 'calendrier', 'thé', 'rayons', 'News', 'All', 'exemples', 'conserver', 'échec', 'libres', 'vieille', 'SUR', '1957', 'décisions', 'plaque', 'dure', 'tribunal', 'alt', 'organisée', 'Qu', 'Constitution', 'conséquence', 'personnalités', 'Hôtels', 'Organisation', 'li', 'Emploi', 'semblent', 'fondation', 'maîtrise', 'essence', 'w', '68', 'leader', 'amateurs', 'magasins', 'bureaux', 'désigne', 'boite', 'coopération', 'retourner', 'propositions', 'Information', 'Music', 'issu', 'défendre', 'populaires', 'prévue', 'rugby', 'mettent', 'lié', 'installations', 'coté', 'King', 'maman', 'bonjour', 'Introduction', 'monument', 'ombre', 'Stéphane', 'Frais', 'remporté', 'journalistes', 'vins', 'D.', 'Moyen', 'Haute', 'it', 'civil', 'révèle', 'couples', 'fins', '1942', 'albums', 'souvenirs', 'Mark', 'transformation', 'tests', 'tourner', 'profondeur', 'Suède', 'ingénieur', 'fans', 'regarde', 'poésie', 'Q', 'touristique', 'terrains', '1955', 'HD', 'dialogue', 'nationales', 'scènes', 'Soins', 'dommage', 'Bourgogne', 'branche', 'après-midi', 'tir', 'ci', 'Seigneur', 'attaques', 'refuse', 'déroule', 'étudiant', 'profession', 'video', 'aimez', '800', 'régionale', 'autonomie', 'navigateur', 'G.', '66', '1943', 'éd.', 'plateforme', 'Veuillez', 'Provence', 'Milan', 'Turquie', 'Edition', 'Irlande', 'chaussures', 'empêcher', 'démocratique', 'HT', '1948', 'effort', 'instruments', 'façade', 'effectué', 'prévention', 'uns', 'Questions', 'rencontré', 'connus', 'accompagné', 'montagnes', 'canadien', 'Compagnie', 'jardins', 'sommes-nous', 'English', 'cent', 'commandant', 'Football', 'débute', 'source1', 'chemins', '1914', 'viande', 'enjeux', 'beurre', 'paraît', 'ul', 'servi', 'européennes', 'spécialisée', 'Mexique', 'WC', 'moteurs', 'intérieure', 'Isabelle', 'Télécharger', 'cinquième', 'utilisées', 'situations', 'Francis', 'devraient', 'Macron', 'Frank', 'capacités', 'personnels', 'visible', 'combats', 'devra', 'fan', 'Jeanne', 'K', 'invité', 'retard', 'réservé', 'galerie', 'Syrie', 'évolue', 'tester', 'acquis', '67', 'Concours', 'footballeur', 'légère', 'Avril', 'successeur', 'interface', 'serez', 'industriel', 'enceinte', 'accepter', 'contemporain', 'Annonce', 'entière', 'Développement', 'réelle', 'parlé', 'associés', 'Version', 'obligation', 'nul', 'déchets', 'appui', 'étudier', 'résolution', 'décédé', 'villa', 'envoie', 'comprenant', '1947', 'banques', 'poisson', 'députés', 'directe', 'excellente', 'établi', 'entend', '84', 'massif', 'suffisamment', 'Aujourd', 'u', 'sauver', 'silence', 'Chris', 'organismes', 'traditionnelle', '69', 'ordres', 'Raymond', 'déclare', 'cliquant', 'billets', 'enseignants', 'routes', 'malheureusement', 'EUR', 'concerts', 'Studio', 'possibilités', 'égalité', 'audio', 'Go', 'Home', 'we', 'stockage', 'assemblée', 'Division', 'prenant', 'mérite', 'effectue', 'thermique', 'énorme', 'Smith', 'propriétés', 'tuer', 'alimentaires', 'judiciaire', 'dimensions', 'devint', 'décor', 'Aussi', 'puissant', 'appartenant', 'récupérer', 'Point', 'Fin', 'naturels', 'sourire', 'couche', 'terminé', 'Lee', 'thèse', 'romans', 'paru', 'Haut', 'ennemi', 'secours', 'installe', 'accueilli', 'fermeture', 'nez', 'désigner', 'tarif', 'intermédiaire', 'Barcelone', 'assistance', 'dossiers', 'Autre', 'Maître', 'rappeler', 'Villa', 'oeil', 'cancer', 'arrête', 'Matériel', 'progrès', 'Records', 'poursuivre', 'Sainte', '1953', 'United', 'Master', 'cache', 'appliquer', 'morceau', 'aspects', 'entraînement', 'océan', 'Rose', 'fou', 'Informatique', 'navire', 'chauffage', 'développe', 'industrielle', '1952', 'confirme', 'fleuve', 'cuisson', 'remis', 'gouverneur', 'meteo', 'douze', 'aimer', '63', 'poche', 'are', 'Congrès', 'constituent', 'exprimer', 'Française', '1941', 'fusion', 'Là', 'Vente', 'Open', 'E.', 'peuples', 'Val', 'plante', 'Croix', 'musulmans', 'Live', 'votes', 'comprends', 'cellules', 'soviétique', 'internationaux', 'disparu', 'tableaux', 'étoile', 'Orange', 'audience', 'globale', 'médecins', 'lits', 'coûts', 'souci', 'transmission', 'Janvier', 'appris', 'orientation', 'ressemble', 'cimetière', 'rentrée', 'synthèse', 'gratuits', 'pensé', 'discussions', 'origines', 'docteur', 'Caroline', 'indépendante', 'recensement', 'cérémonie', 'Eglise', 'passée', 'Luxembourg', 'législatives', 'col', 'Cinéma', 'détachées', 'certes', 'do', 'ère', 'Dernier', 'proposons', 'relève', 'communautés', 'immense', 'Actualité', 'diplôme', 'acquisition', 'We', 'American', 'manifestations', 'chantier', 'déterminer', 'chers', 'télé', '--Autres', 'contribution', 'culte', 'convention', 'voisins', 'Notre-Dame', 'victoires', 'patron', 'montré', 'Alsace', 'tension', 'Ministère', 'définitivement', '1949', 'diversité', 'Man', 'troubles', 'm2', 'endroits', 'adresses', 'Ancien', '61', 'rive', 'Corée', 'mener', 'lol', 'riches', 'Atelier', 'consommateurs', 'montée', 'facteurs', 'adulte', 'UN', 'navires', '↑', 'mobilité', 'originaire', 'majeure', '76', 'formulaire', 'autonome', 'conduire', 'inverse', 'dépenses', 'touristiques', '1938', 'house', 'Tome', 'House', 'plats', 'symbole', 'sportive', 'Design', 'liée', 'privés', 'mathématiques', 'Championnats', 'déplacement', 'Sophie', 'intégrer', 'vol.', 'da', 'immeuble', '1951', 'kmEntre', '00ZNous', 'incendie', 'Serge', 'devis', 'relatives', 'religieuse', 'évidence', 'désir', 'aiment', 'Chicago', 'conséquent', 'regroupe', 'officier', 'fr.wikipedia.org', '99', 'Rock', 'Don', 'union', 'agricoles', 'Armée', 'disent', '74', 'color', 'repos', 'autrement', 'Grenoble', 'créations', 'traiter', 'frontières', 'poudre', 'énergétique', 'aluminium', 'Harry', 'généraux', 'introduction', 'musical', 'cercle', 'accompagner', 'Street', 'liquide', 'voile', 'Iran', 'essayé', 'index.php', 'envoi', 'parvient', 'BD', 'caractères', 'industriels', 'Gîte', 'portrait', 'cultures', 'orange', 'Maintenant', 'comptait', 'empêche', 'débit', 'écouter', 'Copyright', 'administrative', 'nommée', 'Rouge', 'régiment', 'contrats', 'traces', 'soucis', 'gagné', 'Gare', 'gîte', 'Mont', 'maintien', 'XV', 'mène', 'talent', 'Chapitre', 'courrier', 'nourriture', 'pauvres', 'vivent', 'office', 'guerres', 'comédie', 'laissant', 'expositions', 'équivalent', 'perspective', 'dessinée', 'biais', 'communiqué', 'conférences', 'provient', 'assuré', 'traditionnel', 'Fort', 'portent', 'paroisse', 'Beaucoup', 'Ministre', 'intégré', 'diffusée', 'tiens', 'occupation', 'représentent', 'différente', 'H.', 'degré', 'Ecole', 'chanteuse', 'temple', 'journaux', 'retrait', 'contrairement', 'remplir', 'infanterie', 'alcool', 'qualités', 'monte', 'Lettres', 'administrateur', 'modifié', 'AU', 'Education', 'University', 'dizaine', 'Juan', 'Pièce', 'Mot', 'inspiré', '1937', 'limité', 'amitié', 'adapter', 'optique', 'étend', 'Nancy', 'Couleur', 'remplacement', 'jus', 'fédéral', 'Commander', 'motif', 'diffusé', 'morale', 'Laisser', 'modes', 'nationaux', 'max', 'remarque', 'Léon', 'Nature', 'Florence', 'présentent', 'commandement', 'mets', 'Front', 'Antonio', 'Pont', 'individu', 'sentir', 'Action', 'Conseils', 'Presse', 'élevage', 'retrouvé', 'répondu', 'solidarité', 'progressivement', 'enseigne', '2ème', 'paramètres', '1ère', 'Mario', 'coton', 'Team', 'West', 'collectivités', '--Vos', '92', 'toucher', 'Inscription', 'conseillers', 'Hugo', 'Menu', 'Loisirs', 'codes', 'Seine', 'Alex', 'Communication', 'Porte', 'paysages', 'sud-ouest', 'Prince', 'collective', 'accompagnement', 'tentative', 'stations', 'posé', 'parallèle', 'démarches', 'déposé', 'million', 'Demandé', 'privées', 'verte', 'Base', 'Joe', 'réparation', 'publications', 'Force', 'médicaments', 'garantir', 'laboratoire', 'extérieure', 'dirigé', 'proposés', 'candidature', 'consultation', 'consulté', 'conseille', '83', 'race', 'monnaie', 'destruction', 'spécialistes', 'cible', 'astuces', 'administratif', 'bien-être', 'venue', 'Égypte', '1931', 'Miss', 'reproduction', 'compose', 'intelligence', 'Outils', 'deviennent', '93', 'TVA', 'Toujours', 'Octobre', 'signes', 'randonnée', 'dangereux', 'fruit', '2009Sujet', 'boulot', 'Corse', 'Savoie', 'libération', 'édifice', 'numériques', 'spécialement', 'Of', 'offrent', 'contemporaine', 'informatiques', 'occuper', 'manifestation', 'disparition', 'revoir', 'gras', 'communiste', 'Mac', 'défi', 'renforcer', 'conservation', 'informer', 'Travail', 'patient', 'mini', 'motifs', 'com', 'pseudo', 'romaine', 'wiki', 'liaison', 'avoue', '1935', '71', 'Mots', 'provenant', 'ceux-ci', 'venus', 'nécessite', 'envies', 'relais', 'Françoise', 'densité', '1918', 'Juillet', 'maintenance', 'repose', 'voter', 'débats', 'recueil', 'pommes', 'Express', 'Lorraine', 'solide', 'Peu', 'disant', 'profite', '180', 'dépôt', 'attentes', '79', 'imposer', 'fameux', 'Monaco', 'nettoyage', 'Wi-Fi', 'sols', 'Mike', 'Rio', 'attitude', 'fasse', 'retirer', 'éclairage', 'Réunion', 'Fils', 'PDF', 'nomme', 'dédiée', 'mesurer', '82', 'circonscription', 'jugement', 'sud-est', 'It', 'Meilleur', 'fonctionnalités', 'configuration', 'Scott', 'musiciens', 'Production', 'parcs', 'nord-est', 'souris', 'historien', 'colis', 'art.', 'dizaines', 'destinée', 'oreilles', 'Rapport', '00ZTrès', 'Celui-ci', 'voudrais', 'conflits', 'secondaires', '1933', 'toit', 'Classement', 'passent', 'venant', '73', 'Membre', 'béton', 'norme', '81', 'Partie', 'Francisco', 'programmation', 'cru', 'Village', 'annuel', '2018', 'duo', 'doigts', 'épreuves', 'Permission', 'euro', 'magique', 'dents', 'applique', 'oiseau', 'Juifs', 'respectivement', 'quotidienne', 'Temps', 'disques', 'constitution', 'feuille', 'championnats', 'This', 'correctement', 'condamné', 'rentrer', 'Enfant', 'Museum', 'Septembre', 'mourir', 'Versailles', 'Adam', '1934', '140', 'Napoléon', 'Soleil', 'Qualité', 'ministres', 'Commande', 'diamètre', 'Caractéristiques', 'variété', 'interview', 'librairie', 'Certaines', 'aient', 'Département', 'volumes', 'contributions', 'préalable', 'rarement', 'virus', 'considérée', 'retourne', 'Vacances', 'Chef', 'con', 'Port', 'Mary', 'dirige', 'afficher', 'Argentine', 'aventures', 'Défense', 'savent', '1900', 'baie', 'eux-mêmes', 'japonaise', 'VI', 'ajoutant', 'Lettre', 'instrument', 'idéale', 'mobiles', 'abbé', 'génie', 'tablette', 'UE', '88', 'cerveau', 'inconnu', 'reconnaître', 'Bill', 'expédié', 'W', 'lumineux', 'ennemis', 'déplacer', 'Vêtements', 'savez', '......', 'Didier', 'physiques', 'Province', 'rénovation', 'appelés', 'Situé', 'Achat', 'be', 'constructeur', 'compatible', 'linge', 'masculin', '1932', 'stratégique', 'fournisseurs', 'exercices', 'détente', 'bancaire', 'Renault', 'forêts', 'producteurs', '2e', 'exigences', 'lot', 'normale', 'évènements', 'Justice', 'réalisés', 'richesse', 'GPS', 'vas', 'prêts', 'situés', 'olympique', 'dites', 'queue', 'Press', 'blessés', 'Tokyo', 'publier', 'élevée', 'exclusivement', 'Anna', 'polonais', 'chrétiens', 'médical', 'contraintes', 'existent', 'transition', 'roues', 'placer', 'Cité', 'fleur', 'amateur', 'Gabriel', 'relatif', 'tenant', 'us', 'Awards', 'secrets', 'spéciales', 'Vendredi', 'tâches', 'financières', \"O'\", 'centre-ville', 'sportifs', 'chaude', 'éventuellement', 'reçuesAnnonce', 'récente', 'Commerce', 'champions', 'atmosphère', 'présidence', 'accompagne', 'messagerie', 'Novembre', 'Tableau', 'positions', 'urbain', 'Référence', 'bienvenue', 'intègre', 'Gouvernement', '--Divers', 'Épisode', 'sièges', 'Faites', 'Jones', 'Collège', '1926', \"Jusqu'\", 'proposent', 'esthétique', 'évoque', 'croit', 'externe', 'empire', 'datant', 'nouveautés', 'Face', 'Conférence', 'tâche', 'noirs', 'opérateur', 'Orléans', 'recrutement', 'carré', 'pneus', 'Canal', 'salaire', 'offrant', 'Alfred', 'Acheter', 'institution', 'fine', 'pauvre', 'professionnelles', 'étrange', 'courants', 'fermé', 'adaptée', 'arrivent', 'compréhension', 'quasiment', 'Benoît', 'francophone', 'féminine', 'nations', 'V.', 'prête', 'Sébastien', 'hypothèse', '91', 'adaptés', 'statue', 'douleur', 'look', 'Vierge', 'fenêtres', 'sauce', 'Beach', 'forts', 'apparence', 'bénéficie', 'appels', 'encre', 'Rouen', 'infrastructures', 'romain', 'inspiration', 'difficiles', 'inscrits', 'réputation', 'Cordialement', 'suivent', 'Samedi', 'Steve', '►', 'espagnole', 'Année', 'similaire', '86', 'dieu', 'morte', 'fonctionnel', 'régionales', 'prédécesseur', 'conserve', 'câble', 'blocage', 'Quantité', 'médicale', 'résoudre', 'LNH', 'vapeur', 'transformer', 'départements', 'publiés', 'exceptionnelle', 'quelles', 'finances', 'Amour', '1921', 'sombre', 'Bataille', 'scénariste', 'présentée', 'compagnies', 'procédé', 'Blue', 'Jérôme', 'forment', 'courriel', 'tendances', 'nord-ouest', '125', 'normalement', 'quart', 'pur', 'traverse', 'chaînes', 'préciser', 'Zone', 'Oh', 'chœur', 'Téléphone', 'fidèle', 'Venise', 'commandé', '★', '1929', 'Benjamin', 'dame', 'hotel', 'fortes', 'satellite', 'colère', 'trains', 'traite', 'Poste', 'occidentale', 'favorable', 'princesse', 'salut', 'américaines', 'Mairie', 'claire', 'prévisions', 'indiquer', 'battre', 'collègues', 'Environnement', 'Réseau', 'rôles', 'White', 'scrabble', 'menée', 'écart', 'répartition', 'bloc', 'autoroute', 'malade', 'prêtre', 'aérienne', 'discipline', '110', 'Hongrie', 'témoignage', 'sortes', 'lutter', 'évident', 'alliance', 'mn', 'mines', 'bat', 'apparaissent', 'global', 'fournit', 'variable', 'Etats', 'League', 'royal', 'fréquence', 'filtre', 'Intérieur', 'Février', 'remarquable', 'périodes', 'Bob', 'dette', 'sponsorisé', 'Eau', 'adjoint', 'grille', 'adopté', 'quête', 'Néanmoins', 'Vidéos', 'Calendrier', 'congrès', '1919', 'culturelles', 'Bertrand', 'trente', '89', 'C3', 'choc', 'totalité', 'fourni', 'Ã', 'Liège', 'Luc', 'fromage', 'distingue', 'fuite', 'affichage', 'commerciaux', 'commerciales', 'Convention', 'Rhône', 'effectif', 'engagé', 'sauvage', 'Quatre', 'bienvenusCadre', 'arc', 'valide', 'employé', 'URL', 'chats', 'détruit', 'kit', 'Tours', 'Ali', 'recommandons', '160', 'incroyable', 'chargée', '360', 'prévoit', 'adaptées', 'encyclopédie', 'impôt', 'positif', 'campagnes', '--LES', 'rêves', '1917', 'journées', 'Commentaire', 'prépare', 'Dictionnaire', 'expertise', 'Ligne', 'fidèles', 'communiquer', 'tire', 'photographe', 'Samsung', 'montrent', 'Xavier', 'musées', 'prends', 'modalités', 'individuelle', 'adversaire', 'Jeunesse', 'Trump', 'islam', 'OK', 'sec', 'Donald', 'promouvoir', 'module', 'tournage', 'refus', 'réussir', 'présentant', 'end', 'Business', 'Invité', 'disait', 'Management', 'locations', 'Films', 'contacts', 'Jean-Paul', 'vocation', 'Alice', 'bandes', 'news', 'your', 'réussit', 'remercie', 'observation', 'Tony', 'états', 'religieuses', 'prit', 'trucs', 'Localisation', 'fournisseur', 'perso', 'sensible', 'entraîne', 'consacrée', 'Arnaud', 'canadienne', 'municipale', 'quinze', 'localité', 'délais', 'prière', 'Méditerranée', 'Center', 'pensez', 'Activités', 'agglomération', 'cadres', 'smartphone', 'compléter', 'inférieur', 'policiers', 'trouvait', 'Fête', 'aides', 'Grande-Bretagne', 'Vol', 'aire', '1928', 'hectares', 'Julie', 'pédagogique', 'Collectif', 'couvert', 'écologique', 'prestation', 'Sénégal', 'vague', 'Christine', 'réserver', 'impôts', 'garage', 'Route', 'détaillée', 'juridiques', 'due', '1911', 'chevalier', 'naturellement', '98', '2.0', 'pilotes', 'Groupes', 'découvertes', \"y'\", 'av.', 'magie', 'lourd', 'Peut-être', 'êtres', 'complexes', 'analyses', 'aliments', '1901', 'proposées', 'Hélène', 'sixième', 'prisonniers', 'allemands', 'expliqué', 'future', 'console', 'économies', 'isbn', 'comparer', 'pouces', 'Vainqueur', 'étrangère', 'accords', '87', '1910', 'Hors', '94', 'connait', 'Edward', 'comparaison', 'pleinement', 'engager', '96', 'augmente', 'Restaurant', 'pub', 'expert', 'Lac', 'hautes', 'Royaume', 'voisin', 'management', 'régulière', 'jazz', 'essaie', 'spectacles', 'Congo', 'créateur', 'poster', 'exceptionnel', 'shift', 'couper', 'tiré', 'localisation', 'Die', 'concentration', 'Pen', 'paire', 'Animaux', 'commencent', 'contributeurs', 'clientèle', 'registre', 'clôture', 'implique', 'largeur', 'Science', 'farine', 'Inn', 'Hotels.com', 'Fonds', 'longs', 'juifs', 'url', 'RC', 'bijoux', 'contrôler', 'sommeil', 'mit', 'obligations', 'grève', 'sécurisé', 'notice', 'crime', 'initialement', 'clavier', 'soupe', 'UMP', 'répertoire', 'streaming', 'complémentaires', 'Life', 'Disponibilité', 'remonte', 'Sénat', 'Corps', 'Sony', 'flotte', 'Game', 'priorité', 'tonnes', 'sélectionné', 'Yoga', 'manuel', 'milieux', 'Nouveaux', 'tenté', 'proposée', 'Alger', 'Nathalie', 'favoriser', 'facteur', 'Real', 'meubles', 'chinoise', 'ingrédients', 'modifiée', 'évoluer', 'france', 'accessibles', '1925', 'Auguste', 'Jérusalem', 'An', 'aise', 'Bravo', 'ONU', 'enlever', 'foyer', 'Jean-Claude', 'caméra', 'ok', 'Membres', 'ordinaire', 'colonne', 'fiction', 'chronique', 'Claire', '1924', 'administratives', 'spéciaux', 'Panier', 'taxe', 'gardien', 'différences', 'identique', 'douceur', 'artillerie', 'RDV', 'Outre', 'autrefois', 'alerte', 'Annuler', 'hauts', 'maritime', 'peintures', 'Format', 'acide', 'témoignages', 'cycliste', 'panneaux', 'lectures', 'coucher', 'adoption', 'Danemark', 'progression', 'accepté', 'Best', 'professeurs', 'Ford', 'panoramique', 'Entreprise', 'ch', 'Euro', 'pareil', 'drapeau', 'admis', 'confirmé', 'Voyage', 'Dimanche', 'musiques', 'compétence', 'célèbres', 'extraordinaire', 'jouent', 'Group', 'canon', 'J.-C.', 'lune', 'Soyez', 'Carlos', 'maternelle', 'récent', 'Sommaire', '1915', 'boulevard', 'étions', 'constater', 'causes', 'InvitéInvitéSujet', 'investissements', 'tranquille', 'alternative', 'Jean-François', 'CE', 'chances', 'Kit', 'négociations', 'limiter', 'Atlantique', 'Enfants', 'Lot', 'Taille', 'bonus', 'annuelle', 'francs', 'jambes', 'lever', 'pertes', 'stress', 'connaissent', '3ème', 'veuillez', 'quasi', 'Données', 'lunettes', '1912', 'voient', 'habitat', 'fonde', 'Free', 'seules', 'procédures', 'jury', 'Green', 'antique', 'Numéro', 'Jeudi', 'Ukraine', 'nation', 'apparaître', 'garçons', 'Niveau', 'manches', 'riz', 'maîtres', 'hameau', 'ressort', 'récents', 'circonstances', 'québécois', 'rentre', 'newsletter', 'électroniques', 'crimes', 'habitation', 'el', 'all', 'réduite', 'profonde', 'trouvez', 'LED', 'entrées', 'Médaille', 'Naissance', '3e', 'content', 'régler', 'universités', 'peint', 'jusque', 'individuel', 'Jean-Baptiste', 'intervenir', 'Utilisateur', 'blessé', 'maximale', 'téléchargement', 'commander', 'échapper', 'Décembre', 'To', 'sponsoriséSujet', 'souffle', 'Amazon', 'venait', 'pousse', 'plaques', 'ouvriers', 'continent', 'forums', 'terminer', 'auxquels', 'restera', 'britanniques', 'Irak', 'FRANCE', 'trouvant', 'Semaine', 'diagnostic', 'Roland', 'récentes', 'CA', 'caisse', 'imaginer', 'quitté', 'Temple', 'rendent', 'considérer', 'permanence', 'instruction', 'explication', 'contribuer', 'junior', 'Costa', 'tapis', 'Commune', 'Résumé', 'Norvège', 'that', 'atteinte', 'FR', 'Cap', '1927', 'lancée', 'mixte', 'pure', 'micro', 'disponibilitésDu', 'ferroviaire', 'bleue', 'sentiments', 'Fermer', 'vertu', 'mont', 'courts', 'sacré', '130', 'Metz', 'contente', 'séjours', 'universitaires', 'v.', 'sachant', 'resté', 'colonel', 'ménage', 'couvre', 'Utilisation', 'Brown', 'PàS', 'trait', 'ronde', 'officiers', 'Williams', 'oeuvres', 'Hall', 'bisous', 'Test', 'Walter', 'gars', 'serais', 'soutient', 'franchement', 'déposer', 'monastère', 'indice', 'mec', 'Équipe', 'genres', 'identification', '--Présentation', 'Tél', 'Ajoutez', 'accueillant', 'mariée', 'Louise', 'conclusion', 'html', 'interventions', 'précédents', 'destinés', 'abonnement', 'French', 'bouteille', 'abrite', 'communautaire', 'Magazine', 'imagine', 'foule', 'accent', 'citoyen', 'Esprit', 'rappel', 'BMW', 'monsieur', 'trace', 'Public', 'connaitre', 'parfum', 'Mini', 'poèmes', 'réalisées', 'Mathieu', 'culturels', 'Ensemble', 'soutenu', 'Renaissance', 'Eugène', 'spécialisés', 'AS', 'soyez', 'marquée', 'possession', 'Galaxy', 'ml', 's.', 'étudie', 'Bourse', 'geste', 'gâteau', 'Brest', 'grossesse', 'agissait', 'trimestre', 'Charlie', 'School', 'familial', 'joindre', '1913', 'ailes', 'séparation', 'générations', 'réactions', 'obligé', 'Wars', 'Profil', 'cool', 'Maire', 'grosses', 'mine', 'Mobile', 'Construction', 'intéresser', 'occupé', 'intellectuelle', '700', 'room', 'Liens', 'Journée', 'passages', 'Publicité', 'Auvergne', 'évaluer', 'pompe', 'sûrement', 'Finalement', 'cherchez', 'parlent', 'tables', 'tourné', 'classée', 'not', 'centrales', 'vis-à-vis', 'acquérir', 'I.', 'lèvres', 'César', 'London', 'signal', 'actuels', 'Île', 'explications', 'supports', 'prime', 'interprète', 'choisit', 'représenter', 'Magasiner', 'intervient', 'Trouvez', 'Entretien', 'représenté', 'préfecture', 'Manchester', '24h', 'restant', 'Azur', '้', 'sondage', 'Time', 'métrage', 'carbone', '2005Sujet', 'réfugiés', 'Nouvelles', 'nettement', 'Lausanne', 'proposant', 'Chapelle', 'arbitre', 'exercer', 'pouvaient', 'puissent', 'Support', 'testé', 'PARIS', '2010Sujet', 'drôle', 'doté', 'pauvreté', 'usages', 'conformément', 'Scénario', 'posté', 'graves', 'représentations', 'froide', 'oppose', 'Camille', 'permanente', 'littéraires', 'présentées', 'signaler', 'vingtaine', 'intégral', 'dramatique', 'constituée', 'Lundi', 'souligne', 'refaire', 'sonore', 'Reims', 'Modifier', 'spectateurs', 'parvenir', 'arabes', 'Roman', 'Imprimer', 'Ivoire', 'casque', 'Littérature', 'faibles', 'trou', 'suppose', 'Déjà', 'évènement', 'carton', 'Domaine', 'quelqu', 'Avenue', '1922', 'verbe', 'volet', 'diocèse', 'sexuelle', 'avancer', 'futurs', 'animé', 'Texte', 'éthique', 'Linux', 'Sarah', 'matériels', 'inférieure', 'Paru', 'boutons', 'Mardi', 'Juste', 'Dragon', 'universelle', 'Dijon', 'guère', 'Aquitaine', 'observer', 'Lit', 'convaincre', 'meurtre', 'Nintendo', '1916', 'interdiction', 'Kim', 'destin', 'balle', 'Écosse', 'établit', 'voler', 'astéroïde', 'conservé', 'brut', 'opéra', 'cardinal', 'Août', 'passagers', 'Pyrénées', 'Samuel', 'Inc', 'triste', 'verts', 'succession', 'Victoria', 'Bleu', 'incluant', 'Jackson', 'remplace', 'Island', '1923', 'Météo', 'Jour', 'russes', 'abri', 'révolutionnaire', 'aviation', 'puisqu', 'postal', 'communs', 'tube', 'essentielles', 'Deuxième', 'Jeune', 'attends', 'randonnées', 'paradis', 'requête', 'enseignant', 'plaît', 'saisir', 'consultez', 'alertes', 'initiale', 'panneau', 'caractéristique', 'attaquer', 'Recettes', 'commis', 'Caen', 'aile', 'blogs', 'désert', 'Bible', 'Mohamed', 'Section', 'vendeur', 'Hier', 'Little', 'Exposition', 'fonctionner', 'er', 'documentation', 'élaboration', 'chrétienne', 'allé', 'Stock', 'mené', 'précieux', 'supérieures', 'extraits', 'schéma', 'duquel', 'Cameroun', 'internes', 'golf', 'Professeur', 'terrible', 'parisienne', 'Anthony', 'frappe', 'Celle-ci', 'vrais', 'poursuite', 'nationalité', 'officiels', 'cordes', 'fédérale', 'gardé', 'Hervé', 'comprennent', 'intéressé', 'constructions', 'adopter', 'fabricant', 'apparemment', 'civilisation', 'aurai', 'Contactez-nous', 'islamique', 'indiquant', 'feux', 'inutile', 'Machine', 'trés', 'Young', 'Bulletin', 'Contacter', 'parlementaire', 'composants', 'boire', 'couronne', 'bourg', 'agences', 'up', 'poème', 'Roumanie', 'graphiques', 'remarquer', 'fantastique', 'fontsize', 'passés', '̀', 'Alexander', 'indépendants', 'profond', 'publicités', 'Avignon', 'constante', 'Consultez', 'star', 'figures', 'foot', 'épaisseur', 'paraître', 'arguments', 'Allemands', 'tombé', 'introduit', 'Quels', 'sainte', 'magnifiques', 'Toronto', 'volant', 'Angers', 'Légion', 'églises', 'Bar', 'vive', 'rois', 'suivie', 'habite', 'habitant', 'maillot', 'prévenir', 'taxes', 'Malheureusement', 'case', 'cliquer', 'toilette', 'charmant', 'jeter', 'appellation', 'désigné', '1906', 'définit', 'Jim', 'travaillent', 'fiable', 'VII', 'présentés', 'réfléchir', 'chère', 'Kevin', 'argument', 'sportives', 'my', 'Excellent', 'géant', 'produite', 'contribue', 'retrouvent', 'Roy', 'oubliez', 'façons', 'ouverts', 'réserves', 'grecque', 'classés', 'Double', 'pourrais', 'am', 'ouvertes', 'graines', 'Annuaire', 'Laval', 'our', 'host', 'etait', 'sortant', 'Alliance', 'étages', 'voila', 'typique', 'dedans', 'trône', 'profondément', 'battu', 'Maritimes', 'Soit', 'Day', 'souhaitent', 'hein', 'musicien', 'électeurs', 'Mans', 'prévues', 'Administration', 'rapides', 'gueule', 'marins', 'achète', 'devons', 'électorale', 'pratiquement', 'clinique', 'équipage', 'servent', 'spacieux', 'Marques', 'immédiate', 'géographiques', 'insee', 'associée', 'Quelles', 'PME', 'cf.', 'collecte', 'Charlotte', 'valable', 'Editions', 'employeur', 'promo', 'file', 'Boston', 'bateaux', 'dispute', 'revues', 'new', 'couches', 'Fred', 'deviendra', 'coll', 'obligatoires', 'bgcolor', 'traitements', 'verra', 'folie', 'UNE', 'librement', 'rechercher', 'collaborateurs', 'concernés', 'déplacements', 'partagé', 'Technique', 'ya', 'marge', 'powiat', 'escalier', 'Ontario', 'Minimum', 'priori', 'Café', 'manche', 'SNCF', 'Dossier', 'remboursement', 'survie', 'fixation', 'Paiement', 'Six', 'civils', 'W.', 'fournis', 'pensent', 'Lecture', 'zéro', 'séances', 'Mali', 'Vu', 'recommandations', 'Vallée', 'utilité', 'resultat', 'Personne', '1870', 'constate', 'âgé', 'Golf', 'fixé', 'policier', 'provoque', 'ligue', 'onze', 'pot', 'orbite', 'Mercredi', 'more', 'tissus', 'Central', 'néerlandais', 'récupération', 'détruire', 'OFFRE', 'Amsterdam', 'décoré', 'humeur', '1905', 'CV', 'ordinateurs', 'biologique', 'odeur', 'Texas', 'énormément', 'provinces', 'puits', 'Gros', 'septième', 'Obama', '350', 'citron', 'rapporte', 'attendu', 'Sylvie', 'émotions', 'séminaire', 'Né', 'Gilbert', 'vaisseau', 'stages', 'Patrimoine', 'bravo', 'assister', '¨', 'Enseignement', 'Johnson', '97', 'Venez', 'Jean-Luc', 'militants', 'parlant', 'dommages', 'savoir-faire', 'juive', 'artisans', 'Orient', 'estimé', 'satisfaction', 'gentilé', 'ha', 'ventre', 'Fontaine', 'Moulin', 'dormir', 'simplicité', 'tchèque', 'Karl', 'intense', 'chimiques', 'décennies', 'Dame', 'Mission', 'crédits', 'décider', 'fonctionnaires', 'serbe', 'accueillis', 'stable', 'complémentaire', 'universel', 'conquête', 'centaine', 'Allez', 'dépasse', 'philosophe', 'exprime', 'compliqué', 'Beauté', 'excellence', 'Las', 'utilisez', 'fiches', 'preuves', 'Marguerite', 'Stephen', 'plafond', 'drame', 'Trouver', 'recommandé', 'cités', 'haine', 'stay', 'Derniers', 'opérateurs', 'actualités', 'clic', 'abandonné', 'apprécier', 'prochaines', 'exposé', 'cuire', 'cap', 'côtes', 'préserver', 'ballon', 'évaluations', 'procéder', 'correspondent', 'complément', 'Bonsoir', 'marchandises', 'Transport', 'serai', 'disposer', '2017Voir', 'Pages', 'roue', 'Bac', 'SA', 'citation', 'combattre', 'refusé', 'Offre', 'Citation', 'témoin', 'dessert', 'qualifié', 'PSG', 'blanches', 'possèdent', 'probable', 'dirigeant', 'invitons', 'pause', 'pôle', 'adhésion', 'attribué', 'sacs', 'chef-lieu', 'dirigée', 'traditions', 'syndicats', 'manga', 'facture', 'Blanche', 'stratégies', 'heureuse', 'vendus', 'Techniques', 'moral', 'animations', 'issues', 'pensées', 'tailles', 'entraîner', 'Éric', 'Franck', 'étendue', 'forfait', 'hygiène', 'vice-président', '2010Age', 'latine', 'Neuf', 'oeufs', 'cellule', 'conseillé', 'protocole', 'Munich', 'dispositifs', 'anagramme', 'barrage', 'Édouard', 'Up', 'affronter', 'démarrage', 'paris', 'Jean-Louis', 'ferait', 'capables', 'satisfaire', 'communications', 'ingénierie', 'fréquemment', 'bourse', 'traités', 'Aperçu', 'Réalisation', 'actuelles', 'essentielle', 'défini', 'charte', 'serveurs', 'pomme', 'réunions', 'provenance', 'Question', 'catholiques', \"tarifsJusqu'\", 'Historique', 'énergies', 'branches', 'quelconque', 'mousse', 'défis', 'échanger', '0,00', 'horaire', 'apt', 'productions', 'Exemple', 'Johnny', 'TF1', 'Portrait', 'touristes', 'week', 'Statistiques', 'climatique', 'accusé', 'Logement', 'frac', 'époux', 'intéressante', 'canapé', 'Crédit', 'participent', 'rural', 'miroir', 'oublie', 'téléfilm', 'bière', 'correspondance', 'ultime', 'domestiques', 'dégâts', 'gouvernements', 'situées', 'Langue', 'stabilité', 'externes', 'reconnue', 'suspension', 'partiellement', 'gloire', 'majeurs', 'ISBN', 'dévoile', 'instructions', 'photographies', 'immigration', 'Company', 'select', 'institut', 'America', 'papiers', 'exécutif', 'disposent', 'étudié', 'fédération', 'Oise', 'Seul', 'Kong', 'nulle', 'opposé', 'Pôle', 'os', 'Troisième', 'fiscal', 'trajet', 'contribué', 'Brian', 'têtes', 'Gallimard', 'faculté', 'Dark', 'Unitaire', 'médicament', 'qualification', 'chimique', 'certificat', 'Racing', 'héritage', 'Jane', 'talents', 'Award', 'explosion', 'malades', 'confidentialité', 'positive', 'joint', 'sèche', 'ondes', 'nef', 'Carl', 'assistant', 'rond', 'canons', 'exerce', 'notoriété', 'éditeurs', 'URSS', 'plainte', 'idéalement', 'imprimé', 'basket-ball', 'XXX', 'al', 'boutiques', 'bête', 'connues', 'instance', '1907', 'Bay', 'Côté', 'astéroïdes', 'balcon', 'Robin', 'dynastie', 'Finlande', '2011Sujet', 'tués', 'esprits', 'sons', 'Taylor', 'mobilier', 'remonter', 'Jean-Marie', 'réglementation', 'cousin', 'Peugeot', 'Île-de-France', 'routière', 'marié', 'Marque', 'alliés', 'North', 'signer', 'Pinterest', 'vend', 'Celui', 'devaient', 'BTS', 'voyant', 'attirer', 'Suivant', 'nomination', 'digne', 'livré', 'Charte', 'partisans', 'Pacifique', 'habituellement', 'laser', 'circuits', 'délégation', 'Mouvement', 'températures', 'thématique', 'comportements', 'faim', 'réservéDu', 'précédentes', 'élite', 'colonnes', 'reconnaît', 'œufs', 'salarié', 'république', 'ingénieurs', 'ménages', 'merveilleux', 'oreille', 'investir', 'noix', 'vs', 'RSS', 'antenne', 'satisfait', 'Actuellement', 'Reine', 'racines', 'oncle', 'traits', 'motivation', 'Centrale', 'attentat', 'conducteur', 'Grands', 'connaissez', 'imaginaire', 'Contrairement', 'horreur', 'Serbie', 'marcher', 'feront', 'siren', 'filiale', 'Lady', 'relatifs', 'Marketing', 'fermer', 'confirmer', '1908', 'AC', 'T.', 'placée', 'récompense', 'législation', '±', 'ultra', 'fortune', 'Comté', 'traditionnels', 'Power', 'projection', 'moulin', 'Lune', 'bords', 'surpris', 'nécessairement', 'miel', 'hésite', 'Compte', 'remarqué', 'dépasser', 'Images', 'bénéfice', 'Jacob', 'territoriale', 'transmettre', 'iPad', 'have', 'intégralité', 'scrutin', 'compétitions', 'pensais', 'Félix', 'distinction', 'Chili', 'subi', 'préparé', 'réunit', 'naît', 'combinaison', 'réalisations', 'handicap', 'horizon', 'OS', 'FN', 'Oscar', 'orientale', 'Colombie', 'baseball', 'accueillante', 'supprimé', 'filtres', 'trous', 'Virginie', 'Marne', 'Station', 'transforme', 'War', 'pousser', 'Lieu', 'métalliques', 'phare', 'chante', 'fidélité', 'degrés', 'Nicole', 'coûte', 'ordonnance', 'XIXe', 'révolte', 'vies', 'révision', 'Appel', 'dictionnaire', 'Romain', 'sauvegarde', 'Giovanni', 'administrateurs', 'isolation', 'Étienne', 'camps', 'départemental', 'Fleurs', 'Mort', 'Cup', 'primaires', 'grade', 'expansion', 'Classe', 'gmina', 'corruption', 'Hill', 'Chevalier', 'Jean-Jacques', 'domination', 'Prise', 'illustrations', 'entouré', 'litres', 'Garde', 'mg', 'souligné', 'sensibles', 'Jonathan', 'franchise', 'pharmacie', 'High', 'k', 'semblait', 'Besoin', 'manifeste', 'venez', 'terrorisme', 'gentillesse', 'corriger', 'South', 'impériale', 'Suppression', 'artistiques', 'notables', 'Émile', 'Pack', 'masque', 'tue', 'Gratuit', 'Luis', 'nombres', 'sélections', 'Ile', 'Études', 'préférence', 'fausse', 'recul', 'devront', 'associées', 'Opéra', 'immobilière', 'violences', '2011Age', 'AFP', 'thématiques', 'stars', 'conversion', 'carburant', 'tournant', 'apres', 'spacieuse', 'fermés', 'suprême', 'pis', 'annoncer', 'topic', 'FORUM', 'saut', 'justifier', 'celles-ci', 'Doctinaute', 'promotions', 'régionaux', 'abandon', 'ports', 'volontaires', 'Cinq', 'cite', 'ajout', 'récits', 'responsabilités', 'Gaulle', 'susceptibles', 'précédemment', 'reposer', 'lieutenant', 'provisoire', 'DJ', 'sensation', 'sections', 'sanitaire', 'Chaussures', 'prévoir', 'vaisselle', 'Night', 'Occident', 'mentionné', 'consommateur', 'neutre', 'solaires', 'émotion', 'initial', 'mâle', '2017Déjà', 'personnellement', 'intensité', 'constituer', 'Formule', 'intégrée', 'sculpture', 'extrémité', 'parisien', 'Show', 'soldat', 'paragraphe', 'chair', 'boucle', 'Clément', 'banlieue', 'Ier', 'laine', 'voïvodie', 'archevêque', 'Hans', 'mystère', 'recommander', 'plume', 'anime', 'extérieurs', 'continuent', 'Bistro', 'strictement', 'Flash', 'Garantie', 'imprimer', 'data', 'Athènes', 'traditionnelles', 'Découvrir', 'promis', 'statistique', 'mécanisme', 'Tournoi', 'plaine', 'oblige', 'appuie', 'cheminée', 'VTT', 'Barbara', 'Allah', 'poule', 'écrivains', 'installée', 'autorisé', 'évolutions', 'Islam', 'fiche.php', 'fermée', 'pp.', 'Laura', 'renommée', 'requis', 'monté', 'technologique', 'enfer', 'témoins', 'Story', '／', 'jolies', 'cuivre', 'montrant', 'cassini', 'pollution', 'défenseur', 'Petits', 'surfaces', 'élevés', 'tirage', 'Valérie', 'déclarations', 'psychologie', 'XIII', 'volontaire', 'bloqué', 'ampleur', 'Lens', 'facilité', 'cassini.ehess.fr', 'Éducation', 'Sauf', '2,99', 'aperçu', '−', 'Libération', 'chaine', 'mineurs', 'urbanisme', 'doigt', 'imagination', 'quantités', 'symbolique', 'faisons', 'opportunité', 'commissaire', 'finance', 'Xbox', 'concepts', '--Problèmes', 'touché', 'pratiquer', 'baron', 'visibles', 'loup', 'établie', 'aériennes', 'puissante', 'participant', 'phénomènes', 'Concernant', '1896', 'Hong', 'aidé', 'cou', 'voté', 'chimie', 'Champagne', 'catastrophe', 'Début', 'sûre', 'Piscine', 'génial', 'HP', 'Amiens', '101', 'Canton', 'noires', 'styles', 'perspectives', 'attendent', 'Volume', 'remet', 'Certes', 'Video', 'hebdomadaire', 'accidents', 'Auto', 'Boutique', 'Villes', 'attaquant', 'supplément', 'jugé', 'recherchez', 'virtuelle', 'Toulon', 'apport', 'Systèmes', 'Ceux', 'Quoi', 'Commons', 'plait', 'égale', 'assurances', 'découvrez', 'linguistique', 'Analyse', 'invasion', 'robot', 'Long', 'Floride', 'créés', 'Anvers', 'Standard', 'réformes', 'jouant', 'so', 'prochains', 'Siège', 'varie', 'abonner', 'syndicat', 'Petites', 'souple', 'Connectez-vous', 'intitulée', 'anciennement', 'téléphonique', 'box', 'poulet', 'Saint-Pierre', 'bébés', 'spatiale', 'Parking', 'considérés', 'engagements', 'annexe', 'réunis', 'fondateurs', 'salaires', 'toilettes', 'réflexions', 'mauvaises', 'insectes', 'parait', 'couture', 'prouver', 'Photographie', 'précisé', 'Ray', 'choisis', 'gentil', 'effectuée', 'seuil', 'informé', 'apprécie', 'Alan', 'itinéraire', 'clos', 'terrestre', '✉', 'orgue', 'retenir', 'continu', 'dons', 'Alexis', 'devenus', 'Poids', 'descente', 'tabac', 'Wikipedia', 'opus', 'Society', 'debout', 'invitation', 'présidente', 'Rencontre', 'ème', 'problématique', 'illustre', 'insertion', 'poitrine', 'absolue', 'payé', 'apporté', 'Cher', 'College', 'distinguer', 'adhérents', 'ISO', 'Continuer', 'Tweet', 'Secrétaire', 'PSP', 'ruisseau', 'chaise', '1909', 'faune', 'illustration', 'conjoint', 'dose', 'sélectionner', 'one', 'fiscale', 'massage', '1,5', 'bandeau', 'protégé', 'promu', 'Femmes', 'Nouveautés', 'fallu', 'préfet', 'libertés', 'beaux-arts', 'romantique', 'enjeu', 'fibre', 'soeur', 'moi-même', 'accorde', 'Américains', 'curieux', 'subit', 'occasions', 'facebook', 'analyser', 'religions', 'augmenté', 'aussitôt', 'quai', 'Créé', 'Médecin', 'littéralement', 'directrice', 'demandent', 'examens', 'charbon', 'est-elle', 'noblesse', '2012Sujet', 'pleins', 'immédiat', 'historiens', 'neuve', 'Naples', 'Louvre', 'savait', 'écrans', 'juif', 'Abbaye', 'efficaces', 'incontournable', 'spécialité', 'Newsletter', 'investisseurs', 'éliminer', 'tranche', 'Catalogue', 'tiennent', 'cacher', 'gère', 'organes', 'proviennent', 'authentique', 'générique', 'alias', 'adresser', 'acoustique', 'Andrew', 'généraliste', 'studios', 'Biographie', 'financer', 'voyager', 'humidité', 'dépit', 'existait', 'clan', 'correct', 'op', '1793', 'Tel', 'drogue', 'porte-parole', 'sauvages', 'tenues', 'attentats', 'rendant', 'mobilisation', 'soirées', 'show', 'temporaire', 'barbecue', 'inscriptions', 'ref-data4', 'fixer', 'savais', 'adoré', \"Lorsqu'\", 'traversée', 'Arc', 'Libre', '1890', 'suédois', 'oldid', 'compagnon', 'admin', 'Etudes', 'Social', 'exemplaire', 'libérer', 'encontre', 'huiles', 'Wilson', 'Choisissez', 'coupé', 'inclut', 'rédigé', 'filière', 'constaté', 'View', 'paquet', 'essaye', 'dîner', 'ah', 'nice', 'Data', 'juger', 'dieux', 'olive', 'boissons', 'retours', 'phases', 'Manager', '1880', 'égal', 'observations', 'parlement', 'Profitez', 'définitive', 'ref-data8', 'Lewis', 'jouets', 'mécanismes', 'ref-data3', 'délicieux', 'ref-data1', 'ref-data2', 'ref-data5', '105', 'ref-data7', 'ref-data6', 'évolué', 'réside', 'mecs', 'chargement', 'exil', 'dir.', 'destinées', 'circulaire', 'consacre', 'biographie', 'retenu', 'initiatives', 'livrer', 'Lui', 'existant', 'Prague', 'Horaires', 'tort', 'violon', 'Economie', 'dégagée', 'manquer', 'municipales', 'développée', 'coller', 'refuge', 'mange', 'décrire', 'raconter', 'relever', '--Discussions', 'surement', 'papa', 'methodes', 'anonyme', 'Longueur', 'Dan', 'conversation', 'Nouvel', 'parution', 'pédagogiques', 'allée', 'coalition', 'contributeur', 'Lumière', 'shopping', 'Marché', 'Golden', 'révélé', 'amène', 'variables', 'Madagascar', 'sœurs', 'voisine', 'Ressources', 'orthographe', 'nu', 'femelle', 'Surtout', 'offerts', 'consensus', 'composés', 'nomenclatures', 'âgées', 'Manche', 'XIV', 'River', 'CC', 'p.revenumedian', 'Installation', 'Cuba', 'donnera', '2021173', 'Nouvelle-Zélande', 'Secret', 'Papier', 'EST', '2129090', '2123878', 'explorer', 'adversaires', '2129062', '2129059', '2123937', 'rendus', '2129068', '2129076', 'axes', 'noble', 'restes', 'colonie', 'colline', 'Départ', 'garanties', 'entourage', 'Ed', 'Cahiers', 'die', 'francophones', 'géographie', 'frein', 'conte', 'devenant', 'pattes', 'activer', 'p.page2code', 'Lucas', 'faisaient', 'troupe', 'décors', \"avancéeS'\", 'affirmé', 'plate-forme', 'renseignement', 'Notice', 'Info', 'africaine', 'relief', 'appuyer', 'portefeuille', 'tentatives', 'Davis', \"VOIRL'\", 'plongée', 'bis', 'Arabie', 'livret', 'rajouter', 'ref-data9', 'History', 'coordination', 'Annonces', 'phrases', 'partition', 'servant', 'émis', 'SMS', 'courir', 'colle', 'disparaître', 'q', 'Droits', 'fur', 'Mettre', 'Internationale', 'gain', 'transformé', 'wifi', 'TripAdvisor', 'gratuites', 'Martine', 'visibilité', 'sculpteur', 'jean', 'ambition', 'chars', 'bol', 'Rivière', 'susceptible', 'suites', 'réservée', 'rendement', 'expose', 'panne', 'perception', 'trouble', 'For', 'Spa', 'Camping', 'indiqués', 'laissent', '900', '115', 'pc', 'Mémoire', 'systématiquement', 'dirait', 'doctorat', 'terminée', 'text-align', 'Police', 'reportage', 'lacs', 'unies', 'metal', 'pop35', 'Assurance', 'an35', 'if', 'Faculté', 'offerte', 'aquarium', '2534314', 'balade', 'rempli', 'recens35', 'avocats', 'lis', 'Elizabeth', 'sérieusement', 'ambassadeur', '1904', 'confirmation', 'business', 'pop36', 'an36', 'publiées', 'Eh', 'Boîte', 'amoureuse', 'trentaine', '1881', 'Allemand', 'recens36', 'contemporains', 'Lake', 'ICI', 'Tim', 'installés', 'sanitaires', 'amont', 'finition', 'compagnons', 'pop37', 'an37', 'Somme', 'recens37', 'étudiante', 'particulières', 'Points', 'poignée', 'épicerie', 'great', 'vents', 'More', 'Laurence', 'suicide', 'chrétien', 'Langues', 'brillant', 'lourds', 'inspire', 'conçue', 'clocher', 'chapeau', 'mineur', 'planche', 'rotation', 'suffisant', 'abandonner', 'chambresLocation', 'couvrir', 'chirurgie', 'cahier', 'menaces', 'zonages', 'Architecture', 'pop38', 'Règlement', 'an38', 'prenez', 'promenade', 'recens38', 'auxquelles', 'créant', 'tirs', 'souviens', 'noyau', 'travaillant', 'publicitaire', 'Ernest', 'démonstration', 'personnaliser', 'inventaire', 'libéral', 'Médecins', 'Sun', 'Dead', 'continuité', 'Déclaration', 'passées', 'confusion', 'harmonie', 'assaut', 'May', 'eBay', 'armé', 'portugais', 'Marco', 'Nick', 'Girl', 'Européenne', 'remporter', 'Cartes', 'blé', 'sensibilité', 'formats', 'concernent', 'illustré', 'disciplines', 'témoigne', 'courtes', 'Diego', 'abonnés', 'décident', 'assassinat', 'DC', 'définie', 'tablettes', 'mi', 'partielle', 'supporter', 'assurée', 'marquis', 'discret', 'marquer', 'Résistance', 'épargne', 'communal', 'opinions', 'boule', 'symptômes', 'blessures', 'parts', 'convaincu', 'an39', 'pop39', 'K.', 'recens39', 'avère', 'Pâques', 'Librairie', 'ruines', 'days', 'Broché', 'intime', 'commune.asp', 'depcom', 'bombe', 'Valence', 'tubes', 'hâte', 'portraits', 'notions', 'Décoration', 'laver', 'obtention', 'bizarre', 'transparence', 'Times', 'fitness', 'British', 'lourde', 'actu', 'chalet', 'honte', 'interprété', 'reconstruction', 'Chacun', 'variations', 'avez-vous', 'brevet', 'mémoires', 'séquence', 'diriger', 'personnalisé', 'saga', 'provoquer', 'ponts', 'an40', 'Peut', 'distances', 'souffrance', 'organisées', 'Client', 'Madeleine', 'Limoges', 'attire', 'échecs', 'assis', 'Entrée', 'aîné', 'génétique', 'reviendrons', 'prononcé', 'conservateur', 'Saint-Louis', 'savons', 'voyez', 'formidable', 'Brigitte', 'noté', 'Album', 'rangs', 'territoriales', 'can', 'Leurs', 'OU', 'interdite', 'lave', 'couteau', 'vernis', 'chasseurs', '1500', 'monétaire', 'heureusement', 'matches', 'terroristes', 'subir', 'peaux', 'particules', 'Jazz', 'basque', 'perdue', 'Poitiers', 'Notes', 'enquêtes', '---', 'seins', '├', 'offensive', 'time', 'brigade', 'Contacts', 'lots', 'routier', 'Saint-Martin', 'agréables', 'Meilleure', 'Documents', 'cherchent', 'succède', 'diffuser', 'Dentiste', '--Archives', 'pop40', 'accompagnée', 'recens40', 'constat', 'observe', 'espoirs', 'respecte', 'bibliothèques', 'considérable', 'Besançon', 'Hello', 'Taux', 'Marion', 'planification', 'Réserver', 'construits', 'académie', 'tendre', 'partant', 'épée', 'poétique', 'vitesses', '2014-2015', 'propagande', 'caché', 'Coup', 'tomates', 'modules', 'juges', 'profité', 'préférés', 'fabrique', 'Monument', 'XVI', 'Ayant', 'fier', '2012Age', 'raisonnable', 'rassemble', 'charmante', 'Liban', 'Road', '104', 'médicaux', 'originales', 'Traité', 'vestiges', 'moyennes', 'planches', 'cinquante', 'destinations', 'doctrine', 'Réf', 'publicitaires', 'DANS', 'vif', '\\u200e', 'tensions', 'flore', '1800', 'Afghanistan', '1903', 'canaux', 'traduire', 'commentaireCharger', 'black', 'châteaux', 'Media', 'entité', 'Hollywood', 'Rochelle', 'chargés', 'Sac', 'étonnant', 'délégué', 'classification', 'menus', 'contes', 'champignons', 'variés', 'logistique', 'cavalerie', 'mères', 'dirais', 'Lucien', 'trésor', 'prêtres', 'choisissez', 'moines', 'cantons', 'Patrice', 'jette', 'posée', 'Immobilier', 'conformité', 'sénateur', 'rénové', 'tas', 'comptent', 'Fillon', 'Lord', 'tracé', 'créateurs', 'Jimmy', '1860', 'marbre', 'rangement', 'contrôles', 'paysans', 'vélos', 'Quartier', 'amener', 'al.', 'soie', 'concerné', 'gouvernance', 'baignoire', '1902', 'républicain', 'balades', '1886', 'maréchal', 'complexité', 'Havre', 'inconnue', 'identifié', 'Céline', 'Is', 'blessure', 'habitudes', 'coureur', 'Christopher', 'africain', 'originaux', 'rédacteur', 'Pékin', 'System', 'nuages', 'pluriel', 'souverain', 'postés', 'CNRS', 'contiennent', 'renouvellement', 'Extrait', 'flash', 'couverte', 'légitime', 'surnom', 'will', 'légale', 'rigueur', 'amené', 'Gaston', 'Promotion', 'jupe', '1891', 'N.', 'collèges', 'BA', 'allemandes', 'remercier', 'Chemin', 'instar', 'effectuées', 'débutant', 'signification', 'Your', 'solde', '1876', 'VIII', 'rebelles', 'And', 'reposant', 'br', 'vivants', 'usagers', 'were', 'aérien', 'c.', 'cachées', 'an41', 'pop41', 'recens41', 'Hitler', 'Magasin', 'Vosges', 'négatif', 'Rhône-Alpes', 'demandant', 'Olympique', 'invention', 'marchands', 'Libye', 'gagnant', 'inclinaison', 'roses', 'SC', 'métallique', 'Trop', 'fût', 'Series', 'Great', 'payant', 'refuser', 'm.', 'Idéal', 'roulant', 'étendre', 'emballage', 'Molière', 'protégée', '1789', 'combattants', 'bénéfices', 'Amis', 'fixes', 'Finances', 'prochainement', '1830', 'Miller', 'concurrents', 'rurale', 'polémique', 'adolescents', 'contrainte', 'engagée', 'nucléaires', 'perles', 'Vladimir', 'matériau', 'intégrale', 'mélanger', 'remarques', 'départementale', '1848', 'esclaves', 'avancées', 'diminuer', 'affirmer', 'fondamentaux', 'peintres', 'théorique', 'Interview', 'Hauteur', 'figurent', 'menées', 'faciles', 'na', 'légal', 'Observatoire', 'Bad', 'Digital', 'arbitrage', 'protéines', 'annuler', 'purement', 'descriptif', 'arrivés', 'fréquentes', 'maquillage', '--Jeux', 'foie', 'fabriquer', 'coupable', 'mythe', 'chouette', 'donnés', 'blocs', 'pourcentage', 'gestes', 'literie', 'aménagé', '450', 'proprement', 'Sylvain', 'répartis', 'Vieux', '4e', 'gel', 'Jeunes', 'vérification', 'plomb', 'Tunis', 'UNESCO', 'rêver', 'Resort', 'doublé', 'Moto', 'Porto', 'modeste', 'diminution', 'Sir', 'particularité', 'ouvrant', 'Midi', '2013-2014', 'douleurs', 'relevé', 'goûts', 'véritables', 'Gustave', 'relie', 'Western', 'Rendez-vous', 'lampe', 'Supprimer', 'po', 'théories', 'Heureusement', 'larges', 'Télévision', 'renseigner', 'Promotions', '1871', 'Tribunal', 'procureur', 'souhaité', 'FM', 'guematria', 'lycées', 'démission', 'bars', 'Univers', 'couvent', 'lumineuse', 'courante', 'métaux', 'conventions', 'souffre', 'nourrir', 'Montagne', 'pattern', 'dépendance', 'majeures', 'considérablement', 'protège', 'créativité', 'chanter', 'Sortie', 'YouTube', 'engagés', 'régimes', 'organisés', 'OpenEdition', 'apparait', 'gorge', 'devoirs', 'assisté', 'tend', 'Cloud', 'Co', '2009Age', 'conclu', 'effectifs', 'exister', 'carrés', 'Annie', 'représentée', 'racine', 'Ferdinand', 'caractérisée', 'mensuel', 'excès', 'EP', 'pr', 'Près', 'attractions', 'constamment', 'pantalon', 'bénévoles', 'résister', 'had', 'régulier', 'Traitement', 'passionnés', 'colonies', 'Land', 'occidental', 'prétexte', 'médiévale', 'rappelé', 'exploration', 'Huile', 'donna', 'débuté', 'carnet', 'ours', '1895', 'individuelles', 'Dimensions', 'Édition', 'made', 'espérons', '1898', 'consacrer', 'immeubles', 'proportion', 'compromis', 'placés', 'téléphones', 'contraint', 'Géorgie', 'marin', 'chercheur', 'cohérence', 'Pérou', 'fontaine', 'Traduction', 'Matt', 'incident', 'XXe', 'Régime', 'Moins', 'fabricants', 'Cécile', 'retire', 'exact', 'signée', 'scandale', 'résistant', 'pavillon', 'mécaniques', 'Miami', 'désolé', '2015-2016', 'Découverte', 'indien', 'coque', 'accorder', 'Moyen-Orient', 'archéologique', 'Jeff', 'Four', 'Comparer', 'Logo', 'absolu', 'avancé', 'marchand', 'suggestions', '---------------', 'certification', '128', 'Hubert', 'réparer', 'herbe', 'ange', 'laissez', 'supérieurs', 'variétés', 'Ryan', 'rémunération', 'visuel', 'couverts', 'popularité', 'cote', 'Jura', 'musulman', 'guitariste', 'ressource', 'Îles', 'Jean-Marc', 'pouvais', '§', 'falloir', 'Box', 'fauteuil', 'Peinture', 'superbes', \"I'\", 'camion', 'Email', 'Classic', 'Gold', 'Style', 'bordure', 'optimiser', 'Éditeur', 'Games', 'Industrie', 'comptable', 'polonaise', 'Responsable', 'sage', 'Professionnels', 'boîtes', 'della', 'Soin', 'verres', 'bancaires', 'Lionel', 'dames', 'envisager', 'Séjour', 'athlète', 'évêques', 'Justin', 'choisie', 'Heures', 'pack', 'encourager', 'marie', 'By', 'contenir', 'enregistre', 'officielles', 'prendra', 'hjem', 'Ferrari', 'Combien', 'sépare', 'cardiaque', 'moule', 'Space', 'Post', 'Emma', 'Essai', 'inscrite', 'inquiète', 'franc', 'retenue', 'administratifs', 'dés', 'Noire', 'puissants', 'familiales', 'exige', 'surprises', 'Boy', '00ZUn', 'product', 'attraction', 'confie', 'Bruce', 'usines', 'renvoie', 'agriculteurs', 'onde', '3000', 'trio', 'étang', 'Donner', 'faut-il', 'fonctionnelle', 'pension', 'conclure', 'weekend', 'fameuse', 'Palestine', 'galeries', 'Anciens', 'poêle', 'PAS', 'appliquée', 'Midi-Pyrénées', 'Stanley', 'transferts', 'intrigue', 'Campagne', 'renforcement', 'implantation', 'humide', 'Ivan', 'comportant', 'compilation', 'Howard', 'E-mail', 'symboles', 'confié', 'Choisir', 'before', 'défend', 'répression', 'Douglas', 'chic', 'épices', 'manipulation', 'métropole', 'remarquables', 'changeant', 'Rencontres', 'Server', 'leçons', 'Jason', 'imprimante', 'Mondial', 'Discuter', 'Portes', 'alinéa', '220', 'Billy', 'delà', 'Picardie', 'technologiques', 'Agriculture', 'CampagneIdéal', 'originalité', 'nettoyer', 'costume', 'africains', 'Entertainment', 'rendue', 'Prenez', 'Cercle', 'ST', 'suggère', 'appartiennent', 'Vatican', 'per', 'régulation', 'poivre', 'hyper', 'Contrôle', 'brun', 'CP', 'breton', 'UTC', 'psychologique', 'Jean-Michel', 'Mots-clés', 'watch', 'spirituel', 'espérer', 'validation', 'doucement', 'ajoutée', 'Alphonse', 'Préparation', 'courbe', 'rencontrent', '.Le', 'réservoir', 'vagues', 'véritablement', 'réductions', 'alternance', 'Moselle', 'ADN', 'hébergements', 'ascension', 'forteresse', 'ONG', 'Pedro', 'ébauche', 'Marche', 'State', 'métropolitaine', 'aidera', 'CO2', '106', 'casse', '2,5', 'jet', 'traverser', \"p'\", 'Premium', 'emporter', 'coins', 'grandeur', '1899', 'Caisse', 'Oxford', 'communistes', '170', 'résulte', 'stocks', 'Armand', 'index', 'Cadre', 'Aéroport', 'Visite', 'Zurich', 'médiatique', '.jpg', 'Prénom', 'meuble', 'rivières', 'coloris', 'Ton', 'fatigue', 'Recherches', 'multimédia', 'média', 'théologie', 'set', 'urbains', 'athlétisme', 'tempête', 'retiré', 'Johann', 'réels', 'Hôpital', 'estimer', 'sortent', 'chaleureuse', 'Simple', 'Presses', 'saints', 'Andrea', 'sommaire', 'cents', 'Mère', '＼', 'Générales', 'Anderson', '1866', 'GT', 'Sydney', 'Sauvegarder', 'Économie', 'socialistes', 'sélectionnés', 'fuir', 'poil', 'larmes', 'soi-même', 'vocabulaire', 'pilotage', 'love', 'Conseiller', 'Fil', 'animateur', 'Genre', 'colloque', 'réagir', 'jaunes', 'Last', 'démontrer', 'archipel', '240', 'emploie', '1200', 'promet', '1861', 'paramètre', 'Bas', 'Online', '1889', 'alarme', 'sanctions', 'déploiement', 'Alimentation', 'merde', 'favori', '00ZLogement', 'Saint-Jean', 'ensembles', 'divorce', 'lentement', 'manuscrit', 'bloquer', 'mondes', 'passionné', 'pop42', 'majoritairement', 'an42', 'recens42', 'successivement', 'Java', 'Au-delà', 'inspecteur', 'évite', 'Pokémon', 'balles', '1.1', 'libéré', 'écho', 'Fox', 'patience', 'ail', 'solidaire', 'Fiat', 'fumée', 'procurer', 'semblable', 'tombée', 'Vert', 'substance', 'copier', 'bénéficient', 'équipés', 'tribu', 'bla', '▪', 'versant', 'refait', 'défauts', 'aborder', 'cartouche', 'Fax', 'Chinois', 'enregistrés', 'uniques', 'Sept', 'efficacement', 'détention', 'reliant', 'Croatie', 'pré', 'Carlo', 'Baby', 'apprécierez', '2012-2013', 'guides', 'tentent', 'Indonésie', 'Jersey', 'Egypte', 'industries', 'Résultat', 'Figaro', 'sagesse', 'croissant', 'Abonnez-vous', 'formant', 'attaché', 'tunnel', 'Andy', 'espérant', 'statuts', 'attribution', 'Automobile', 'sauter', 'PIB', 'émergence', 'vedette', 'Transports', 'âmes', 'fois-ci', 'Ignace', 'Der', 'cherché', 'Allen', 'climatisation', 'interroger', 'intéressantes', 'attache', 'Casa', '1850', 'Line', '750', 'stationnement', 'Seulement', 'express', 'matelas', 'Lisbonne', 'idéologie', 'fréquente', 'saisie', 'campus', 'age', 'Gironde', 'renommé', 'grand-père', 'affluent', 'Forces', 'Restauration', 'Books', 'coffre', 'curiosité', 'critère', 'monstre', 'anges', 'adoptée', 'conclusions', 'OM', 'évoqué', 'Varsovie', 'fous', 'Thaïlande', 'Gîtes', 'Objets', 'séjourner', 'Orchestre', 'Activité', 'musicales', 'Bulgarie', 'précipitations', 'céréales', 'standards', 'collègue', 'Moteur', 'développeurs', 'envoyés', '1897', 'Hamilton', 'HC', 'basses', 'ouvrier', 'prévus', 'futures', 'multitude', 'Suivez', 'regroupant', 'caractérise', 'héritier', 'Good', 'AUX', 'z', 'détendre', 'BBC', 'Animation', 'Junior', 'Élections', 'assise', 'vignes', 'Instagram', 'Définition', 'diable', 'Tibet', 'inédit', 'General', 'dominante', 'Sélection', 'serre', 'permettrait', 'Institute', 'enseignements', 'www.youtube.com', 'chantiers', 'Infirmiers', 'incapable', 'opposer', 'Résidence', '102', 'stratégiques', 'plastiques', 'Ok', 'master', '←', 'collectifs', 'déficit', 'Productions', 'banc', 'référendum', 'recevrez', 'palette', 'reçoivent', 'huitième', 'dépression', 'descendre', 'Décès', 'chapitres', 'Stage', 'Jordan', 'Gordon', 'Argent', \"T'\", 'Ottawa', 'négociation', 'hop', 'Lion', 'accordé', 'salade', 'chroniques', 'ramener', 'Maxime', 'migrants', 'Second', 'réfrigérateur', 'seigneurs', 'CHF', 'imposé', 'Parfois', 'frigo', 'ignore', 'East', 'robots', 'Fille', 'gravité', 'Isère', 'Mieux', 'Vendée', 'résidences', 'invisible', 'triple', 'batteries', 'Simone', 'complètes', 'dénonce', 'péninsule', 'deja', 'indicateurs', 'gré', 'dessinateur', 'manager', 'saurait', 'exploiter', 'Rugby', 'appuyant', 'balance', 'forumAccueilCréer', 'médicales', 'mentale', '1872', 'chaises', 'formés', 'sculptures', 'Joueur', 'automobiles', 'accessoire', 'étiquette', 'domestique', 'has', 'Saint-Denis', 'intelligent', 'belges', 'milliard', 'syndrome', '17h', 'implication', 'MP', 'textile', 'races', 'Patricia', 'Cabinet', 'Proche', 'Technologies', 'paisible', 'constructeurs', 'anti', 'occupent', 'arrival', 'rumah', 'Nîmes', 'nette', 'transmis', 'concevoir', 'Rhin', 'manqué', 'regrette', 'out', 'bleus', 'Vietnam', 'Arrondissement', 'transféré', 'soumettre', 'promesse', 'lumières', 'Anniversaire', 'fines', 'buteur', 'merveille', 'Problème', 'coule', 'Réservez', 'Boris', 'globalement', 'fosse', 'Kate', 'christianisme', 'uniforme', 'biologie', 'First', 'biodiversité', 'arrêtés', 'bouteilles', 'pertinence', 'souveraineté', 'Solutions', 'treize', 'bulletin', 'qualifiés', 'quinzaine', 'fabriqué', 'Frères', 'Challenge', '1885', 'écologie', 'individuels', 'Francesco', 'indispensables', 'icône', 'essentiels', 'commença', 'Distribution', 'Fantasy', 'apparu', 'massacre', 'préférée', 'portait', 'optimale', 'marais', 'tranquillité', '14h', 'minimale', 'Calais', 'entrepreneurs', 'vieilles', 'gothique', 'amende', 'Morgan', 'Lisa', 'tit', '5e', 'salons', 'Dordogne', 'sondages', 'classées', 'pdf', 'obstacles', 'divisé', 'produisent', 'détermination', 'commerçants', '2016-2017', 'extérieures', 'australien', 'Devant', 'PAR', 'formée', 'Voix', 'Audi', 'Bande', 'Printemps', 'âgés', 'affluence', 'Steven', 'occupée', 'élimination', 'valorisation', 'aimerai', 'probleme', 'messe', 'Casino', 'coach', 'apartment', 'précisions', 'grotte', 'touches', 'spirituelle', 'onglet', 'Conservatoire', 'tournois', 'verser', 'aménagements', 'entretenir', 'voisines', 'exclusion', 'Être', 'médailles', 'sociologie', 'arrêts', 'Tant', 'çà', 'cessé', 'veuve', 'assurent', 'Match', 'Saint-Étienne', 'poussé', 'restait', 'Déco', 'Kelly', 'NBA', 'propreté', 'spécialités', 'sentiers', 'capture', 'révéler', 'agrandir', 'cosy', 'agissant', 'filet', 'fouilles', 'lanceur', 'vêtement', 'élégant', 'méditation', 'abandonne', 'qualifie', 'sois', 'associe', 'affecté', 'autorise', 'italiens', 'List', 'infrastructure', 'Hommes', 'soumise', 'ID', 'pm', 'désirez', 'autorisés', 'Blues', 'Véronique', 'constitutionnel', 'Bébé', 'indices', 'Store', 'nov', 'quarante', 'Couronne', '00ZRoom', '4ème', 'Pharmacie', 'Youtube', 'camarades', 'annulation', 'science-fiction', 'comprise', 'manières', 'boulangerie', 'natale', 'effectués', 'résidents', 'correction', 'partagée', 'solides', 'cave', 'placement', 'difficilement', 'diplômé', 'virtuel', 'masculine', '--Photos', 'commodités', 'PlayStation', 'pseudonyme', 'Spécial', 'Venezuela', 'voulant', 'surveiller', 'reconnus', 'rayonnement', 'pop34', 'coffret', 'Unité', 'mignon', 'puissances', '1893', 'Old', 'fraîche', 'expressions', 'texture', 'People', 'Situation', 'Index', 'inspirée', 'Franz', 'dérivés', 'corde', 'profitez', 'réédition', 'bah', 'Précédent', 'Objet', 'Antiquité', 'économiser', 'soigner', 'assurant', 'an34', 'dignité', 'back', '113', '103', 'Nobel', 'postale', 'TypeEntire', 'festivals', 'Licence', 'beautiful', 'curé', 'Friedrich', 'divisions', 'indications', 'Do', 'cuillère', 'recens34', 'gares', 'climatiques', 'lame', 'tramway', 'visiblement', 'Honda', 'fluide', 'doubles', 'parent', 'varier', 'substances', 'mètre', 'Lambert', 'départementales', 'applicables', 'convertir', 'enveloppe', 'finis', 'Aires', 'Academy', 'Comédie', 'réunir', 'chants', 'extraction', 'hiérarchie', 'devrais', 'progresser', 'distribué', 'Mathématiques', 'Champs', 'Espagnol', 'fibres', 'coco', 'accessibilité', 'étroite', 'qualifier', 'Romains', 'leçon', '2020', 'ES', 'reçus', 'THE', 'folle', 'Japonais', 'immobilières', 'prononciation', 'oct', 'supporters', 'stand', 'Chauffage', 'emporte', 'rassemblement', 'fautes', 'crainte', 'paiements', 'neufs', 'foyers', 'enthousiasme', 'Contre', 'concernées', 'accomplir', 'Plaza', 'magazines', 'blonde', '2011-2012', 'Inter', 'garantit', 'pianiste', 'identiques', 'équipées', 'appellent', 'pile', 'porteur', 'Liberté', 'Autant', 'unes', 'Disneyland', 'validité', 'nocturne', 'légers', 'DS', 'rapprocher', 'acceptation', 'Maman', 'violation', 'Abraham', 'RechercherRésultats', 'Capitaine', 'Galles', 'Coucou', 'contraste', 'spa', 'appliqué', 'rachat', 'Montage', 'tri', 'battant', 'réaliste', 'Voyages', 'Motif', 'parquet', 'isolé', 'D2', 'iOS', 'Will', 'compositions', '--les', 'Bavière', 'clics', '112', 'satellites', 'déplace', 'Hot', '18h', 'Turin', 'potentiels', 'prisonnier', 'détient', 'neveu', 'Grandes', 'citations', 'baignade', 'Canon', 'systématique', 'violent', 'grand-mère', 'Caraïbes', 'Pau', 'Aix', 'Mis', 'Hugues', 'podium', 'secs', 'endémique', 'assiste', 'déguster', 'Sites', 'échappe', 'abusif', 'fiabilité', 'parlementaires', 'Fabrice', 'brésilien', 'Angel', 'pâtes', 'Carter', 'foncé', 'faillite', 'st', 'Batterie', 'permettront', 'mourut', 'motos', 'crises', 'woning', 'intéressants', 'Inscrivez-vous', 'attendait', 'industrielles', 'architectes', 'Yann', 'déçu', 'entré', 'wc', 'Partenaires', 'regardant', 'suffrages', 'Dakar', 'exprimé', 'devise', 'céramique', 'gravure', 'comparateur', 'frappé', 'Chat', 'Elisabeth', 'assiette', 'provoqué', 'IX', 'signifiant', 'dépôts', 'District', 'Wii', 'mystérieux', 'Objectif', 'Seuls', 'conclut', 'voulons', 'pochette', 'souligner', 'parcourir', 'goûter', 'taper', 'sale', '118', 'opposant', 'Zoom', 'sublime', 'Cédric', 'fondamentale', 'peut-on', 'Suites', 'repasser', 'bits', 'een', 'grise', 'séparés', '1894', 'veste', 'lignée', 'good', 'Walt', 'québécoise', 'Mémoires', 'Mariage', 'briques', 'loisir', 'Bilan', 'littoral', 'organe', 'détection', 'Renaud', 'impeccable', 'Magic', 'gendarmerie', 'prouve', 'Coin', 'irlandais', 'Miguel', 'coureurs', 'exécuter', 'Contexte', 'turc', 'baroque', '●', '--Concours', 'ex.', 'lavage', 'fausses', 'secrète', '1892', 'Final', 'Ain', 'av', 'applicable', 'Moore', 'descendants', 'Ball', 'Agenda', 'width', 'magnétique', 'souhait', 'Philippines', 'éponyme', 'clichés', 'trouvés', 'dotée', 'Equipements', 'écris', '2010-2011', 'Application', 'divine', 'registres', 'Ø', 'grain', 'Be', 'nés', 'Ahmed', 'Out', 'people', 'intervenants', 'intérieurs1', 'Colin', 'reservation', 'Etude', 'trophée', 'suggestion', 'lapin', 'restée', 'essor', 'poches', 'séparer', 'records', 'baisser', 'maîtresse', 'Actes', 'rénovée', 'cinématographique', 'sommets', 'dits', 'bulle', 'suffisante', 'Cycle', 'décennie', 'terroriste', 'appelait', 'Pape', 'entrepreneur', 'accueillants', 'Mo', 'open', 'animale', 'reçut', 'Ouverture', 'racisme', 'Augustin', 'restaurer', 'transactions', 'Fernando', 'capitalisme', 'avouer', 'cloud', 'montres', 'regardé', 'extra', 'composant', 'creux', 'envergure', 'Jr', 'minéraux', 'permettait', 'fournie', 'script', 'entités', 'clean', 'Paix', 'ascenseur', 'Bank', 'Philip', 'éteint', 'indienne', 'attribuée', 'maïs', 'Passion', 'job', 'recherché', 'péché', 'formules', 'favorise', 'kr', 'transporter', 'activement', 'dénomination', 'inauguré', 'archéologie', 'copié', 'militant', 'âgée', 'bataillon', 'séparé', 'Eva', 'désire', 'rumeurs', 'Bonaparte', '108', 'détaillé', 'développés', 'Square', 'Confédération', 'Etienne', 'Canadiens', 'richesses', 'Languedoc-Roussillon', 'partent', 'boules', 'domine', 'indicatif', 'pic', 'Hygiène', 'Unies', 'portables', 'miracle', 'costumes', 'marron', 'baiser', 'vivante', 'cirque', 'estimation', 'Nelson', 'animée', 'Di', 'porteurs', '00ZAccueil', 'nid', 'Cadeaux', 'remercions', 'Bell', 'cotation', 'rejet', 'Plage', 'Julia', 'Métiers', 'Party', 'Paulo', 'associer', 'Budapest', 'sanctuaire', 'Réseaux', '1867', 'teint', 'Gary', 'passait', 'déclin', 'collier', 'Corporation', 'Baie', 'référencement', 'détenus', 'épaule', 'Agnès', 'absent', 'enregistrements', 'Ferme', 'entourée', 'team', 'soutenue', 'anneau', 'Bach', 'disposant', 'étoilesà', 'Insee', 'quatorze', 'répondent', 'exacte', 'partagent', 'poils', 'CFA', 'collines', 'maîtriser', 'diffuse', 'organique', 'rédiger', 'viendra', 'Désolé', 'supporte', 'déclarer', 'doré', 'passions', 'légendes', 'Navy', '109', 'processeur', 'observé', 'Autour', 'survivre', 'M6', 'bouger', 'cyclisme', 'Burkina', 'gâteaux', 'favorables', 'flexible', 'confortables', 'sérénité', 'Bluetooth', 'Ski', 'Phil', 'collectivité', 'Russell', '5000', 'pénal', 'philosophique', 'réglage', 'levée', 'Course', 'Syndicat', 'faiblesse', 'prestige', 'obtenus', 'Pratique', 'Bisous', 'Objectifs', 'aval', 'créatures', 'XP', 'gite', 'simultanément', 'Robe', 'marqués', 'parlait', 'estimations', 'hésiter', 'tr', 'Play', 'considèrent', 'aptProperty', 'dragon', 'assassiné', 'Cambridge', 'échéant', 'assemblage', 'ment', 'adopte', 'voudrait', 'civiles', 'Forme', 'sentier', 'anagrammes', 'Adrien', 'opportunités', 'munitions', 'composer', 'Changer', 'Possibilité', '190', 'Coran', 'suisses', 'esclavage', 'infini', 'Main', '1888', 'prof', 'décorée', 'kmHôtels', 'Atlas', 'ira', 'étanchéité', 'sympathiques', 'connectés', 'SI', 'grandement', 'municipaux', 'asiatique', 'Bel', 'acides', 'Britanniques', 'roller', 'ruban', 'Auvergne-Rhône-Alpes', 'privilégié', 'like', 'sortis', 'sensibilisation', 'seize', 'ciblées', \"M'\", 'volontairement', 'Clark', 'Relations', 'encadrement', 'ose', 'Project', 'aisément', 'Franche-Comté', 'Guinée', 'Maine', 'Coffret', '107', 'juridiction', 'doutes', 'déroulement', 'consommer', 'décorations', 'décrite', 'intentions', 'Ortograf', 'hôpitaux', 'variante', 'protégés', 'Roche', 'préface', 'judiciaires', 'Halloween', 'Intel', 'interdire', 'fragile', 'Ardèche', 'Bijoux', 'São', 'smartphones', 'durs', 'défaites', 'fixée', 'pente', 'élégance', '135', 'Hérault', 'herbes', 'amuser', 'aveugle', 'Cyril', 'influences', 'manuscrits', 'condamnation', 'chêne', 'challenge', 'Jennifer', 'exclusif', 'Stars', 'idem', 'Global', 'Mohammed', 'DR', 'pardon', 'détruite', 'réveil', 'google', 'Pétrole', 'académique', 'trouvée', 'carrières', 'Sicile', 'reproduire', 'Vendu', 'leaders', 'compteur', 'écrites', 'sexuel', 'Casablanca', 'affiches', 'Juridique', 'fesses', 'Sources', 'innovations', 'Valls', 'boue', 'Studios', 'pois', 'Citroën', '2009-2010', 'Potter', 'vérifié', 'Laboratoire', 'orthodoxe', 'déc', 'occidentaux', 'capteur', 'précises', 'Champions', 'musulmane', 'VOUS', '1851', 'ヽ', 'Shanghai', 'monstres', 'terroir', '✔', 'remplacée', 'desservie', 'réservations', 'illustrer', 'calculer', 'traductions', 'poussière', 'compétitivité', 'Cathédrale', 'intéressés', 'trouvera', 'Séries', 'spectaculaire', 'valider', 'réuni', 'volontiers', 'Haïti', 'laboratoires', 'Michèle', 'guise', 'copies', 'poussée', 'élue', 'poursuivi', 'résume', 'Devenir', 'chèque', 'AVEC', 'sèches', 'Network', 'récolte', 'Wifi', '111', 'là-bas', 'dépassé', 'Mercedes', 'fournies', 'impulsion', 'sphère', 'lève', 'variantes', 'Wild', 'catch', 'variation', 'Recevez', 'tenait', 'spécifiquement', 'baies', 'approvisionnement', 'relevant', 'ancêtres', 'Islande', 'nobles', 'exclusive', 'lisse', 'carnets', 'Voiture', 'surnommé', 'allié', 'Largeur', 'prestigieux', 'occurrence', 'agression', 'firme', 'perdus', 'Matthieu', 'agenda', 'autel', 'revers', 'lion', 'recense', 'Parallèlement', 'spectateur', 'hongrois', 'problématiques', 'interaction', 'Championship', 'asile', 'améliorations', '1792', 'déterminé', 'nommés', 'Guadeloupe', 'Juliette', 'Située', 'démontre', 'Light', 'automatiques', 'figurant', 'rouler', 'Firefox', 'actionnaires', 'Dave', 'évacuation', 'retraites', 'optimisation', 'maux', '2013Sujet', 'Grégoire', '1875', 'végétaux', 'rapproche', 'mythique', 'visuelle', 'tarte', 'Écrit', 'gestionnaire', 'batailles', 'entretiens', 'adapte', 'modernité', '1878', 'armés', 'réputé', 'Golfe', 'passes', 'Clé', 'Jardins', 'ongles', 'synonyme', 'dispo', 'misère', 'tribus', 'Pop', 'considération', 'défilé', 'célébrer', 'indication', 'éventuelle', 'gardes', 'inauguration', 'indiquée', 'Document', '205', 'mosquée', 'possédant', 'posséder', 'musicaux', 'ambassade', 'pédagogie', 'demi-finale', 'nouveauté', 'humanitaire', 'coupes', 'aube', 'CDI', 'copains', 'Marianne', 'Aisne', 'échantillon', 'admirer', 'Comte', 'mythologie', 'Valley', 'cabine', 'évoquer', 'initiation', 'Clermont', 'traitant', 'boisson', 'Cookies', 'actives', 'Dommage', 'terminal', '--Forum', 'allure', 'Africa', 'Astuces', 'bâti', 'palmarès', 'cherchant', 'internaute', 'vaisseaux', 'répondant', 'tenus', 'serviettes', 'Martinique', 'UA', 'apportent', 'Sacs', 'Souvent', 'vertus', 'Affaire', 'arnaque', '1815', 'vertes', 'continuation', 'fermes', 'clef', 'maintient', '2008-2009', 'Jacqueline', 'den', 'amiral', 'Faut', 'draps', '£', 'créées', '--LA', 'a-t-elle', 'infection', 'concentrer', 'révélation', 'lourdes', 'Critique', 'calculs', 'rap', 'linéaire', 'agisse', 'directive', 'Germain', '1882', 'croise', 'significative', 'amusant', 'ferai', 'mentions', 'Death', 'robes', 'conçus', 'artisan', 'Stone', 'WordPress', 'rez-de-chaussée', 'Foot', 'trompe', 'admission', 'Valentin', 'Religion', 'remplaçant', 'Danse', 'reviens', 'disparaît', 'suspendu', 'variées', '.-', 'automated', 'Edmond', 'massive', 'OTAN', 'States', 'Envie', 'auberge', 'inox', 'comptant', 'Part', 'colonisation', 'vintage', 'tranches', 'reviennent', '¯', 'PLUS', 'Pakistan', 'démo', 'plu', 'faille', 'connaissais', 'saura', 'carrelage', 'élémentaire', 'Raoul', 'Publications', 'bébéEquipements', 'comptabilité', 'Projets', 'biologiques', 'seigneurie', 'canceled', 'dangers', 'approches', 'téléphoniques', 'close', 'Occitanie', 'gay', 'exposés', 'démarrer', 'go', 'intérieurs', 'put', 'statues', 'Thèmes', 'Physique', 'Assistance', 'Tapis', '230', '1887', 'Empereur', 'visuels', 'mentionne', 'illusion', 'dissolution', 'hauteurs', 'positionnement', 'voyageur', 'sérieuse', 'vus', 'Ian', 'piège', 'énormes', 'nue', '--LE', 'Giuseppe', 'animés', '1884', 'prononcer', 'concession', 'Diane', 'tasse', 'Landes', '1.2', 'sonores', 'saveurs', 'éleveurs', 'bacs', 'Avez-vous', 'inventé', 'urbaines', '1856', \"Quelqu'\", 'Raphaël', 'Read', 'envisage', 'évalué', 'procédés', 'JavaScript', 'PVC', 'demi-grand', 'croyances', 'caméras', 'Bio', 'Gaza', 'visité', 'compatibles', 'botanique', 'singulier', 'Arrivée', 'donnait', 'productivité', 'Buenos', '1-0', 'émet', 'ABC', 'Forêt', 't-il', 'baise', 'fonte', 'Vivre', 'coquine', 'écossais', 'Parcours', 'Toyota', 'IT', 'localités', 'tolérance', 'arrivant', '1846', '10h', 'Nation', 'artificielle', 'crochet', 'navette', 'sacrée', '123', 'sensations', 'réelles', 'disais', 'exprimés', 'ridicule', 'baptisé', 'Seuil', 'renouvelables', 'marches', 'causer', 'débutants', 'view', 'chaussée', 'censure', 'documentaires', 'pénale', 'jolis', 'rapporté', 'atouts', 'dégustation', 'existants', 'interactions', 'manteau', 'Marina', 'MP3', 'déposée', 'sont-ils', 'profils', 'baptême', 'élevées', 'diesel', 'apparente', 'arrestation', 'maxi', 'href', 'évoluant', 'Oliver', '1790', 'Ni', 'informe', 'Charente', 'Navigation', 'Fait', 'posts', 'prévision', 'Secrets', 'Ancienne', 'Bâle', 'indiquent', 'alternatives', 'Figure', 'intermédiaires', 'Vichy', 'grains', 'Hégésippe', 'Cœur', 'dérive', 'quarts', 'manquent', 'créent', 'Agent', 'obstacle', 'Reste', 'médiation', 'petit-déjeuner', 'Autorité', 'Tennis', 'nommer', 'im', 'fiscalité', 'Arles', 'attaqué', '--Annonces', 'Aventure', 'croient', 'plomberie', 'los', 'Thème', 'socle', 'Song', 'Queen', '1840', 'alphabet', 'Accéder', 'tantôt', 'diplômes', 'Bâtiment', 'légères', 'épaules', 'coiffure', 'Original', 'madame', 'commercialisation', 'Ross', 'CS', 'quarantaine', 'commissions', 'caisses', 'Autrement', 'bémol', 'plancher', 'appartenance', 'papillon', 'für', 'XI', 'fêter', 'Feu', 'promesses', 'rapprochement', 'indépendantes', 'Circuit', 'Trophée', 'Raison', 'vanille', 'dépannage', 'équation', 'accordée', 'Train', 'spécialisées', 'approuvé', 'Ciel', 'Victoire', 'oxygène', 'mutation', 'blues', 'signalé', 'Guyane', 'plutot', 'composent', 'Singapour', 'poules', 'câbles', 'préférable', 'répétition', 'Almouggar.com', 'Coeur', 'Disque', 'Géographie', 'chatte', 'Beau', 'Clermont-Ferrand', 'géré', 'Parker', 'Town', 'numero', 'accuse', 'enceintes', 'fondamental', 'Sud-Ouest', 'archéologiques', 'PêcheVoir', 'Anjou', 'Poser', 'Constantinople', 'Cliquer', 'autrui', 'entends', 'manoir', 'Princesse', 'vendue', 'XVIIIe', 'passa', 'duplicate', 'ci-après', '--Espace', 'partiel', 'kms', 'Populaire', 'Utiliser', 'recevez', 'casser', 'médiéval', 'valoir', 'tuto', 'manifestants', 'ad', 'coloniale', 'traversé', 'Paradis', 'sexuelles', 'sévère', 'oubli', 'Créez', 'roche', 'mien', 'admissibilité', 'Sud-Est', 'entame', 'tactile', 'alphabétique', 'solitaire', 'Élisabeth', 'adorable', 'commenter', 'XII', 'introduire', 'éducatif', 'Limousin', 'intellectuels', 'diplomatique', 'Saint-Pétersbourg', 'about', 'consultant', 'Wallonie', 'championne', 'correcte', 'Italien', 'dynamiques', 'Monique', 'accompagnés', 'intéressent', 'Beaux-Arts', 'porc', 'Jerry', 'fonctionnalité', 'amies', 'dénoncer', 'instants', 'encyclopédique', 'Opération', 'mat', 'quotidiens', 'Royale', 'positifs', 'Forums', 'maturité', 'mondiaux', 'enseigner', 'organisateurs', 'détriment', '127', 'PSPsexy', 'Vegas', 'Istanbul', 'pots', 'instances', 'puce', 'Revenir', 'signale', 'minuit', 'quoique', 'généralistes', 'matinée', 'pouce', 'renoncer', 'mental', 'budgétaire', 'essayez', 'imposant', 'Back', '--A', 'remercié', 'Meuse', 'achève', 'angles', 'Special', 'Flandre', 'Mail', 'Rica', 'Azure', 'Alberto', 'Malte', 'Who', 'audit', 'intervalle', '117', 'Stéphanie', 'municipalités', 'annoncée', 'épais', 'espérance', 'fonctionnaire', 'dettes', 'technicien', 'ouvrent', 'Gard', 'orale', 'approfondie', 'tomes', 'dessiner', 'basilique', 'Crédits', 'méchant', 'recommandation', 'Batman', 'tribunaux', 'dira', 'vain', 'Jusqu', '--Histoire', 'PDG', 'Hunter', 'escalade', 'tre', 'minorité', 'Rights', 'améliore', 'Universal', 'Rousseau', 'Faso', '--Questions', 'wikipédia', 'localement', 'Parfait', 'athlètes', 'Puy', 'volets', 'Rachel', 'liaisons', 'Métropole', 'SAS', 'Ingénieur', 'Lorient', 'Chantal', 'acheteur', 'lingerie', 'planètes', 'enregistrée', 'Quelque', 'congé', 'confondre', '1883', 'Caire', 'prétend', 'ranger', 'châssis', 'chaos', 'électoral', 'générer', '19h', 'impasse', 'mutuelle', 'radical', 'Prendre', 'icone', 'renouveler', 'périmètre', 'Bush', 'Conception', 'Téléchargez', 'Pauline', 'interroge', 'carrefour', 'Villeneuve', 'regards', 'scénarios', 'Annecy', 'vous-même', 'digital', 'poètes', 'Fonction', 'Bruges', 'possédait', 'entrent', 'cotisations', 'demandeurs', 'convivial', 'moine', 'barrière', '1863', 'résolu', '26Localisation', 'Stratégie', 'SE', 'nuage', 'Ann', 'détour', 'triangle', 'licences', 'Alexandra', '1Sauter', 'intégrés', 'décida', 'Stockholm', 'négocier', 'empreinte', '2013Age', 'mâles', 'Book', 'Gallery', 'désignation', 'segment', 'Posted', 'pourriez', 'sexualité', 'semblables', 'Actu', 'Professionnel', 'flèche', 'semestre', 'Constantin', 'chaudes', 'toiture', 'confluence', 'Retourner', '6e', 'Explorer', 'compact', 'quotidiennement', 'incarne', 'InvitéInvité', 'oeuf', 'démontré', 'Audio', 'allais', 'Marvel', 'buffet', 'coucou', 'Otto', 'DO', 'finissent', 'réguliers', 'alimenter', 'humides', 'discrimination', 'Micro', 'tribune', '♦', 'Comics', 'démocrate', 'Personnellement', 'Damien', 'franchir', 'vigne', 'para', 'gr', 'blason', 'cycles', 'caoutchouc', 'investi', 'saumon', 'consacrés', '9h', 'étendu', 'Aude', 'Roberto', 'cinquantaine', 'Bons', 'préoccupations', 'pirates', 'Equitation', 'rassembler', 'piliers', 'simulation', 'cinéaste', 'pompiers', '-10,5', 'réplique', 'aéronautique', 'guider', 'végétation', 'Full', 'désordre', 'LG', 'Catalogne', 'envoyée', 'Charme', 'Cologne', 'unis', 'pensait', 'complot', 'israélien', 'dictature', 'details', 'free', 'Chronique', 'Sociétés', 'croisière', 'fun', 'représentés', 'Berne', 'porno', 'appliquent', 'accroître', 'Metal', 'profondes', 'cliniques', 'there', 'prototype', 'vraies', '1865', 'Abs', 'Edouard', 'over', 'enchères', 'priorités', 'VOTRE', 'inégalités', 'séduire', 'injection', 'Effectivement', 'passez', 'éclat', 'Salvador', '1862', 'potable', 'synthétique', '11h', 'confier', 'restés', 'chauffer', 'aborde', 'EDF', 'Troyes', 'ressenti', 'jambe'])\n",
            "[-0.0842, -0.0388, 0.0456, -0.0559, -0.0366, 0.0241, 0.0919, -0.0214, 0.0179, -0.1384, -0.0202, -0.1276, -0.0163, 0.0644, -0.1042, 0.0152, -0.0191, 0.0761, -0.0149, 0.0261, 0.0354, -0.077, -0.0034, 0.0941, -0.0169, 0.1621, 0.2469, -0.009, 0.0335, 0.0022, -0.0168, -0.0063, 0.0149, -0.0182, 0.0205, 0.0628, -0.3591, -0.0155, 0.0188, 0.0503, -0.0251, 0.0328, 0.04, 0.0639, -0.1502, 0.1655, 0.0538, 0.0762, -0.1086, -0.0351, 0.0534, 0.0267, 0.0255, 0.038, 0.0026, 0.3703, 0.0797, -0.0189, 0.4854, 0.0882, 0.0483, 0.224, 0.0077, -0.2437, -0.0396, -0.0343, -0.1632, -0.0818, -0.0074, 0.0008, -0.0255, -0.0482, -0.4431, -0.0576, -0.0413, -0.0182, -0.0852, -0.0737, 0.2608, -0.0044, -0.0147, -0.0486, -0.2496, -1.3323, -0.0243, -0.0382, 0.0852, 0.0166, 0.0292, -0.0092, 0.0345, -0.0205, 0.0806, -0.0287, 0.0068, -0.3224, -0.0187, -0.0661, -0.043, 0.4115, 0.021, 0.0019, 0.0826, 0.0753, 0.0254, 0.0634, 0.0524, -0.0342, -0.0224, 0.3635, 0.0102, -0.0121, -0.3234, 0.1405, 0.0347, 0.029, -0.0187, 0.0473, -0.067, 0.0084, -0.0503, -0.0469, -0.1019, 0.1343, -0.0289, 0.0632, 0.0699, 0.0675, 0.0196, -0.0432, 0.0576, 0.0173, 0.0264, 0.0001, 0.026, -0.0262, -0.3346, -0.025, 0.1202, 0.0655, 0.0264, -0.0396, 0.0032, -0.0192, -0.0364, -0.0285, 0.0278, 0.0017, -0.0048, -0.0001, -0.0395, 0.002, -0.1174, 0.0715, 0.0118, -0.0433, 0.0497, -0.0519, 0.0654, -0.0596, 0.006, 0.1493, 0.01, 0.0117, -0.1024, -0.0334, 0.0252, -0.2275, -0.0043, -0.0623, 0.3386, 0.0622, 0.0344, -0.3352, -0.0398, -0.161, -0.0401, -0.2124, 0.0329, 0.0056, -0.0218, -0.007, 0.1279, 0.0429, -0.0155, 0.0529, 0.1669, 0.0851, -0.4496, -0.0199, 0.1243, 0.0296, 0.0625, 0.5931, -0.0495, -0.0263, 0.0038, 0.0456, -0.0591, 0.0706, 0.046, 0.0196, 0.0271, 0.0136, 0.0427, 0.1151, 0.0651, 0.0513, 0.3261, -0.0095, -0.1681, 0.0631, 0.4491, 0.0119, -0.0168, -0.0606, -0.2383, -0.0494, 0.1051, 0.0095, -0.0175, -0.0459, 0.094, 0.0788, 0.0581, -0.0833, 0.0291, 0.0228, 0.004, -0.2135, -0.045, -0.2637, -0.0708, -0.0272, 0.0321, -0.0116, 0.0079, -0.0634, 0.1234, -0.0904, 0.0501, -0.0339, -0.0494, 0.0714, 0.1486, 0.1024, 0.0903, 0.0458, -0.0289, -0.0185, -0.034, 0.0427, -0.033, -0.0147, -0.2744, -0.0971, 0.0208, 0.0127, -0.0412, 0.0009, -0.0658, 0.0333, -0.0383, 0.0523, -0.019, 0.0391, 0.0702, 0.0231, 0.0573, 0.083, -0.1997, -0.0273, -0.0001, 0.002, -0.0557, 0.0669, -0.0026, 0.1349, 0.0173, -0.0312, -0.0388, 0.032, 0.0129, -0.0233, 0.0034, -0.0373, 0.0239, -0.07, 0.0412, 0.0402, 0.0019, -0.0405, -0.0111, -0.0038, 0.008, 0.1887, 0.0118, 0.3069, -0.0106, 0.0579]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the weight matrix\n",
        "\n",
        "NOw we build a matrix over the dataset associating each word to its vector. For each word in dataset’s vocabulary, we check if it is on FastText’s vocabulary:\n",
        "* if yes: load its pre-trained word vector. \n",
        "* else: we initialize a random vector."
      ],
      "metadata": {
        "id": "GTA0vXeevSuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_dim = 300\n",
        "matrix_len = len(vocab)\n",
        "weights_matrix = np.zeros((matrix_len, emb_dim))\n",
        "words_found = 0\n",
        "\n",
        "for i in range(0, len(vocab)):\n",
        "    word = vocab.lookup_token(i)\n",
        "    try: \n",
        "        weights_matrix[i] = vectors[word]\n",
        "        words_found += 1\n",
        "    except KeyError:\n",
        "        weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
        "weights_matrix = torch.from_numpy(weights_matrix)\n",
        "print( weights_matrix.size())"
      ],
      "metadata": {
        "id": "4XXFTaRxvRNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2277e9a8-dfdc-48d8-b7a8-b8acf6b4e806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([43072, 300])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding layer\n",
        "\n",
        "The code below defines a function that builds the embedding layer using the pretrained embeddings."
      ],
      "metadata": {
        "id": "EACvg0Uw7qje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "    num_embeddings, embedding_dim = weights_matrix.size()\n",
        "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    emb_layer.load_state_dict({'weight': weights_matrix}) # <----\n",
        "    if non_trainable:\n",
        "        emb_layer.weight.requires_grad = False\n",
        "\n",
        "    return emb_layer, num_embeddings, embedding_dim"
      ],
      "metadata": {
        "id": "e-miRfkYvZpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model definition\n",
        "\n",
        "Now we can modify our model to add this embedding layer. \n",
        "\n",
        "Note that the embedding bag now takes pre initialized weights. "
      ],
      "metadata": {
        "id": "VcLWQgu877rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedforwardNeuralNetModel2(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim, output_dim, weights_matrix):\n",
        "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
        "        # just always do it in an nn.Module\n",
        "        super(FeedforwardNeuralNetModel2, self).__init__()\n",
        "\n",
        "        # Define the parameters that you will need. \n",
        "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
        "\n",
        "        self.embedding_sum = nn.EmbeddingBag.from_pretrained(  self.embedding.weight, mode='sum')\n",
        "\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "\n",
        "        # Non-linearity\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "\n",
        "        embedded = self.embedding_sum(text, offsets)\n",
        "\n",
        "        # Linear function  # LINEAR\n",
        "        out = self.fc1(embedded)\n",
        "\n",
        "        # Non-linearity  # NON-LINEAR\n",
        "        out = self.sigmoid(out) \n",
        "\n",
        "        # Linear function (readout)  # LINEAR\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "fXOPuCv_vZrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "emb_dim = 300\n",
        "hidden_dim = 64\n",
        "\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "model = FeedforwardNeuralNetModel2(emb_dim, hidden_dim, num_class, weights_matrix).to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1) # <----\n",
        "total_accu = None\n",
        "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(dev_iter, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_iter, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val: # <----\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "metadata": {
        "id": "qgyfNh2UvZuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9141f041-18bd-4e4b-dab7-7b682af9073e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time:  0.26s | valid accuracy    0.448 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time:  0.26s | valid accuracy    0.645 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time:  0.26s | valid accuracy    0.663 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time:  0.27s | valid accuracy    0.572 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time:  0.27s | valid accuracy    0.661 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time:  0.26s | valid accuracy    0.661 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time:  0.25s | valid accuracy    0.661 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time:  0.30s | valid accuracy    0.661 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time:  0.27s | valid accuracy    0.661 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time:  0.27s | valid accuracy    0.661 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.embedding.weight[vocab.lookup_indices(['mauvais'])])"
      ],
      "metadata": {
        "id": "fSg2Lzr-vZwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b66adc2-81c1-4e44-ac18-95c94ef5ef98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.3040,  1.0493,  0.4355,  ..., -0.3605,  0.5034, -1.3554]],\n",
            "       grad_fn=<IndexBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9qEjcsQbvZzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AoTg7YfzvZ2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Mg2CnIfwvZ4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ds9f4gVqvZ74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sDQqOpocvZ-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SGrMMu7IvaAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uCIF74jAv4z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can also explore the embeddings that are created by the architecture. Run the script in interactive mode, and issue the following commands at the python prompt :\n",
        "```\n",
        "m = model.layers[0].get_weights()[0]\n",
        "tp3_utils.calcSim(’mauvais’,  w2i, i2w, m)\n",
        "```\n",
        "The first line extract the embedding matrix from the model, and the second line computes the most similar embeddings for the word 'mauvais', using cosine similarity. Do the results make sense ? Try another word with a positive connotation."
      ],
      "metadata": {
        "id": "qaufg-Bf6Yz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oY4Ot8BZv41l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Nfd6REqEv45v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_iter = AG_NEWS(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "PbYkqv7BtJ3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoVJ18s_oxkn",
        "outputId": "9bdd0324-4d37-43ba-e595-47a75f1f6a31"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import sklearn\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "train_path = \"allocine_train.tsv\"\n",
        "dev_path = \"allocine_dev.tsv\"\n",
        "test_path = \"allocine_test.tsv\"\n",
        "\n",
        "# This will be the size of the vectors reprensenting the input\n",
        "MAX_FEATURES = 5000 \n",
        "\n",
        "# Load train set\n",
        "train_df = pd.read_csv(train_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "    \n",
        "# -- VECTORIZE\n",
        "print(\"Creating features from bag of words...\")  \n",
        "vectorizer = CountVectorizer( analyzer = \"word\", max_features = MAX_FEATURES ) \n",
        "train_data_features = vectorizer.fit_transform(train_df[\"review\"])\n",
        "# -- TO DENSE\n",
        "x_train = train_data_features.toarray()\n",
        "y_train = np.asarray(train_df[\"sentiment\"])\n",
        "print( \"TRAIN:\", x_train.shape )\n",
        "\n",
        "dev_df = pd.read_csv(dev_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "dev_data_features = vectorizer.transform(dev_df[\"review\"])\n",
        "x_dev = dev_data_features.toarray()\n",
        "y_dev = np.asarray(dev_df[\"sentiment\"])\n",
        "print( \"DEV:\", x_dev.shape )\n",
        "\n",
        "test_df = pd.read_csv(test_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "test_data_features = vectorizer.transform(test_df[\"review\"])\n",
        "x_test = test_data_features.toarray()\n",
        "y_test = np.asarray(test_df[\"sentiment\"])\n",
        "print( \"TEST:\", x_test.shape )\n",
        "\n",
        "count_train = x_train.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating features from bag of words...\n",
            "TRAIN: (5027, 5000)\n",
            "DEV: (549, 5000)\n",
            "TEST: (544, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Load the data\n",
        "\n",
        "Note that batch size is chosen here."
      ],
      "metadata": {
        "id": "NKM4vI_bny7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data into TENSORS\n",
        "\n",
        "def load_data( x_train, y_train, x_dev, y_dev, x_test, y_test, batch_size=1 ):\n",
        "    #batch_size = 1 # == no batch\n",
        "    # create Tensor dataset\n",
        "    train_data = TensorDataset(torch.from_numpy(x_train).to(torch.float), torch.from_numpy(y_train))\n",
        "\n",
        "    # dataloaders\n",
        "    # make sure to SHUFFLE your data\n",
        "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "    \n",
        "    dev_data = TensorDataset(torch.from_numpy(x_dev).to(torch.float), torch.from_numpy(y_dev))\n",
        "    dev_loader = DataLoader(dev_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "    test_data = TensorDataset(torch.from_numpy(x_test).to(torch.float), torch.from_numpy(y_test))\n",
        "    test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "    return train_loader, dev_loader, test_loader"
      ],
      "metadata": {
        "id": "VDZsMXoUnyCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOeZCY09o6CV"
      },
      "source": [
        "### 1.3 Neural Network Definition\n",
        "\n",
        "Now we can build our learning model.\n",
        "\n",
        "▶▶**What are the elements that can be changed here?**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOLUTION\n",
        "Note that here you can change: hidden_dim, number of hidden layers, activation function."
      ],
      "metadata": {
        "id": "2ku11PL8payo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvmc-_zqoxvF"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
        "        # just always do it in an nn.Module\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "\n",
        "        # Define the parameters that you will need. \n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Non-linearity\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Linear function  # LINEAR\n",
        "        out = self.fc1(x)\n",
        "\n",
        "        # Non-linearity  # NON-LINEAR\n",
        "        out = self.sigmoid(out) \n",
        "\n",
        "        # Linear function (readout)  # LINEAR\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Training function\n",
        "\n",
        "▶▶**What are the elements that can be changed here?**"
      ],
      "metadata": {
        "id": "DVrIvVbjoxU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOLUTION\n",
        "\n",
        "Here you can change: the total number of epochs, the criterion / loss, the optimizer which also includes the learning rate."
      ],
      "metadata": {
        "id": "JDdIcpBBpdpc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnNx8hZJox3v"
      },
      "source": [
        "# TRAINING\n",
        "def train( model, train_loader, optimizer, num_epochs=5 ):\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, total_acc, total_count = 0, 0, 0\n",
        "        for input, label in train_loader:\n",
        "            # Step1. Clearing the accumulated gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Step 2. Forward pass to get output/logits\n",
        "            outputs = model( input )\n",
        "\n",
        "            # Step 3. Compute the loss, gradients, and update the parameters by\n",
        "            # calling optimizer.step()\n",
        "            # - Calculate Loss: softmax --> cross entropy loss\n",
        "            loss = criterion(outputs, label)\n",
        "            # - Getting gradients w.r.t. parameters\n",
        "            loss.backward()\n",
        "            # - Updating parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulating the loss over time\n",
        "            train_loss += loss.item()\n",
        "            total_acc += (outputs.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "\n",
        "        # Compute accuracy on train set at each epoch\n",
        "        print('Epoch: {}. Loss: {}. ACC {} '.format(epoch, train_loss/count_train, total_acc/count_train))\n",
        "        \n",
        "        total_acc, total_count = 0, 0\n",
        "        train_loss = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQRgrMEael9W"
      },
      "source": [
        "### 1.5 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldDubAPDox5K"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "def evaluate( model, dev_loader ):\n",
        "    predictions = []\n",
        "    gold = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input, label in dev_loader:\n",
        "            probs = model(input)\n",
        "            predictions.append( torch.argmax(probs, dim=1).cpu().numpy()[0] )\n",
        "            gold.append(int(label))\n",
        "\n",
        "    print(classification_report(gold, predictions))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Runing an experiment\n",
        "\n",
        "Run the model as last week and save the score on dev for future comparison."
      ],
      "metadata": {
        "id": "eGJ2WtzFpKgA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E05Ui0G9es1o"
      },
      "source": [
        "### TEST #1\n",
        "\n",
        "▶▶**Describe the setting of the 'default' experiment**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOLUTION\n",
        "\n",
        "**BoW, 5000 features, Batch size: 1, 1 hidden layer, hidden size: 4, activation: sigmoid, learning rate: 0.1, optimizer: SGD, max epochs: 5**"
      ],
      "metadata": {
        "id": "2x0xsRY1sZDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_loader, dev_loader, test_loader = load_data( x_train, y_train, x_dev, y_dev, x_test, y_test, batch_size=1 )"
      ],
      "metadata": {
        "id": "WV9IEMA8oxlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcGyjXbUoxx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4195d579-4ec0-4041-8d2a-550119f89fc5"
      },
      "source": [
        "# Many choices here!\n",
        "VOCAB_SIZE = MAX_FEATURES\n",
        "input_dim = VOCAB_SIZE \n",
        "hidden_dim = 4\n",
        "output_dim = 2\n",
        "\n",
        "learning_rate = 0.1\n",
        "num_epochs = 5\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "optimizer = torch.optim.SGD( model_bow.parameters(), lr=learning_rate )\n",
        "\n",
        "# Train and evaluate\n",
        "\n",
        "train( model_bow, train_loader, optimizer )\n",
        "\n",
        "evaluate( model_bow, dev_loader )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0. Loss: 0.525575307463908. ACC 0.7344340560970758 \n",
            "Epoch: 1. Loss: 0.3772354137683169. ACC 0.8370797692460712 \n",
            "Epoch: 2. Loss: 0.30906129108996105. ACC 0.8687089715536105 \n",
            "Epoch: 3. Loss: 0.26841267688326875. ACC 0.8899940322259797 \n",
            "Epoch: 4. Loss: 0.23668165647298592. ACC 0.9047145414760295 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.86      0.81       230\n",
            "           1       0.89      0.81      0.85       319\n",
            "\n",
            "    accuracy                           0.83       549\n",
            "   macro avg       0.83      0.83      0.83       549\n",
            "weighted avg       0.84      0.83      0.83       549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "169R-kzbTU4E"
      },
      "source": [
        "## 3. Exercises\n",
        "\n",
        "Now, try to change:\n",
        "* Batch size 1 --> 50, 100, 1000\n",
        "* Increase max number of epochs (with best batch size)\n",
        "* Change the size of the hidden layer\n",
        "* Try with 1 additional layers \n",
        "* Change the number of input features\n",
        "\n",
        "How does this affect the loss and the performance of the model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "KmphBx-8TpK3",
        "outputId": "d9f4dc93-ea15-4c8f-f8c2-9da1fcf64244"
      },
      "source": [
        "# -----------------------------------------------------\n",
        "# Change Batch size\n",
        "# -----------------------------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import sklearn\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "\n",
        "train_path = \"allocine_train.tsv\"\n",
        "dev_path = \"allocine_dev.tsv\"\n",
        "test_path = \"allocine_test.tsv\"\n",
        "\n",
        "# This will be the size of the vectors reprensenting the input\n",
        "MAX_FEATURES = 5000 \n",
        "\n",
        "# Load train set\n",
        "train_df = pd.read_csv(train_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "    \n",
        "# -- VECTORIZE\n",
        "print(\"Creating features from bag of words...\")  \n",
        "vectorizer = CountVectorizer( analyzer = \"word\", max_features = MAX_FEATURES ) \n",
        "train_data_features = vectorizer.fit_transform(train_df[\"review\"])\n",
        "# -- TO DENSE\n",
        "x_train = train_data_features.toarray()\n",
        "y_train = np.asarray(train_df[\"sentiment\"])\n",
        "print( \"TRAIN:\", x_train.shape )\n",
        "\n",
        "dev_df = pd.read_csv(dev_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "dev_data_features = vectorizer.transform(dev_df[\"review\"])\n",
        "x_dev = dev_data_features.toarray()\n",
        "y_dev = np.asarray(dev_df[\"sentiment\"])\n",
        "print( \"DEV:\", x_dev.shape )\n",
        "\n",
        "count_train = x_train.shape[0]\n",
        "\n",
        "\n",
        "# -- TO TENSORS\n",
        "# create Tensor dataset\n",
        "train_data = TensorDataset(torch.from_numpy(x_train).to(torch.float), torch.from_numpy(y_train))\n",
        "\n",
        "# dataloaders\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "    \n",
        "dev_data = TensorDataset(torch.from_numpy(x_dev).to(torch.float), torch.from_numpy(y_dev))\n",
        "dev_loader = DataLoader(dev_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "test_data = TensorDataset(torch.from_numpy(x_test).to(torch.float), torch.from_numpy(y_test))\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "# Many choices here!\n",
        "VOCAB_SIZE = MAX_FEATURES\n",
        "input_dim = VOCAB_SIZE \n",
        "hidden_dim = 4\n",
        "output_dim = 2\n",
        "\n",
        "learning_rate = 0.1\n",
        "num_epochs = 5\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the model\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train and evaluate\n",
        "\n",
        "train( model, train_loader )\n",
        "\n",
        "evaluate( model, dev_loader )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating features from bag of words...\n",
            "TRAIN: (5027, 5000)\n",
            "DEV: (549, 5000)\n",
            "TEST: (544, 5000)\n",
            "Epoch: 0. Loss: 0.013512300513054852. ACC 0.6230356077183211 \n",
            "Epoch: 1. Loss: 0.012526845573928405. ACC 0.7189178436443207 \n",
            "Epoch: 2. Loss: 0.011339280912172778. ACC 0.7642729261985279 \n",
            "Epoch: 3. Loss: 0.010151959115334385. ACC 0.8058484185398846 \n",
            "Epoch: 4. Loss: 0.009153070276955475. ACC 0.8273324050129301 \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-fe148935cf8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-60-e625f16bba39>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, dev_loader)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mgold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtCPp1QkBwvn"
      },
      "source": [
        "### Answers: Batch size\n",
        "\n",
        "Increasing the batch size leads to a faster training a degradation in terms of performance\n",
        "\n",
        "see e.g. (Keskar et al. 2016) https://arxiv.org/abs/1609.04836 :\n",
        "* *It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize ... large-batch methods tend to converge to sharp minimizers of the training and testing functions-and as is well known, sharp minima lead to poorer generalization*\n",
        "\n",
        "Trade-off between faster training and generalization ability.\n",
        "\n",
        "People often test typical values of32, 64, 128, 256, 512 and 1024 (but it depends also on the size of the training data, here we have a very small dataset).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoddNfAjDqUK",
        "outputId": "88b4e5eb-a674-43f5-e9fe-4e46e3ba4384"
      },
      "source": [
        "# -----------------------------------------------------\n",
        "# Increase Batch size AND epochs\n",
        "# -----------------------------------------------------\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# create Tensor dataset\n",
        "train_data = TensorDataset(torch.from_numpy(x_train).to(torch.float), torch.from_numpy(y_train))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 100 #no batch, or batch = 1\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow_b8 = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "optimizer = torch.optim.SGD(model_bow_b8.parameters(), lr=learning_rate)\n",
        "\n",
        "train( model_bow_b8, train_loader, num_epochs=10 )\n",
        "\n",
        "evaluate( model_bow_b8, dev_loader )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0. Loss: 0.007128170216228181. ACC 0.502884424109807 \n",
            "Epoch: 1. Loss: 0.006857696894696344. ACC 0.5637557191167695 \n",
            "Epoch: 2. Loss: 0.006673726334542433. ACC 0.6033419534513627 \n",
            "Epoch: 3. Loss: 0.0064349481948972435. ACC 0.6415357071812214 \n",
            "Epoch: 4. Loss: 0.006109899072588285. ACC 0.6803262383131092 \n",
            "Epoch: 5. Loss: 0.005702022247435868. ACC 0.7396061269146609 \n",
            "Epoch: 6. Loss: 0.005377006935082067. ACC 0.7708374776208474 \n",
            "Epoch: 7. Loss: 0.005075840197573226. ACC 0.7885418738810424 \n",
            "Epoch: 8. Loss: 0.004838178296295008. ACC 0.7943107221006565 \n",
            "Epoch: 9. Loss: 0.004533350378755122. ACC 0.8183807439824945 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.75      0.76       230\n",
            "           1       0.82      0.85      0.83       319\n",
            "\n",
            "    accuracy                           0.81       549\n",
            "   macro avg       0.80      0.80      0.80       549\n",
            "weighted avg       0.80      0.81      0.80       549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GDh3cdgTmLt",
        "outputId": "74f2dbf8-3c0a-4054-b98c-f6d64d431189"
      },
      "source": [
        "# -----------------------------------------------------\n",
        "# Change Hidden layer size\n",
        "# -----------------------------------------------------\n",
        "\n",
        "hidden_dim = 3000\n",
        "\n",
        "# create Tensor dataset\n",
        "train_data = TensorDataset(torch.from_numpy(x_train).to(torch.float), torch.from_numpy(y_train))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 1 #no batch, or batch = 1\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow_b8_h50 = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "optimizer = torch.optim.SGD(model_bow_b8_h50.parameters(), lr=learning_rate)\n",
        "\n",
        "train( model_bow_b8_h50, train_loader )\n",
        "\n",
        "evaluate( model_bow_b8_h50, dev_loader )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0. Loss: 2.9019488983119452. ACC 0.6333797493534912 \n",
            "Epoch: 1. Loss: 0.893522029110294. ACC 0.7777998806445195 \n",
            "Epoch: 2. Loss: 0.666859273842346. ACC 0.8265367018102248 \n",
            "Epoch: 3. Loss: 0.4748491511739731. ACC 0.8557787945096479 \n",
            "Epoch: 4. Loss: 0.4348698714605433. ACC 0.8744778197732246 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.84      0.80       230\n",
            "           1       0.88      0.82      0.85       319\n",
            "\n",
            "    accuracy                           0.83       549\n",
            "   macro avg       0.82      0.83      0.83       549\n",
            "weighted avg       0.83      0.83      0.83       549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPztYNLLETVo"
      },
      "source": [
        "## Answer: Hidden layer size\n",
        "\n",
        "Increase the performance in terms of macro F1, maybe? Longer training time.\n",
        "\n",
        "No magic number, that needs to be optimized. In general, a few typical values are tested:  \n",
        "* *Using too few neurons in the hidden layers will result in something called underfitting. Underfitting occurs when there are too few neurons in the hidden layers to adequately detect the signals in a complicated data set.* \n",
        "* *too many neurons in the hidden layers may result in overfitting. Overfitting occurs when the neural network has so much information processing capacity that the limited amount of information contained in the training set is not enough to train all of the neurons in the hidden layers. A second problem [is increasing] the time it takes to train the network.*\n",
        "* \"Rules\"\n",
        "  * *The number of hidden neurons should be between the size of the input layer and the size of the output layer.* \n",
        "  * *The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.* \n",
        "  * *The number of hidden neurons should be less than twice the size of the input layer.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P2c3B29Ll0G"
      },
      "source": [
        "## Change the size of the input layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Il3I-BKSLdJ6",
        "outputId": "b0e0dd5c-0b3d-40b9-8d9b-596073dd0205"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import sklearn\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# This will be the size of the vectors reprensenting the input\n",
        "MAX_FEATURES = 500\n",
        "\n",
        "# Load train set\n",
        "train_df = pd.read_csv(\"allocine_train.tsv\", header=0,\n",
        "                    delimiter=\"\\t\", quoting=3)\n",
        "\n",
        "print(\"Creating features from bag of words...\")  \n",
        "vectorizer = CountVectorizer(\n",
        "    analyzer = \"word\",\n",
        "    max_features = MAX_FEATURES\n",
        ") \n",
        "train_data_features = vectorizer.fit_transform(train_df[\"review\"])\n",
        "\n",
        "x_train = train_data_features.toarray()\n",
        "y_train = np.asarray(train_df[\"sentiment\"])\n",
        "print( \"TRAIN:\", x_train.shape )\n",
        "count_train = x_train.shape[0]\n",
        "\n",
        "\n",
        "# Transform to tensors\n",
        "# create Tensor dataset\n",
        "train_data = TensorDataset(torch.from_numpy(x_train).to(torch.float), torch.from_numpy(y_train))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 1 #no batch, or batch = 1\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "# Process dev data\n",
        "\n",
        "dev = pd.read_csv(\"allocine_dev.tsv\", header=0,\n",
        "                   delimiter=\"\\t\", quoting=3)\n",
        "dev_data_features = vectorizer.transform(dev[\"review\"])\n",
        "x_dev = dev_data_features.toarray()\n",
        "y_dev = np.asarray(dev[\"sentiment\"])\n",
        "print( \"DEV:\", x_dev.shape )\n",
        "\n",
        "dev_data = TensorDataset(torch.from_numpy(x_dev).to(torch.float), torch.from_numpy(y_dev))\n",
        "dev_loader = DataLoader(dev_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating features from bag of words...\n",
            "TRAIN: (5027, 500)\n",
            "DEV: (549, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ni4om7CBQ9LU",
        "outputId": "67dff844-4feb-434d-9ae4-0e6f7ff22645"
      },
      "source": [
        "# -----------------------------------------------------\n",
        "# Change Hidden layer size\n",
        "# -----------------------------------------------------\n",
        "\n",
        "hidden_dim = 50\n",
        "input_dim = 500\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow_500 = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "optimizer = torch.optim.SGD(model_bow_500.parameters(), lr=learning_rate)\n",
        "\n",
        "train( model_bow_500, train_loader )\n",
        "\n",
        "evaluate( model_bow_500, dev_loader )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0. Loss: 0.5523177005176193. ACC 0.7115575890192958 \n",
            "Epoch: 1. Loss: 0.4214836642410772. ACC 0.8012731251243286 \n",
            "Epoch: 2. Loss: 0.37971238855068423. ACC 0.8257409986075194 \n",
            "Epoch: 3. Loss: 0.34775492108362466. ACC 0.8436443206683907 \n",
            "Epoch: 4. Loss: 0.3202604617960704. ACC 0.8567734235130297 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.76      0.75       230\n",
            "           1       0.82      0.80      0.81       319\n",
            "\n",
            "    accuracy                           0.78       549\n",
            "   macro avg       0.78      0.78      0.78       549\n",
            "weighted avg       0.78      0.78      0.78       549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frD2-OqvQEbk"
      },
      "source": [
        "## 4. Continuous bag of words\n",
        "\n",
        "The feature representation we use above – in which a word is either ‘on’ or ‘off’ – works well, but it does not allow the network to learn similarity between words. \n",
        "As we’ll see during the lectures, we can also make use of embeddings (a rich vector representation for each word, that implicitly captures the similarity between words). \n",
        "This is the approach we’ll take here: each individual word is represented by an embedding vector of 50 dimensions. \n",
        "\n",
        "In order to create the representation of a document, we will take all the embeddings of the words that appear in the document, and sum them together (or take their average).\n",
        "So instead of having an input vector of size 5000, we now have an input vector of size 50, that represents the ‘average’, combined meaning of all the words in the document taken together. \n",
        "\n",
        "Crucially,  the  neural  network  will  also  learn  the  embeddings  during  training :  the  embeddings  of  the network are also parameters that are optimized according to the loss function.\n",
        "\n",
        "\n",
        "### Exercise\n",
        "* Open the file *sentiment_cbow.py*, and read through the comments. Make sure you understand what each part of the script is doing.\n",
        "* Run the code. Examine loss and accuracy. Is the result better or worse than the previous network architecture ? Why ?\n",
        "* The architecture in the script does not contain a hidden layer: it directly feeds the input layer (the average  document  embedding)  to  the  output  layer.  Add  a  hidden  layer  of  16  neurons  and  a  relu activation function. How does this change the results ?\n",
        "* We can also explore the embeddings that are created by the architecture. Run the script in interactive mode, and issue the following commands at the python prompt :\n",
        "```\n",
        "m = model.layers[0].get_weights()[0]\n",
        "tp3_utils.calcSim(’mauvais’,  w2i, i2w, m)\n",
        "```\n",
        "The first line extract the embedding matrix from the model, and the second line computes the most similar embeddings for the word 'mauvais', using cosine similarity. Do the results make sense ? Try another word with a positive connotation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwaPnt4ZRVd4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Non-linearity\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Linear function  # LINEAR\n",
        "        out = self.fc1(x)\n",
        "\n",
        "        # Non-linearity  # NON-LINEAR\n",
        "        out = self.sigmoid(out) \n",
        "\n",
        "        # Linear function (readout)  # LINEAR\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mleuckdzCMhF"
      },
      "source": [
        "# Sources\n",
        "* https://www.scholars.northwestern.edu/en/publications/on-large-batch-training-for-deep-learning-generalization-gap-and-\n",
        "* https://arxiv.org/abs/1303.2314\n",
        "* https://mydeeplearningnb.wordpress.com/2019/02/23/convnet-for-classification-of-cifar-10/\n",
        "* Introduction to Neural Networks for Java (second edition) by Jeff Heaton: https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recall\n",
        "\n",
        "Before starting the practical session, let's take a look again at some code in pytorch.\n",
        "\n",
        "## PyTorch tutorial: BoW classifier\n",
        "* From: https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html\n",
        "\n",
        "This is a simple logistic regression, that is we map BoW representations to log probabilities over labels: we pass the input through an affine map and then do log softmax.\n",
        "\n",
        "We compute: log Softmax(Ax+b)"
      ],
      "metadata": {
        "id": "vAUlvTJR4JkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
        "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
        "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
        "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
        "\n",
        "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
        "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
        "\n",
        "# word_to_ix maps each word in the vocab to a unique integer, which will be its\n",
        "# index into the Bag of words vector\n",
        "word_to_ix = {}\n",
        "for sent, _ in data + test_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "print(word_to_ix)\n",
        "\n",
        "VOCAB_SIZE = len(word_to_ix)\n",
        "NUM_LABELS = 2\n",
        "\n",
        "\n",
        "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
        "\n",
        "    def __init__(self, num_labels, vocab_size):\n",
        "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
        "        # just always do it in an nn.Module\n",
        "        super(BoWClassifier, self).__init__()\n",
        "\n",
        "        # Define the parameters that you will need.  In this case, we need A and b,\n",
        "        # the parameters of the affine mapping.\n",
        "        # Torch defines nn.Linear(), which provides the affine map.\n",
        "        # Make sure you understand why the input dimension is vocab_size\n",
        "        # and the output is num_labels!\n",
        "        self.linear = nn.Linear(vocab_size, num_labels)\n",
        "\n",
        "        # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n",
        "        # to worry about that here\n",
        "\n",
        "    def forward(self, bow_vec):\n",
        "        # Pass the input through the linear layer,\n",
        "        # then pass that through log_softmax.\n",
        "        return F.log_softmax(self.linear(bow_vec), dim=1)\n",
        "\n",
        "\n",
        "def make_bow_vector(sentence, word_to_ix):\n",
        "    vec = torch.zeros(len(word_to_ix))\n",
        "    for word in sentence:\n",
        "        vec[word_to_ix[word]] += 1\n",
        "    return vec.view(1, -1)\n",
        "\n",
        "\n",
        "def make_target(label, label_to_ix):\n",
        "    return torch.LongTensor([label_to_ix[label]])\n",
        "\n",
        "\n",
        "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
        "\n",
        "# the model knows its parameters.  The first output below is A, the second is b.\n",
        "# Whenever you assign a component to a class variable in the __init__ function\n",
        "# of a module, which was done with the line\n",
        "# self.linear = nn.Linear(...)\n",
        "# Then through some Python magic from the PyTorch devs, your module\n",
        "# (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters\n",
        "for param in model.parameters():\n",
        "    print(param)\n",
        "\n",
        "# To run the model, pass in a BoW vector\n",
        "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
        "with torch.no_grad():\n",
        "    sample = data[0]\n",
        "    bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
        "    log_probs = model(bow_vector)\n",
        "    print(log_probs)"
      ],
      "metadata": {
        "id": "ucUefxWn4IU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run on test data before we train, just to see a before-and-after\n",
        "with torch.no_grad():\n",
        "    for instance, label in test_data:\n",
        "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "        log_probs = model(bow_vec)\n",
        "        print(log_probs)\n",
        "\n",
        "# Print the matrix column corresponding to \"creo\"\n",
        "print(next(model.parameters())[:, word_to_ix[\"creo\"]])\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Usually you want to pass over the training data several times.\n",
        "# 100 is much bigger than on a real data set, but real datasets have more than\n",
        "# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.\n",
        "for epoch in range(100):\n",
        "    for instance, label in data:\n",
        "        # Step 1. Remember that PyTorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Make our BOW vector and also we must wrap the target in a\n",
        "        # Tensor as an integer. For example, if the target is SPANISH, then\n",
        "        # we wrap the integer 0. The loss function then knows that the 0th\n",
        "        # element of the log probabilities is the log probability\n",
        "        # corresponding to SPANISH\n",
        "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "        target = make_target(label, label_to_ix)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        log_probs = model(bow_vec)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss = loss_function(log_probs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for instance, label in test_data:\n",
        "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "        log_probs = model(bow_vec)\n",
        "        print(log_probs)\n",
        "\n",
        "# Index corresponding to Spanish goes up, English goes down!\n",
        "print(next(model.parameters())[:, word_to_ix[\"creo\"]])"
      ],
      "metadata": {
        "id": "Ygr45_dX7-V_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}