{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP2_masterLiTL_2122.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCHhtzOXQ2po"
      },
      "source": [
        "# TP 2: Linear Algebra and Feedforward neural network\n",
        "Master LiTL - 2021-2022\n",
        "\n",
        "## Requirements\n",
        "In this section, we will go through some code to learn how to manipulate matrices and tensors, and we will take a look at some PyTorch code that allows to define, train and evaluate a simple neural network. \n",
        "The modules used are the the same as in the previous session, *Numpy* and *Scikit*, with the addition of *PyTorch*. They are all already available within colab. \n",
        "\n",
        "## Part 1: Linear Algebra\n",
        "\n",
        "In this section, we will go through some python code to deal with matrices and also tensors, the data structures used in PyTorch.\n",
        "\n",
        "Sources:    \n",
        "* Linear Algebra explained in the context of deep learning: https://towardsdatascience.com/linear-algebra-explained-in-the-context-of-deep-learning-8fcb8fca1494\n",
        "* PyTorch tutorial: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\n",
        "* PyTorch doc on tensors: https://pytorch.org/docs/stable/torch.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3Hk9fJuBVxk"
      },
      "source": [
        "## 1.1 Numpy arrays\n",
        "\n",
        "NumPy’s main object is the homogeneous multidimensional array. It is a table of elements (usually numbers), all of the same type\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2IvCK4gPUAv"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([1,2])\n",
        "print(\"Our input vector with 2 elements:\\n\", x)\n",
        "print( \"x shape:\", x.shape)\n",
        "print( \"x data type\", x.dtype)\n",
        "# Give a list of elements\n",
        "# a = np.array(1,2,3,4)    # WRONG\n",
        "# a = np.array([1,2,3,4])  # RIGHT\n",
        "\n",
        "# Generate a random matrix (with e generator, for reproducible results)\n",
        "rng = np.random.default_rng(seed=42)\n",
        "W = rng.random((3, 2))\n",
        "print(\"\\n Our weight matrix, of shape 3x2:\\n\", W)\n",
        "print( \"W shape:\", W.shape)\n",
        "print( \"W data type\", W.dtype)\n",
        "\n",
        "# Bias, a scalar\n",
        "b = 1\n",
        "\n",
        "# Now, try to multiply\n",
        "h = W.dot(x) + b\n",
        "print(\"\\n Our h layer:\\n\", h)\n",
        "print( \"h shape:\", h.shape)\n",
        "print( \"h data type\", h.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKzJk0aaPUv4"
      },
      "source": [
        "# Useful transformations\n",
        "h = h.reshape((3,1))\n",
        "print(\"\\n h reshape:\\n\", h)\n",
        "print( \"h shape:\", h.shape)\n",
        "\n",
        "h1 = np.transpose(h)\n",
        "print(\"\\n h transpose:\\n\", h1)\n",
        "print( \"h shape:\", h1.shape)\n",
        "\n",
        "h2 = h.T\n",
        "print(\"\\n h transpose:\\n\", h2)\n",
        "print( \"h shape:\", h2.shape)\n",
        "\n",
        "Wt = W.T\n",
        "print(\"\\nW:\\n\", W)\n",
        "print(\"\\nW.T:\\n\", Wt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpIkzqN6PaJR"
      },
      "source": [
        "## numpy code to create identity matrix\n",
        "import numpy as np\n",
        "a = np.eye(4)\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il-lX6VCA7gk"
      },
      "source": [
        "## 1.2 Tensors\n",
        "\n",
        "For neural networks implementation in PyTorch, we use tensors: \n",
        "* a specialized data structure that are very similar to arrays and matrices\n",
        "* used to encode the inputs and outputs of a model, as well as the model’s parameters\n",
        "* similar to NumPy’s ndarrays, except that tensors can run on GPUs or other specialized hardware to accelerate computing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPqpGGZPCRT-"
      },
      "source": [
        "### 1.2.1 Tensor initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaEdsMG6BAh0"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Tensor initialization\n",
        "\n",
        "## from data. The data type is automatically inferred.\n",
        "data = [[1, 2], [3, 4]]\n",
        "x_data = torch.tensor(data)\n",
        "print( \"x_data\", x_data)\n",
        "print( \"data type x_data=\", x_data.dtype)\n",
        "\n",
        "## from a numpy array\n",
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)\n",
        "print(\"\\nx_np\", x_np)\n",
        "print( \"data type, np_array=\", np_array.dtype, \"x_data=\", x_np.dtype)\n",
        "\n",
        "## from another tensor\n",
        "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
        "print(f\"\\nOnes Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
        "\n",
        "## with random values\n",
        "shape = (2, 3,) # shape is a tuple of tensor dimensions\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFDVEZcBCWF_"
      },
      "source": [
        "### 1.2.2 Tensor attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS4TtR9DCJcq"
      },
      "source": [
        "# Tensor attributes\n",
        "tensor = torch.rand(3, 4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu8RM6O7CaKO"
      },
      "source": [
        "### 1.2.3 Move to GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT7n30VpCOzF"
      },
      "source": [
        "# We move our tensor to the GPU if available\n",
        "if torch.cuda.is_available():\n",
        "  tensor = tensor.to('cuda')\n",
        "  print(f\"Device tensor is stored on: {tensor.device}\")\n",
        "else:\n",
        "  print(\"no gpu\")\n",
        "\n",
        "print(tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdqHVRkHCcgq"
      },
      "source": [
        "**If you’re using Colab, allocate a GPU by going to Edit > Notebook Settings.**\n",
        "\n",
        "▶▶ **move to GPU, and re run last cells.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyZPKBvOGsyf"
      },
      "source": [
        "# We move our tensor to the GPU if available\n",
        "if torch.cuda.is_available():\n",
        "  tensor = tensor.to('cuda')\n",
        "  print(f\"Device tensor is stored on: {tensor.device}\")\n",
        "else:\n",
        "  print(\"no gpu\")\n",
        "\n",
        "print(tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8um7SDWGCp8o"
      },
      "source": [
        "### 1.2.4 Tensor operations\n",
        "\n",
        "Doc: https://pytorch.org/docs/stable/torch.html\n",
        "\n",
        "▶▶ **Look at slicing operations.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yLviqmYC3sZ"
      },
      "source": [
        "# Tensor operations: similar to numpy arrays\n",
        "\n",
        "tensor = torch.ones(4, 4)\n",
        "print(tensor)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# TODO: What do you expect?\n",
        "# ---------------------------------------------------------\n",
        "## Slicing\n",
        "print(\"\\nSlicing\")\n",
        "tensor[:,1] = 0 \n",
        "print(tensor)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# TODO: Change the first column with the value in l\n",
        "# ---------------------------------------------------------\n",
        "l =[1.,2.,3.,4.] \n",
        "\n",
        "\n",
        "print(tensor)\n",
        "\n",
        "\n",
        "## Concatenation\n",
        "print(\"\\nConcatenate\")\n",
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)\n",
        "\n",
        "## Multiplication: element_wise\n",
        "print(\"\\nMultiply\")\n",
        "# This computes the element-wise product\n",
        "t2 = tensor.mul(tensor)\n",
        "print(f\"tensor.mul(tensor) \\n {t2} \\n\")\n",
        "# Alternative syntax:\n",
        "t3 = tensor * tensor\n",
        "print(f\"tensor * tensor \\n {t3}\")\n",
        "\n",
        "## Matrix multiplication\n",
        "t4 = tensor.matmul(tensor.T)\n",
        "print(f\"tensor.matmul(tensor.T) \\n {t4} \\n\")\n",
        "# Alternative syntax:\n",
        "t5 = tensor @ tensor.T\n",
        "print(f\"tensor @ tensor.T \\n {t5}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ulTT2k_Hs97"
      },
      "source": [
        "The tensor is stored on CPU by default.\n",
        "\n",
        "▶▶ **Initialize the tensor using *device='cuda'*: where are stored t1, ..., t5?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxW1jtX-GOfd"
      },
      "source": [
        "### 1.2.5 Exercise\n",
        "\n",
        "▶▶ **Compute the tensor h, using the same data for x and W as at the beginning of this TP.**\n",
        "\n",
        "```\n",
        "x = np.array([1,2])\n",
        "rng = np.random.default_rng(seed=42)\n",
        "W = rng.random((3, 2))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwIanFgWD_YJ"
      },
      "source": [
        "# --------------------------------------------------------\n",
        "# TODO: Write the code to compute h = W.x+b\n",
        "# --------------------------------------------------------\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na_tJOnfGDIz"
      },
      "source": [
        "**Note:** when multiplying matrices, we need to have the same data type, e.g. not **x** with *int* and **W** with *float*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lql9bH39G4Mw"
      },
      "source": [
        "# Other notes on tensors\n",
        "\n",
        "## Operations that have a _ suffix are in-place. For example: x.copy_(y), x.t_(), will change x.\n",
        "print(tensor, \"\\n\")\n",
        "tensor.add_(5)\n",
        "print(tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGmy-dtuOtiw"
      },
      "source": [
        "# Part 2: Feedforward Neural Network\n",
        "\n",
        "In this practical session, we will explore a simple neural network architecture for NLP applications ; specifically, we will train a feedforward neural network for sentiment analysis, using the same dataset of reviews as in the previous session.  We will also keep the bag of words representation. \n",
        "\n",
        "\n",
        "Sources:\n",
        "* This TP is inspired by a TP by Tim van de Cruys\n",
        "* https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/\n",
        "* https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
        "* https://medium.com/swlh/sentiment-classification-using-feed-forward-neural-network-in-pytorch-655811a0913f \n",
        "* https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdSyhJqpVczO"
      },
      "source": [
        "## 2.1 Read and load the data\n",
        "\n",
        "First, we need to understand how to use text data. Here we will keep the bag of word representation, as in the previous session. \n",
        "\n",
        "You can find different ways of dealing with the input data. The simplest solution is to use the DataLoader from PyTorch:    \n",
        "* the doc here https://pytorch.org/docs/stable/data.html and here https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "* an example of use, with numpy array: https://www.kaggle.com/arunmohan003/sentiment-analysis-using-lstm-pytorch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "You can also find many datasets for text ready to load in pytorch on: https://pytorch.org/text/stable/datasets.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxRbwziSV_BY"
      },
      "source": [
        "#### 2.1.1 Build BoW vectors\n",
        "\n",
        "The code below allows to use scikit methods you already know to generate the bag of word representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoVJ18s_oxkn"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import sklearn\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# This will be the size of the vectors reprensenting the input\n",
        "MAX_FEATURES = 5000 \n",
        "\n",
        "# Load train and test set\n",
        "train = pd.read_csv(\"allocine_train.tsv\", header=0,\n",
        "                    delimiter=\"\\t\", quoting=3)\n",
        "test = pd.read_csv(\"allocine_test.tsv\", header=0,\n",
        "                   delimiter=\"\\t\", quoting=3)\n",
        "\n",
        "print(\"Creating features from bag of words...\")\n",
        "\n",
        "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
        "# bag of words tool.  \n",
        "vectorizer = CountVectorizer(\n",
        "    analyzer = \"word\",\n",
        "    max_features = MAX_FEATURES\n",
        ") \n",
        "\n",
        "# fit_transform() performs two operations; first, it fits the model\n",
        "# and learns the vocabulary; second, it transforms our training data\n",
        "# into feature vectors. The input to fit_transform should be a list of\n",
        "# strings.\n",
        "train_data_features = vectorizer.fit_transform(train[\"review\"])\n",
        "\n",
        "# output from vectorizer is a sparse array; our classifier needs a\n",
        "# dense array\n",
        "x_train = train_data_features.toarray()\n",
        "\n",
        "# construct a matrix of two columns (one for positive class, one for\n",
        "# negative class) where the correct class is indicated with 1 and the\n",
        "# incorrect one with 0\n",
        "y_train = np.asarray(train[\"sentiment\"])\n",
        "\n",
        "print( \"TRAIN:\", x_train.shape )\n",
        "count_train = x_train.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt00MaMmW1_P"
      },
      "source": [
        "#### 2.1.2 Transform to tensors\n",
        "\n",
        "Now we need to transform our data to tensors, to provide them as input to PyTorch.\n",
        "\n",
        "* **torch.utils.data.TensorDataset(*tensors)**: Dataset wrapping tensors. Take tensors as inputs, obtained via **torch.from_numpy( an numpy array )**. Note: don't forget to transform tensor type to float, with **to(torch.float)** (or cryptic error saying it was expecting long...).\n",
        "* **DataLoader**: \n",
        "\n",
        "```\n",
        "DataLoader(\n",
        "    dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    collate_fn=None,\n",
        "    pin_memory=False,\n",
        " )\n",
        " ```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMLPp3vnoxnG"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# create Tensor dataset\n",
        "train_data = TensorDataset(torch.from_numpy(x_train).to(torch.float), torch.from_numpy(y_train))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 1 #no batch, or batch = 1\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOeZCY09o6CV"
      },
      "source": [
        "## 2.2 Neural Network\n",
        "\n",
        "Now we can build our learning model.\n",
        "\n",
        "For this TP, we're going to walk through the code of a simple feedforward neural network, with one hidden layer.\n",
        "\n",
        "This network takes as input bag of words vectors, exactly as our 'classic' models: each review is represented by a vector of the size the number of tokens in the vocabulary with '1' when a word is present and '0' for the other words. \n",
        "\n",
        "▶▶ **What is the input dimension?**\n",
        "\n",
        "▶▶ **What is the output dimension?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvmc-_zqoxvF"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function ==> W1\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Non-linearity ==> g\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Linear function (readout) ==> W2\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        y = g(x.W1+b).W2\n",
        "        '''\n",
        "        # Linear function  # LINEAR ==> x.W1+b\n",
        "        out = self.fc1(x)\n",
        "\n",
        "        # Non-linearity  # NON-LINEAR ==> h1 = g(x.W1+b)\n",
        "        out = self.sigmoid(out) \n",
        "\n",
        "        # Linear function (readout)  # LINEAR ==> y = h1.W2\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWLDfLGxpBvn"
      },
      "source": [
        "We need to set up the values for the hyper-parameters, and define the form of the loss and the optimization methods.\n",
        "\n",
        "▶▶ **What is the hidden dimension?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcGyjXbUoxx9"
      },
      "source": [
        "# Many choices here!\n",
        "VOCAB_SIZE = MAX_FEATURES\n",
        "input_dim = VOCAB_SIZE \n",
        "hidden_dim = 4\n",
        "output_dim = 2\n",
        "\n",
        "learning_rate = 0.1\n",
        "num_epochs = 5\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15WR_Jdtoxze"
      },
      "source": [
        "# Initialization of the model\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPt_VbCMqoD2"
      },
      "source": [
        "### Training the network\n",
        "\n",
        "▶▶ **Look at the loss after each training step.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnNx8hZJox3v"
      },
      "source": [
        "# Start training\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, total_acc, total_count = 0, 0, 0\n",
        "    for input, label in train_loader:\n",
        "\n",
        "        # Clearing the accumulated gradients\n",
        "        # torch *accumulates* gradients. Before passing in a\n",
        "        # new instance, you need to zero out the gradients from the old\n",
        "        # instance\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits \n",
        "        # = apply all our functions: y = g(x.W1+b).W2\n",
        "        outputs = model( input )\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, label)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        # Here is the way to find how to modify the parameters in\n",
        "        # order to lower the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # -- a useful print\n",
        "        # Accumulating the loss over time\n",
        "        train_loss += loss.item()\n",
        "        total_acc += (outputs.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "\n",
        "    # Compute accuracy on train set at each epoch\n",
        "    print('Epoch: {}. Loss: {}. ACC {} '.format(epoch, train_loss/count_train, total_acc/count_train))\n",
        "        \n",
        "    total_acc, total_count = 0, 0\n",
        "    train_loss = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzMl5wdnqtCW"
      },
      "source": [
        "### Evaluate the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E81LmpAJq2ic"
      },
      "source": [
        "# ---------------------------------------------\n",
        "# TODO: Process the Test data\n",
        "# ---------------------------------------------\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldDubAPDox5K"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "predictions = []\n",
        "gold = []\n",
        "\n",
        "# Disabling gradient calculation is useful for inference, \n",
        "# when you are sure that you will not call Tensor.backward()\n",
        "with torch.no_grad():\n",
        "    for input, label in valid_loader:\n",
        "        probs = model(input)\n",
        "        # argmax: allows to find the class with the higher score\n",
        "        predictions.append( torch.argmax(probs, dim=1).cpu().numpy()[0] ) \n",
        "        gold.append(int(label))\n",
        "\n",
        "print(classification_report(gold, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq-jspmLL387"
      },
      "source": [
        "## 3. Move to GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pydK_h3QLZfO"
      },
      "source": [
        "## 1- Define the device to be used\n",
        "\n",
        "# CUDA for PyTorch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuV1OjAdMHOX"
      },
      "source": [
        "## 2- No change here\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function ==> W1\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Non-linearity ==> g\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Linear function (readout) ==> W2\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        y = g(x.W1+b).W2\n",
        "        '''\n",
        "        # Linear function  # LINEAR ==> x.W1+b\n",
        "        out = self.fc1(x)\n",
        "\n",
        "        # Non-linearity  # NON-LINEAR ==> h1 = g(x.W1+b)\n",
        "        out = self.sigmoid(out) \n",
        "\n",
        "        # Linear function (readout)  # LINEAR ==> y = h1.W2\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7mmAyoPMziY"
      },
      "source": [
        "## 3- Move your model to the GPU\n",
        "\n",
        "# Initialization of the model\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "## ------------ CHANGE HERE -----------------\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANibLgnhL9jU"
      },
      "source": [
        "## 4- Move your data to GPU\n",
        "\n",
        "# Start training\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, total_acc, total_count = 0, 0, 0\n",
        "    for input, label in train_loader:\n",
        "        ## ------------ CHANGE HERE -----------------\n",
        "        input = input.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model( input )\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, label)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulating the loss over time\n",
        "        train_loss += loss.item()\n",
        "        total_acc += (outputs.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "\n",
        "    # Compute accuracy on train set at each epoch\n",
        "    print('Epoch: {}. Loss: {}. ACC {} '.format(epoch, train_loss/count_train, total_acc/count_train))\n",
        "        \n",
        "    total_acc, total_count = 0, 0\n",
        "    train_loss = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSXQF-ViNUH4"
      },
      "source": [
        "# -- 5- Again, move your data to GPU\n",
        "\n",
        "#test = pd.read_csv(\"allocine_test.tsv\", header=0,\n",
        "#                   delimiter=\"\\t\", quoting=3)\n",
        "\n",
        "test_data_features = vectorizer.transform(test[\"review\"])\n",
        "x_test = test_data_features.toarray()\n",
        "y_test = np.asarray(test[\"sentiment\"])\n",
        "\n",
        "print( \"TEST:\", x_test.shape )\n",
        "\n",
        "# create Tensor datasets\n",
        "valid_data = TensorDataset(torch.from_numpy(x_test).to(torch.float), torch.from_numpy(y_test))\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "predictions = []\n",
        "gold = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for input, label in valid_loader:\n",
        "        ## ------------ CHANGE HERE -----------------\n",
        "        input = input.to(device)\n",
        "        #label = label.to(device) not useful for inference\n",
        "\n",
        "        probs = model(input)\n",
        "        predictions.append( torch.argmax(probs, dim=1).cpu().numpy()[0] )\n",
        "        gold.append(int(label))\n",
        "\n",
        "print(classification_report(gold, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}