{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP6_masterLiTL_2122_WSOL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This module allows to remove accents: a fast, but not very clean solution\n",
        "# to standardize the text (in reviews people often forget accents)\n",
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXI2yERddTle",
        "outputId": "7d708493-192b-47ef-eb0e-69615065c5be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 20.5 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 24.9 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 92 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 194 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 204 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 215 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 225 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 5.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TP6: Natural Language Generation\n",
        "\n",
        "In this practical session, we will test a very simple model for text generation based on a neural language model, using a RNN.\n",
        "\n",
        "Here we use the data from the French dataset reviews, but it's not ideal, it's too small.\n",
        "\n",
        "If you have some time, you can also try:\n",
        "- to implement the functions to read the data in sent_recipes.txt that contains sentence split recipes. -These data could be used with a seq2seq model where the goal is to generate the next sentence, each line containing the input and target sentence)\n",
        "- I followed an existing tutorial using movie plots, you can test its code in the notebook here (modify the number of iterations to get better results): https://colab.research.google.com/drive/1ARI_F0RKV-L4GvmyTPPStZWhmqf737D-?usp=sharing using the data here: https://drive.google.com/file/d/1PakdWMKYNyC5-2G_CSlLtkBsHezFpMHJ/view?usp=sharing "
      ],
      "metadata": {
        "id": "AqqSLN94TMTW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xsi5qL2ruObG"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Load the data\n",
        "\n",
        "The code below allows to read the data, as usual, except that we ignore the sentiment label. We also don't use the dev set, so I added the dev and test data to our training set.\n",
        "\n",
        "Here we also lower case the text and do some pre-processing to ignore punctuations, in order to focus on words. "
      ],
      "metadata": {
        "id": "QsEAfX8KXMx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unidecode\n",
        "\n",
        "\n",
        "# read movie data \n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "train_path = \"allocine_train.tsv\"\n",
        "dev_path = \"allocine_dev.tsv\"\n",
        "test_path = \"allocine_test.tsv\"\n",
        "\n",
        "# Load train, dev and test set\n",
        "train_df = pd.read_csv(test_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "dev_df = pd.read_csv(dev_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "test_df = pd.read_csv(test_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "\n",
        "# splits the string sentence by space, we don t need the sentiment label here\n",
        "tokenizer = get_tokenizer( None ) \n",
        "train_iter = []\n",
        "for i in train_df.index:\n",
        "    #train_iter.append( tuple( [train_df[\"sentiment\"][i], train_df[\"review\"][i]] ) )\n",
        "    train_iter.append( train_df[\"review\"][i] )\n",
        "for i in dev_df.index:\n",
        "    train_iter.append( dev_df[\"review\"][i] )\n",
        "for i in test_df.index:\n",
        "    train_iter.append( test_df[\"review\"][i] )\n",
        "\n",
        "print( \"Original first review:\\t\", train_iter[0])\n",
        "\n",
        "# Optional: lower casing \n",
        "train_iter = [review.lower() for review in train_iter]\n",
        "print( \"Lower case:\\t\", train_iter[0])\n",
        "\n",
        "# Optional: remove accents\n",
        "train_iter = [unidecode.unidecode(accented_string) for accented_string in train_iter]\n",
        "print( \"Accents removed:\\t\", train_iter[0])\n",
        "\n",
        "# Optional: clean text, rmove punctuation to focus on alphabet\n",
        "train_iter = [re.sub(\"[^a-z' ]\", \"\", i) for i in train_iter]\n",
        "print( \"Remove punctuation, numbers..:\\t\", train_iter[0])\n",
        "\n",
        "\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\", \"<PAD\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) #simple mapping to self\n",
        "print( \"Test pipeline:\", text_pipeline( train_iter[0] ) )\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print( \"\\nVocab size:\", vocab_size)"
      ],
      "metadata": {
        "id": "1DNMil5O05lh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "788efd93-96e6-4f31-839a-b45d9e7658ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original first review:\t Une grosse daube. La premiere saison etait pas mal, bon y'avait pas mal d'incohérences, mais bon les téléspectateurs sont pas trop regardant donc ça passe ... mais alors la 2e saison !! L'intérêt de l'intrigue de départ s'est envolée forcément, mais on se dit qu'ils vont peut etre trouver une idée qui donne un quelconque intéret à l'histoire. Meme si on emploie des procédés extremes, comme tuer des personnages centraux de la saison 1 pour ne pas s'en encombrer par la suite ... Comme rien ne choque personne on est plus à ça pres ! Tout le long de la saison 2 on se demande où ça va aller, on se dit \"mais qu'est ce que c'est que ce truc ??\", des épisodes font carrément marrer, tellement les scénaristes ont cherché des situations tarabiscotées pour aller là où ils avaient envie d'aller ... Le pompon pour le dernier épisode parce que c'est ENORME !!! A la fin de l'épisode, une phrase m'est sortie spontanément, c'est \" Quel gros caca vraiment\". Donc voilà la saison 3 ce sera sans moi, je me demande comment j'ai tenu toute la saison 2 dis donc ... Quand je vois que des séries avec un vrai scénario comme Day break s'arretent faute d'audimat, et que des bouses pareilles survivent parce que plein de gens regardent, je trouve ça triste ... Enfin bon je m'en fais pas trop, Prison Break finira surement comme Alias, une fin en grand n'importe quoi à la moitié de la derniere saison, pour stopper le ridicule...\n",
            "Lower case:\t une grosse daube. la premiere saison etait pas mal, bon y'avait pas mal d'incohérences, mais bon les téléspectateurs sont pas trop regardant donc ça passe ... mais alors la 2e saison !! l'intérêt de l'intrigue de départ s'est envolée forcément, mais on se dit qu'ils vont peut etre trouver une idée qui donne un quelconque intéret à l'histoire. meme si on emploie des procédés extremes, comme tuer des personnages centraux de la saison 1 pour ne pas s'en encombrer par la suite ... comme rien ne choque personne on est plus à ça pres ! tout le long de la saison 2 on se demande où ça va aller, on se dit \"mais qu'est ce que c'est que ce truc ??\", des épisodes font carrément marrer, tellement les scénaristes ont cherché des situations tarabiscotées pour aller là où ils avaient envie d'aller ... le pompon pour le dernier épisode parce que c'est enorme !!! a la fin de l'épisode, une phrase m'est sortie spontanément, c'est \" quel gros caca vraiment\". donc voilà la saison 3 ce sera sans moi, je me demande comment j'ai tenu toute la saison 2 dis donc ... quand je vois que des séries avec un vrai scénario comme day break s'arretent faute d'audimat, et que des bouses pareilles survivent parce que plein de gens regardent, je trouve ça triste ... enfin bon je m'en fais pas trop, prison break finira surement comme alias, une fin en grand n'importe quoi à la moitié de la derniere saison, pour stopper le ridicule...\n",
            "Accents removed:\t une grosse daube. la premiere saison etait pas mal, bon y'avait pas mal d'incoherences, mais bon les telespectateurs sont pas trop regardant donc ca passe ... mais alors la 2e saison !! l'interet de l'intrigue de depart s'est envolee forcement, mais on se dit qu'ils vont peut etre trouver une idee qui donne un quelconque interet a l'histoire. meme si on emploie des procedes extremes, comme tuer des personnages centraux de la saison 1 pour ne pas s'en encombrer par la suite ... comme rien ne choque personne on est plus a ca pres ! tout le long de la saison 2 on se demande ou ca va aller, on se dit \"mais qu'est ce que c'est que ce truc ??\", des episodes font carrement marrer, tellement les scenaristes ont cherche des situations tarabiscotees pour aller la ou ils avaient envie d'aller ... le pompon pour le dernier episode parce que c'est enorme !!! a la fin de l'episode, une phrase m'est sortie spontanement, c'est \" quel gros caca vraiment\". donc voila la saison 3 ce sera sans moi, je me demande comment j'ai tenu toute la saison 2 dis donc ... quand je vois que des series avec un vrai scenario comme day break s'arretent faute d'audimat, et que des bouses pareilles survivent parce que plein de gens regardent, je trouve ca triste ... enfin bon je m'en fais pas trop, prison break finira surement comme alias, une fin en grand n'importe quoi a la moitie de la derniere saison, pour stopper le ridicule...\n",
            "Remove punctuation, numbers..:\t une grosse daube la premiere saison etait pas mal bon y'avait pas mal d'incoherences mais bon les telespectateurs sont pas trop regardant donc ca passe  mais alors la e saison  l'interet de l'intrigue de depart s'est envolee forcement mais on se dit qu'ils vont peut etre trouver une idee qui donne un quelconque interet a l'histoire meme si on emploie des procedes extremes comme tuer des personnages centraux de la saison  pour ne pas s'en encombrer par la suite  comme rien ne choque personne on est plus a ca pres  tout le long de la saison  on se demande ou ca va aller on se dit mais qu'est ce que c'est que ce truc  des episodes font carrement marrer tellement les scenaristes ont cherche des situations tarabiscotees pour aller la ou ils avaient envie d'aller  le pompon pour le dernier episode parce que c'est enorme  a la fin de l'episode une phrase m'est sortie spontanement c'est  quel gros caca vraiment donc voila la saison  ce sera sans moi je me demande comment j'ai tenu toute la saison  dis donc  quand je vois que des series avec un vrai scenario comme day break s'arretent faute d'audimat et que des bouses pareilles survivent parce que plein de gens regardent je trouve ca triste  enfin bon je m'en fais pas trop prison break finira surement comme alias une fin en grand n'importe quoi a la moitie de la derniere saison pour stopper le ridicule\n",
            "Test pipeline: [11, 434, 760, 3, 89, 28, 149, 10, 108, 75, 6211, 10, 108, 1342, 19, 75, 7, 948, 29, 10, 59, 705, 118, 38, 116, 19, 101, 3, 341, 28, 1148, 2, 204, 2, 271, 522, 4305, 857, 19, 23, 35, 207, 192, 312, 81, 95, 357, 11, 272, 17, 208, 15, 1287, 185, 5, 100, 42, 48, 23, 4268, 13, 5476, 4383, 50, 1442, 13, 45, 2248, 2, 3, 28, 22, 20, 10, 393, 4272, 40, 3, 109, 50, 52, 20, 2885, 506, 23, 14, 24, 5, 38, 1062, 25, 8, 302, 2, 3, 28, 23, 35, 244, 44, 38, 129, 838, 23, 35, 207, 19, 1592, 32, 9, 21, 9, 32, 547, 13, 43, 191, 1098, 2527, 154, 7, 136, 92, 717, 13, 278, 6024, 22, 838, 3, 44, 87, 897, 237, 1223, 8, 5394, 22, 8, 371, 60, 150, 9, 21, 601, 5, 3, 76, 2, 232, 11, 3210, 3132, 1008, 5942, 21, 367, 184, 2238, 41, 118, 194, 3, 28, 32, 470, 53, 104, 12, 54, 244, 169, 39, 712, 174, 3, 28, 1499, 118, 80, 12, 426, 9, 13, 66, 36, 15, 195, 62, 50, 4054, 130, 1844, 1518, 3980, 4, 9, 13, 2862, 2595, 2750, 150, 9, 412, 2, 343, 2109, 12, 67, 38, 950, 133, 75, 12, 1153, 965, 10, 59, 93, 130, 4432, 527, 50, 427, 11, 76, 16, 131, 410, 143, 5, 3, 613, 2, 3, 533, 28, 22, 5950, 8, 392]\n",
            "\n",
            "Vocab size: 8788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare sequences\n",
        "\n",
        "Here, we will give our model fixed-length sequences (n-grams), with the length of the sequences as an hyper-parameter that can be changed. \n",
        "\n",
        "Another option would be to work directly on sentences (with padding to deal with sequences of different lengths)."
      ],
      "metadata": {
        "id": "VMQ9Ye_Z25fY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create sequences of length 5 tokens\n",
        "def create_seq(text, seq_len = 5):\n",
        "    sequences = []\n",
        "    text = text.strip()\n",
        "    # if the number of tokens in 'text' is greater than 5\n",
        "    if len(text.split()) > seq_len:\n",
        "      for i in range(seq_len, len(text.split())):\n",
        "        # select sequence of tokens\n",
        "        seq = text.split()[i-seq_len:i+1]\n",
        "        # add to the list\n",
        "        sequences.append(\" \".join(seq))\n",
        "      return sequences\n",
        "    # if the number of tokens in 'text' is less than or equal to 5\n",
        "    else:\n",
        "      return [text]"
      ],
      "metadata": {
        "id": "plXGnAMN25rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seqs = [create_seq(i, seq_len = 5) for i in train_iter]\n",
        "# merge list-of-lists into a single list\n",
        "seqs = sum(seqs, [])\n",
        "# count of sequences\n",
        "len(seqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAw-eGi33mr2",
        "outputId": "5d525060-5413-480f-ef0f-69b22bc454ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "105806"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input and target data\n",
        "\n",
        "Now we create input and target sequences from our training data: the target is simply the sequence following the input one. This way, our model starts with an input word, the first in the input sequence, and tries to predict the next token, until the last of the target sequence. For example: \n",
        "* input: Une grosse daube. La premiere \n",
        "* target: grosse daube. La premiere saison\n",
        "\n",
        "A cleaner solution is to segment into sentences and add special characters signaling a start and end of a sequence. "
      ],
      "metadata": {
        "id": "nRiA9K474RHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create inputs and targets (x and y)\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for s in seqs:\n",
        "  if len(s.split()[:-1]) != 0:\n",
        "    x.append(\" \".join(s.split()[:-1]).strip())\n",
        "    y.append(\" \".join(s.split()[1:]).strip())\n",
        "print( x[0])\n",
        "print(y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8akGHRNs4RSd",
        "outputId": "f4ba4ba7-6015-45ec-e2f5-f961a3299a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "une grosse daube la premiere\n",
            "grosse daube la premiere saison\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Map to integer\n",
        "\n",
        "Now we map our token sequences to integer lists. Here we also add padding when necessary."
      ],
      "metadata": {
        "id": "QhMsWehh4ZwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_integer_seq(seq, max_len=5):\n",
        "  int_seq = text_pipeline(seq)\n",
        "  while len(int_seq)!=max_len:\n",
        "      int_seq.append(vocab.lookup_indices([\"<PAD>\"])[0])\n",
        "  return int_seq\n",
        "\n",
        "# convert text sequences to integer sequences\n",
        "x_int = [get_integer_seq(i) for i in x]\n",
        "y_int = [get_integer_seq(i) for i in y]\n",
        "\n",
        "# convert lists to numpy arrays\n",
        "x_int = np.array(x_int)\n",
        "y_int = np.array(y_int)\n",
        "\n",
        "print(x_int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxNHC2-b4Z6u",
        "outputId": "03d3c0b5-eefb-4e45-8c76-c390480e9375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  11  434  760    3   89]\n",
            " [ 434  760    3   89   28]\n",
            " [ 760    3   89   28  149]\n",
            " ...\n",
            " [  67   84   59 2392    5]\n",
            " [  84   59 2392    5    3]\n",
            " [  59 2392    5    3 1298]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batches\n",
        "\n",
        "Batches are simply list of sequences."
      ],
      "metadata": {
        "id": "BXuKn3gAZBW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(arr_x, arr_y, batch_size):\n",
        "    # iterate through the arrays\n",
        "    prv = 0\n",
        "    for n in range(batch_size, arr_x.shape[0], batch_size):\n",
        "      x = arr_x[prv:n,:]\n",
        "      y = arr_y[prv:n,:]\n",
        "      prv = n\n",
        "      yield x, y"
      ],
      "metadata": {
        "id": "DRt8bB105X8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Model definition\n",
        "\n",
        "The model is very similar to chat we've seen until now.\n",
        "\n",
        "▶▶ **Write the '__init__(..) part, using and embedding layer and an LSTM**"
      ],
      "metadata": {
        "id": "xXx-n-lmZLOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WordLSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n",
        "        super().__init__()\n",
        "\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        self.emb_layer = nn.Embedding(vocab_size, 200)\n",
        "\n",
        "        ## define the LSTM\n",
        "        self.lstm = nn.LSTM(200, n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        ## define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## define the fully-connected layer\n",
        "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "        ## pass input through embedding layer\n",
        "        embedded = self.emb_layer(x)     \n",
        "        ## Get the outputs and the new hidden state from the lstm\n",
        "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
        "        ## pass through a dropout layer\n",
        "        out = self.dropout(lstm_output)\n",
        "        #out = lstm_output  \n",
        "        out = out.reshape(-1, self.n_hidden) \n",
        "        ## put \"out\" through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        # if GPU is available\n",
        "        if (torch.cuda.is_available()):\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        # if GPU is not available\n",
        "        else:\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "tKUHl8fxusSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training function is also very similar to what we saw before. "
      ],
      "metadata": {
        "id": "AUy2wyo_Z3vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
        "    # optimizer\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    # loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # push model to GPU\n",
        "    net.cuda()\n",
        "    counter = 0\n",
        "    net.train()\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        for x, y in get_batches(x_int, y_int, batch_size):\n",
        "            counter+= 1\n",
        "            # convert numpy arrays to PyTorch arrays\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            # push tensors to GPU\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            # detach hidden states\n",
        "            # https://discuss.pytorch.org/t/solved-why-we-need-to-detach-variable-which-contains-hidden-representation/1426/4\n",
        "            h = tuple([each.data for each in h])\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(-1))\n",
        "            # back-propagate error\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            # update weigths\n",
        "            opt.step()            \n",
        "            if counter % print_every == 0:\n",
        "              print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                    \"Step: {}...\".format(counter))"
      ],
      "metadata": {
        "id": "zdxK4FqVusWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict\n",
        "\n",
        "The predict function takes as input some tokens: the model needs a first input to predict the next tokens. The functions below can be used to generate some text based on an input sequence. "
      ],
      "metadata": {
        "id": "oYVCRuaaa0va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict next token\n",
        "def predict(net, tkn, h=None):\n",
        "  # tensor inputs \n",
        "  x = np.array( [vocab.lookup_indices( [tkn] ) ] )\n",
        "  inputs = torch.from_numpy(x)\n",
        "  # push to GPU\n",
        "  inputs = inputs.cuda()\n",
        "  # detach hidden state from history \n",
        "  h = tuple([each.data for each in h])\n",
        "  # get the output of the model\n",
        "  out, h = net(inputs, h)\n",
        "  # get the token probabilities\n",
        "  p = F.softmax(out, dim=1).data\n",
        "  p = p.cpu()\n",
        "  p = p.numpy()\n",
        "  p = p.reshape(p.shape[1],)\n",
        "  # get indices of top 3 values\n",
        "  top_n_idx = p.argsort()[-3:][::-1]\n",
        "  # randomly select one of the three indices\n",
        "  sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return vocab.lookup_tokens( [sampled_token_index] ), h\n",
        "\n",
        "\n",
        "# function to generate text\n",
        "def sample(net, size, prime='un petit'):\n",
        "    out_tokens = prime.split()\n",
        "    # push to GPU\n",
        "    net.cuda()\n",
        "    net.eval()\n",
        "    # batch size is 1\n",
        "    h = net.init_hidden(1)\n",
        "    toks = prime.split()\n",
        "    # predict next token\n",
        "    token, h = predict(net, toks[-1], h)\n",
        "    out_tokens.append(token[0])\n",
        "    for i in range(1, size-1):\n",
        "      token, h = predict(net, out_tokens[-1], h)\n",
        "      out_tokens.append(token[0])\n",
        "    print(' '.join(out_tokens))\n"
      ],
      "metadata": {
        "id": "SL-D2qnyvJQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Run experiments\n",
        "\n",
        "You can now start training a model. Once trained, you can use it to generate texts using the predict and sample functions below.\n",
        "\n",
        "▶▶ **Try to vary the hyper-parameters and see the influence on the results:**\n",
        "* start with 2 iterations \n",
        "* increase the number of iterations\n",
        "* increase the size of the hidden layer and the number of hidden layers\n",
        "* Try with GRU"
      ],
      "metadata": {
        "id": "heYMEw2maITD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the model\n",
        "net1 = WordLSTM( n_hidden=32, n_layers=1, drop_prob=0.3, lr=0.001 )\n",
        "# push the model to GPU (avoid it if you are not using the GPU)\n",
        "net1.cuda()\n",
        "print(net1)\n",
        "\n",
        "# train the model\n",
        "train(net1, batch_size = 16, epochs=2, print_every=2000)\n",
        "\n",
        "# Evaluation\n",
        "sample(net1, 15)\n",
        "sample(net1, 15, prime = \"une des\")\n",
        "sample(net1, 15, prime = \"une serie\")\n",
        "sample(net1, 15, prime = \"ils\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki2lO8dSusUp",
        "outputId": "8746a345-b54c-480d-881b-fe34bfce0374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordLSTM(\n",
            "  (emb_layer): Embedding(8788, 200)\n",
            "  (lstm): LSTM(200, 32, batch_first=True, dropout=0.3)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=32, out_features=8788, bias=True)\n",
            ")\n",
            "Epoch: 1/2... Step: 2000...\n",
            "Epoch: 1/2... Step: 4000...\n",
            "Epoch: 1/2... Step: 6000...\n",
            "Epoch: 2/2... Step: 8000...\n",
            "Epoch: 2/2... Step: 10000...\n",
            "Epoch: 2/2... Step: 12000...\n",
            "un petit monde pas a la saison est une bonne ile deserte a chaque personnage est\n",
            "une des acteurs a la saison qui est tres tres bien la premiere et je n'ai\n",
            "une serie qui est une etoile et le suspense a un peu de la serie qui\n",
            "ils ne pas de la serie qui est tres bien on ne se dit pas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment\n",
        "\n"
      ],
      "metadata": {
        "id": "lyqOyq9mgG3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the model\n",
        "net2 = WordLSTM( n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001 )\n",
        "# push the model to GPU (avoid it if you are not using the GPU)\n",
        "net2.cuda()\n",
        "print(net2)\n",
        "\n",
        "# train the model\n",
        "train(net2, batch_size = 16, epochs=2, print_every=2000)\n",
        "\n",
        "# Evaluation\n",
        "sample(net2, 15)\n",
        "sample(net2, 15, prime = \"un des\")\n",
        "sample(net2, 15, prime = \"une serie\")\n",
        "sample(net2, 15, prime = \"ils\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKU2haT8gNFa",
        "outputId": "f6c44c0a-275d-44e1-b7ec-a0651c42b38e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordLSTM(\n",
            "  (emb_layer): Embedding(8788, 200)\n",
            "  (lstm): LSTM(200, 256, num_layers=4, batch_first=True, dropout=0.3)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=8788, bias=True)\n",
            ")\n",
            "Epoch: 1/2... Step: 2000...\n",
            "Epoch: 1/2... Step: 4000...\n",
            "Epoch: 1/2... Step: 6000...\n",
            "Epoch: 2/2... Step: 8000...\n",
            "Epoch: 2/2... Step: 10000...\n",
            "Epoch: 2/2... Step: 12000...\n",
            "un petit fait de cette premiere serie et de montrer les acteurs sont et de plus\n",
            "un des episodes et les personnages est de la saison est a la premiere serie et\n",
            "une serie a la premiere bonne saison de regarder les episodes de cette serie est de\n",
            "ils fait a la premiere saison de la serie est de cette ile de plus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the model\n",
        "net3 = WordLSTM( n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001 )\n",
        "# push the model to GPU (avoid it if you are not using the GPU)\n",
        "net3.cuda()\n",
        "print(net3)\n",
        "\n",
        "# train the model\n",
        "train(net3, batch_size = 16, epochs=20, print_every=2000)\n",
        "\n",
        "# Evaluation\n",
        "sample(net3, 15)\n",
        "sample(net3, 15, prime = \"une des\")\n",
        "sample(net3, 15, prime = \"une serie\")\n",
        "sample(net3, 15, prime = \"ils\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvekSJ0WhhuB",
        "outputId": "7b6dc39c-a81e-4655-beb5-89c660d95f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordLSTM(\n",
            "  (emb_layer): Embedding(8788, 200)\n",
            "  (lstm): LSTM(200, 256, num_layers=4, batch_first=True, dropout=0.3)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=8788, bias=True)\n",
            ")\n",
            "Epoch: 1/20... Step: 2000...\n",
            "Epoch: 1/20... Step: 4000...\n",
            "Epoch: 1/20... Step: 6000...\n",
            "Epoch: 2/20... Step: 8000...\n",
            "Epoch: 2/20... Step: 10000...\n",
            "Epoch: 2/20... Step: 12000...\n",
            "Epoch: 3/20... Step: 14000...\n",
            "Epoch: 3/20... Step: 16000...\n",
            "Epoch: 3/20... Step: 18000...\n",
            "Epoch: 4/20... Step: 20000...\n",
            "Epoch: 4/20... Step: 22000...\n",
            "Epoch: 4/20... Step: 24000...\n",
            "Epoch: 4/20... Step: 26000...\n",
            "Epoch: 5/20... Step: 28000...\n",
            "Epoch: 5/20... Step: 30000...\n",
            "Epoch: 5/20... Step: 32000...\n",
            "Epoch: 6/20... Step: 34000...\n",
            "Epoch: 6/20... Step: 36000...\n",
            "Epoch: 6/20... Step: 38000...\n",
            "Epoch: 7/20... Step: 40000...\n",
            "Epoch: 7/20... Step: 42000...\n",
            "Epoch: 7/20... Step: 44000...\n",
            "Epoch: 7/20... Step: 46000...\n",
            "Epoch: 8/20... Step: 48000...\n",
            "Epoch: 8/20... Step: 50000...\n",
            "Epoch: 8/20... Step: 52000...\n",
            "Epoch: 9/20... Step: 54000...\n",
            "Epoch: 9/20... Step: 56000...\n",
            "Epoch: 9/20... Step: 58000...\n",
            "Epoch: 10/20... Step: 60000...\n",
            "Epoch: 10/20... Step: 62000...\n",
            "Epoch: 10/20... Step: 64000...\n",
            "Epoch: 10/20... Step: 66000...\n",
            "Epoch: 11/20... Step: 68000...\n",
            "Epoch: 11/20... Step: 70000...\n",
            "Epoch: 11/20... Step: 72000...\n",
            "Epoch: 12/20... Step: 74000...\n",
            "Epoch: 12/20... Step: 76000...\n",
            "Epoch: 12/20... Step: 78000...\n",
            "Epoch: 13/20... Step: 80000...\n",
            "Epoch: 13/20... Step: 82000...\n",
            "Epoch: 13/20... Step: 84000...\n",
            "Epoch: 14/20... Step: 86000...\n",
            "Epoch: 14/20... Step: 88000...\n",
            "Epoch: 14/20... Step: 90000...\n",
            "Epoch: 14/20... Step: 92000...\n",
            "Epoch: 15/20... Step: 94000...\n",
            "Epoch: 15/20... Step: 96000...\n",
            "Epoch: 15/20... Step: 98000...\n",
            "Epoch: 16/20... Step: 100000...\n",
            "Epoch: 16/20... Step: 102000...\n",
            "Epoch: 16/20... Step: 104000...\n",
            "Epoch: 17/20... Step: 106000...\n",
            "Epoch: 17/20... Step: 108000...\n",
            "Epoch: 17/20... Step: 110000...\n",
            "Epoch: 17/20... Step: 112000...\n",
            "Epoch: 18/20... Step: 114000...\n",
            "Epoch: 18/20... Step: 116000...\n",
            "Epoch: 18/20... Step: 118000...\n",
            "Epoch: 19/20... Step: 120000...\n",
            "Epoch: 19/20... Step: 122000...\n",
            "Epoch: 19/20... Step: 124000...\n",
            "Epoch: 20/20... Step: 126000...\n",
            "Epoch: 20/20... Step: 128000...\n",
            "Epoch: 20/20... Step: 130000...\n",
            "Epoch: 20/20... Step: 132000...\n",
            "un petit succes de la serie je me met en place mais je me demande bien\n",
            "une des actrices sont tres bons mais je ne suis pas la saison mais la saison\n",
            "une serie est vraiment geniale les personnages ont un suspense au moins la meilleure serie est\n",
            "ils sont tres irritants mais la deuxieme et la serie est morte necessaires de la\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additional exercises\n",
        "\n",
        "Some possible improvements:\n",
        "\n",
        "Data:\n",
        "* Try with sentences (+SOS and EOS symbols)\n",
        "* Try with a restricted vocabulary (trimming unfrequent words)\n",
        "* Try with a cleaner pre-processing\n",
        "\n",
        "Model:\n",
        "* Try to use the sentiment as a condition\n",
        "\n",
        "\n",
        "If you want to explore generation with a seq2seq: https://pytorch.org/tutorials/beginner/chatbot_tutorial.html with the notebook https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/cf54d584af1322e88020549223e907dc/chatbot_tutorial.ipynb "
      ],
      "metadata": {
        "id": "vVGLCc3Ba2-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sources\n",
        "https://www.analyticsvidhya.com/blog/2020/08/build-a-natural-language-generation-nlg-system-using-pytorch/\n",
        "\n",
        "https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html\n",
        "\n"
      ],
      "metadata": {
        "id": "ukBfxemtguzB"
      }
    }
  ]
}