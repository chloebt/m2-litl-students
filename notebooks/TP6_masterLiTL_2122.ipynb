{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP6_masterLiTL_2122.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This module allows to remove accents: a fast, but not very clean solution\n",
        "# to standardize the text (in reviews people often forget accents)\n",
        "!pip install unidecode"
      ],
      "metadata": {
        "id": "OXI2yERddTle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TP6: Natural Language Generation\n",
        "\n",
        "In this practical session, we will test a very simple model for text generation based on a neural language model, using a RNN.\n",
        "\n",
        "Here we use the data from the French dataset reviews, but it's not ideal, it's too small.\n",
        "\n",
        "\n",
        "I followed an existing tutorial using movie plots, you can test its code in the notebook here (modify the number of iterations to get better results): https://colab.research.google.com/drive/1ARI_F0RKV-L4GvmyTPPStZWhmqf737D-?usp=sharing using the data here: https://drive.google.com/file/d/1PakdWMKYNyC5-2G_CSlLtkBsHezFpMHJ/view?usp=sharing "
      ],
      "metadata": {
        "id": "AqqSLN94TMTW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xsi5qL2ruObG"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Load the data\n",
        "\n",
        "The code below allows to read the data, as usual, except that we ignore the sentiment label. We also don't use the dev set, so I added the dev and test data to our training set.\n",
        "\n",
        "Here we also lower case the text and do some pre-processing to ignore punctuations, in order to focus on words. "
      ],
      "metadata": {
        "id": "QsEAfX8KXMx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unidecode\n",
        "\n",
        "\n",
        "# read movie data \n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "train_path = \"allocine_train.tsv\"\n",
        "dev_path = \"allocine_dev.tsv\"\n",
        "test_path = \"allocine_test.tsv\"\n",
        "\n",
        "# Load train, dev and test set\n",
        "train_df = pd.read_csv(test_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "dev_df = pd.read_csv(dev_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "test_df = pd.read_csv(test_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "\n",
        "# splits the string sentence by space, we don t need the sentiment label here\n",
        "tokenizer = get_tokenizer( None ) \n",
        "train_iter = []\n",
        "for i in train_df.index:\n",
        "    #train_iter.append( tuple( [train_df[\"sentiment\"][i], train_df[\"review\"][i]] ) )\n",
        "    train_iter.append( train_df[\"review\"][i] )\n",
        "for i in dev_df.index:\n",
        "    train_iter.append( dev_df[\"review\"][i] )\n",
        "for i in test_df.index:\n",
        "    train_iter.append( test_df[\"review\"][i] )\n",
        "\n",
        "print( \"Original first review:\\t\", train_iter[0])\n",
        "\n",
        "# Optional: lower casing \n",
        "train_iter = [review.lower() for review in train_iter]\n",
        "print( \"Lower case:\\t\", train_iter[0])\n",
        "\n",
        "# Optional: remove accents\n",
        "train_iter = [unidecode.unidecode(accented_string) for accented_string in train_iter]\n",
        "print( \"Accents removed:\\t\", train_iter[0])\n",
        "\n",
        "# Optional: clean text, rmove punctuation to focus on alphabet\n",
        "train_iter = [re.sub(\"[^a-z' ]\", \"\", i) for i in train_iter]\n",
        "print( \"Remove punctuation, numbers..:\\t\", train_iter[0])\n",
        "\n",
        "\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\", \"<PAD\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) #simple mapping to self\n",
        "print( \"Test pipeline:\", text_pipeline( train_iter[0] ) )\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print( \"\\nVocab size:\", vocab_size)"
      ],
      "metadata": {
        "id": "1DNMil5O05lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare sequences\n",
        "\n",
        "Here, we will give our model fixed-length sequences (n-grams), with the length of the sequences as an hyper-parameter that can be changed. \n",
        "\n",
        "Another option would be to work directly on sentences (with padding to deal with sequences of different lengths)."
      ],
      "metadata": {
        "id": "VMQ9Ye_Z25fY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create sequences of length 5 tokens\n",
        "def create_seq(text, seq_len = 5):\n",
        "    sequences = []\n",
        "    text = text.strip()\n",
        "    # if the number of tokens in 'text' is greater than 5\n",
        "    if len(text.split()) > seq_len:\n",
        "      for i in range(seq_len, len(text.split())):\n",
        "        # select sequence of tokens\n",
        "        seq = text.split()[i-seq_len:i+1]\n",
        "        # add to the list\n",
        "        sequences.append(\" \".join(seq))\n",
        "      return sequences\n",
        "    # if the number of tokens in 'text' is less than or equal to 5\n",
        "    else:\n",
        "      return [text]"
      ],
      "metadata": {
        "id": "plXGnAMN25rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seqs = [create_seq(i, seq_len = 5) for i in train_iter]\n",
        "# merge list-of-lists into a single list\n",
        "seqs = sum(seqs, [])\n",
        "# count of sequences\n",
        "len(seqs)"
      ],
      "metadata": {
        "id": "pAw-eGi33mr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input and target data\n",
        "\n",
        "Now we create input and target sequences from our training data: the target is simply the sequence following the input one. This way, our model starts with an input word, the first in the input sequence, and tries to predict the next token, until the last of the target sequence. For example: \n",
        "* input: Une grosse daube. La premiere \n",
        "* target: grosse daube. La premiere saison\n",
        "\n",
        "A cleaner solution is to segment into sentences and add special characters signaling a start and end of a sequence. "
      ],
      "metadata": {
        "id": "nRiA9K474RHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create inputs and targets (x and y)\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for s in seqs:\n",
        "  if len(s.split()[:-1]) != 0:\n",
        "    x.append(\" \".join(s.split()[:-1]).strip())\n",
        "    y.append(\" \".join(s.split()[1:]).strip())\n",
        "print( x[0])\n",
        "print(y[0])"
      ],
      "metadata": {
        "id": "8akGHRNs4RSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Map to integer\n",
        "\n",
        "Now we map our token sequences to integer lists. Here we also add padding when necessary."
      ],
      "metadata": {
        "id": "QhMsWehh4ZwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_integer_seq(seq, max_len=5):\n",
        "  int_seq = text_pipeline(seq)\n",
        "  while len(int_seq)!=max_len:\n",
        "      int_seq.append(vocab.lookup_indices([\"<PAD>\"])[0])\n",
        "  return int_seq\n",
        "\n",
        "# convert text sequences to integer sequences\n",
        "x_int = [get_integer_seq(i) for i in x]\n",
        "y_int = [get_integer_seq(i) for i in y]\n",
        "\n",
        "# convert lists to numpy arrays\n",
        "x_int = np.array(x_int)\n",
        "y_int = np.array(y_int)\n",
        "\n",
        "print(x_int)"
      ],
      "metadata": {
        "id": "fxNHC2-b4Z6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batches\n",
        "\n",
        "Batches are simply list of sequences."
      ],
      "metadata": {
        "id": "BXuKn3gAZBW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(arr_x, arr_y, batch_size):\n",
        "    # iterate through the arrays\n",
        "    prv = 0\n",
        "    for n in range(batch_size, arr_x.shape[0], batch_size):\n",
        "      x = arr_x[prv:n,:]\n",
        "      y = arr_y[prv:n,:]\n",
        "      prv = n\n",
        "      yield x, y"
      ],
      "metadata": {
        "id": "DRt8bB105X8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Model definition\n",
        "\n",
        "The model is very similar to chat we've seen until now.\n",
        "\n",
        "▶▶ **Write the '__init__(..) part, using and embedding layer and an LSTM**"
      ],
      "metadata": {
        "id": "xXx-n-lmZLOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WordLSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n",
        "        super().__init__()\n",
        "\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        ## --- TODO: define the embedding layer\n",
        "\n",
        "\n",
        "        ## --- TODO: define the LSTM\n",
        "        \n",
        "        \n",
        "        ## define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## --- TODO: define the fully-connected layer\n",
        "           \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "        ## pass input through embedding layer\n",
        "        embedded = self.emb_layer(x)     \n",
        "        ## Get the outputs and the new hidden state from the lstm\n",
        "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
        "        ## pass through a dropout layer\n",
        "        out = self.dropout(lstm_output)\n",
        "        #out = lstm_output  \n",
        "        out = out.reshape(-1, self.n_hidden) \n",
        "        ## put \"out\" through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        # if GPU is available\n",
        "        if (torch.cuda.is_available()):\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        # if GPU is not available\n",
        "        else:\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "tKUHl8fxusSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training function is also very similar to what we saw before. "
      ],
      "metadata": {
        "id": "AUy2wyo_Z3vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
        "    # optimizer\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    # loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # push model to GPU\n",
        "    net.cuda()\n",
        "    counter = 0\n",
        "    net.train()\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        for x, y in get_batches(x_int, y_int, batch_size):\n",
        "            counter+= 1\n",
        "            # convert numpy arrays to PyTorch arrays\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            # push tensors to GPU\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            # detach hidden states\n",
        "            # https://discuss.pytorch.org/t/solved-why-we-need-to-detach-variable-which-contains-hidden-representation/1426/4\n",
        "            h = tuple([each.data for each in h])\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(-1))\n",
        "            # back-propagate error\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            # update weigths\n",
        "            opt.step()            \n",
        "            if counter % print_every == 0:\n",
        "              print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                    \"Step: {}...\".format(counter))"
      ],
      "metadata": {
        "id": "zdxK4FqVusWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict\n",
        "\n",
        "The predict function takes as input some tokens: the model needs a first input to predict the next tokens. The functions below can be used to generate some text based on an input sequence. "
      ],
      "metadata": {
        "id": "oYVCRuaaa0va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict next token\n",
        "def predict(net, tkn, h=None):\n",
        "  # tensor inputs \n",
        "  x = np.array( [vocab.lookup_indices( [tkn] ) ] )\n",
        "  inputs = torch.from_numpy(x)\n",
        "  # push to GPU\n",
        "  inputs = inputs.cuda()\n",
        "  # detach hidden state from history \n",
        "  h = tuple([each.data for each in h])\n",
        "  # get the output of the model\n",
        "  out, h = net(inputs, h)\n",
        "  # get the token probabilities\n",
        "  p = F.softmax(out, dim=1).data\n",
        "  p = p.cpu()\n",
        "  p = p.numpy()\n",
        "  p = p.reshape(p.shape[1],)\n",
        "  # get indices of top 3 values\n",
        "  top_n_idx = p.argsort()[-3:][::-1]\n",
        "  # randomly select one of the three indices\n",
        "  sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return vocab.lookup_tokens( [sampled_token_index] ), h\n",
        "\n",
        "\n",
        "# function to generate text\n",
        "def sample(net, size, prime='un petit'):\n",
        "    out_tokens = prime.split()\n",
        "    # push to GPU\n",
        "    net.cuda()\n",
        "    net.eval()\n",
        "    # batch size is 1\n",
        "    h = net.init_hidden(1)\n",
        "    toks = prime.split()\n",
        "    # predict next token\n",
        "    token, h = predict(net, toks[-1], h)\n",
        "    out_tokens.append(token[0])\n",
        "    for i in range(1, size-1):\n",
        "      token, h = predict(net, out_tokens[-1], h)\n",
        "      out_tokens.append(token[0])\n",
        "    print(' '.join(out_tokens))\n"
      ],
      "metadata": {
        "id": "SL-D2qnyvJQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Run experiments\n",
        "\n",
        "You can now start training a model. Once trained, you can use it to generate texts using the predict and sample functions below.\n",
        "\n",
        "▶▶ **Try to vary the hyper-parameters and see the influence on the results:**\n",
        "* start with 2 iterations \n",
        "* increase the number of iterations\n",
        "* increase the size of the hidden layer and the number of hidden layers\n",
        "* Try with GRU"
      ],
      "metadata": {
        "id": "heYMEw2maITD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the model\n",
        "net1 = WordLSTM( n_hidden=32, n_layers=1, drop_prob=0.3, lr=0.001 )\n",
        "# push the model to GPU (avoid it if you are not using the GPU)\n",
        "net1.cuda()\n",
        "print(net1)\n",
        "\n",
        "# train the model\n",
        "train(net1, batch_size = 16, epochs=2, print_every=2000)\n",
        "\n",
        "# Evaluation\n",
        "sample(net1, 15)\n",
        "sample(net1, 15, prime = \"une des\")\n",
        "sample(net1, 15, prime = \"une serie\")\n",
        "sample(net1, 15, prime = \"ils\")"
      ],
      "metadata": {
        "id": "Ki2lO8dSusUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2JMP_4aa_gvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YI_ZPoOg_g5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additional exercises\n",
        "\n",
        "Some possible improvements:\n",
        "\n",
        "Data:\n",
        "* Try with sentences (+SOS and EOS symbols)\n",
        "* Try with a restricted vocabulary (trimming unfrequent words)\n",
        "* Try with a cleaner pre-processing\n",
        "\n",
        "Model:\n",
        "* Try to use the sentiment as a condition\n",
        "\n",
        "\n",
        "If you want to explore generation with a seq2seq: https://pytorch.org/tutorials/beginner/chatbot_tutorial.html with the notebook https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/cf54d584af1322e88020549223e907dc/chatbot_tutorial.ipynb \n",
        "\n",
        "You can also try to implement the functions to read the data in sent_recipes.txt that contains sentence split recipes. These data could be used with a seq2seq model where the goal is to generate the next sentence, each line containing the input and target sentence."
      ],
      "metadata": {
        "id": "vVGLCc3Ba2-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sources\n",
        "https://www.analyticsvidhya.com/blog/2020/08/build-a-natural-language-generation-nlg-system-using-pytorch/\n",
        "\n",
        "https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html\n",
        "\n"
      ],
      "metadata": {
        "id": "ukBfxemtguzB"
      }
    }
  ]
}