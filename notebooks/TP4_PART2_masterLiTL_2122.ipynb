{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP4_PART2_masterLiTL_2122.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCHhtzOXQ2po"
      },
      "source": [
        "# TP 4: Training a Feedforward neural network (Part 2)\n",
        "Master LiTL - 2021-2022\n",
        "\n",
        "## Requirements\n",
        "In this part, we will use continuous representations of words, namely Continuous Bag of Words that is randomly initialized embeddings, and then pretrained embeddings.\n",
        "In order to create the representation of a document, we will take all the embeddings of the words that appear in the document, and sum them together (or take their average).\n",
        "So instead of having an input vector of size 5000, we now have an input vector of size e.g. 50, that represents the ‘average’, combined meaning of all the words in the document taken together. \n",
        "\n",
        "Crucially,  the  neural  network  will  also  learn  the  embeddings  during  training :  the  embeddings  of  the network are also parameters that are optimized according to the loss function.\n",
        "\n",
        "The dataset remains the French set of reviews labeled with sentiment.\n",
        "\n",
        "We will compare our model to the scores obtained previously with bag of word representations.\n",
        "\n",
        "Once you've read and understood the code below to read the data, you can run your model and make some experiments, varying the hyper-parameters: \n",
        "* size of the embeddings, \n",
        "* size of the hidden layer, \n",
        "* activation function, \n",
        "* number of iterations.\n",
        "\n",
        "Try to save your results and print some vizualizations: \n",
        "+ Plot the cost function during training with different values for the learning rate\n",
        "+ Plot the accuracy wrt the size of the embedding layer\n",
        "+ Plot the accuracy wrt to the number of training examples "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT7n30VpCOzF"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If you’re using Colab, allocate a GPU by going to Edit > Notebook Settings.\n",
        "# We move our tensor to the GPU if available\n",
        "if torch.cuda.is_available():\n",
        "  print(f\"GPU ok\")\n",
        "else:\n",
        "  print(\"no gpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdSyhJqpVczO"
      },
      "source": [
        "\n",
        "## 1. Read the data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "train_path = \"allocine_train.tsv\"\n",
        "dev_path = \"allocine_dev.tsv\"\n",
        "test_path = \"allocine_test.tsv\"\n",
        "\n",
        "# Load train set\n",
        "train_df = pd.read_csv(train_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "train_iter = []\n",
        "for i in train_df.index:\n",
        "    #print( train_df[\"sentiment\"][i], train_df[\"review\"][i])\n",
        "    train_iter.append( tuple( [train_df[\"sentiment\"][i], train_df[\"review\"][i]] ) )\n",
        "\n",
        "print( '\\n'.join( [ str(train_iter[i][0])+'\\t'+train_iter[i][1] for i in range(0,10) ] ) )\n",
        "\n",
        "\n",
        "dev_df = pd.read_csv(dev_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "test_df = pd.read_csv(test_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "dev_iter, test_iter = [], []\n",
        "for i in dev_df.index:\n",
        "    dev_iter.append( tuple( [dev_df[\"sentiment\"][i], dev_df[\"review\"][i]] ) )\n",
        "\n",
        "for i in test_df.index:\n",
        "    test_iter.append( tuple( [test_df[\"sentiment\"][i], test_df[\"review\"][i]] ) )"
      ],
      "metadata": {
        "id": "oNpGiuaRxlvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time, we don't directly use the bag of word representation built by scikit.\n",
        "\n",
        "We need to tokenize our data, and build the corresponding vocabulary (on the train set)."
      ],
      "metadata": {
        "id": "xlXairmKu8Xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# splits the string sentence by space.\n",
        "tokenizer = get_tokenizer( None ) \n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "ZBctRAcZsQSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vocabulary\n",
        "\n",
        "Here the vocabulary is a specific object in Pytorch, check the existing functions to use it here: https://pytorch.org/text/stable/vocab.html\n",
        "\n",
        "For example, the vocabulary directly converts a list of tokens into integers."
      ],
      "metadata": {
        "id": "Tus9Kedas5dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab(['Avant', 'cette', 'série', ','])"
      ],
      "metadata": {
        "id": "tb6TYA9Is5v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶▶ **Now, try to retrieve the indice of the word 'mauvais'.** "
      ],
      "metadata": {
        "id": "3aAwvzFavjIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "k9cKqyj3vjT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Text and label pipelines\n",
        "\n",
        "The text pipeline converts a text string into a list of integers based on the lookup table defined in the vocabulary. \n",
        "\n",
        "The label pipeline converts the label into integers. "
      ],
      "metadata": {
        "id": "_4zDQHrstB12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) #simple mapping to self"
      ],
      "metadata": {
        "id": "eUNfcXNUtCBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_pipeline('Avant cette série, je ne connaissais que Urgence')"
      ],
      "metadata": {
        "id": "ArT6VMS23I0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_pipeline('0')"
      ],
      "metadata": {
        "id": "LySlUUzY3FEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate data batch and iterator\n",
        "\n",
        "Here we also use *torch.utils.data.DataLoader* with an iterable dataset, here a simple list of labels and text reviews, as saved in *train_iter*.\n",
        "\n",
        "Before sending to the model, we apply a function, *collate_fn*, to our input data:\n",
        "* The input to *collate_fn* is a batch of data with the batch size in *DataLoader*, \n",
        "* *collate_fn* processes them according to the data processing pipelines declared previously. \n",
        "\n",
        "In 'collate_batch', we define how we want to pre-process our data.\n",
        "\n",
        "The function is directly called within *DataLoader*:\n",
        "```\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
        "```\n",
        "\n",
        "Below: \n",
        "* the text entries in the original data batch input are packed into a list and concatenated as a single tensor. \n",
        "* the offset is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor\n",
        "* Label is a tensor saving the labels of individual text entries."
      ],
      "metadata": {
        "id": "Nj21QrBotJrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label.to(device), text_list.to(device), offsets.to(device)\n",
        "\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "GE82N6mmuxYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define the model\n",
        "\n",
        "### Bag of embeddings\n",
        "\n",
        "The model is composed of the nn.EmbeddingBag layer: https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html\n",
        "\n",
        "* mode (string, optional) – \"sum\", \"mean\" or \"max\". Default=mean.\n",
        "\n",
        "### Weight initialization\n",
        "\n",
        "Weight initialization is done by default uniformly. You can also specify the initialization as done below, and choose among varied options: https://pytorch.org/docs/stable/nn.init.html. Further info there https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch or there https://discuss.pytorch.org/t/clarity-on-default-initialization-in-pytorch/84696/3"
      ],
      "metadata": {
        "id": "B6jAI7orupfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
        "        # just always do it in an nn.Module\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "\n",
        "        # Define the parameters that you will need. \n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True) # <----\n",
        "\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "\n",
        "        # Non-linearity\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc2.bias.data.fill_(0.01)\n",
        "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc2.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "\n",
        "        embedded = self.embedding(text, offsets)\n",
        "\n",
        "        # Linear function  # LINEAR\n",
        "        out = self.fc1(embedded)\n",
        "\n",
        "        # Non-linearity  # NON-LINEAR\n",
        "        out = self.sigmoid(out) \n",
        "\n",
        "        # Linear function (readout)  # LINEAR\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "2GxkAeRQuqfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and evaluation functions\n",
        "\n",
        "Other functions for training and evaluating, but roughly doing the same as previously."
      ],
      "metadata": {
        "id": "Bq7EqLhGy_es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predicted_label = model(text, offsets)\n",
        "        loss = criterion(predicted_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predicted_label = model(text, offsets)\n",
        "            loss = criterion(predicted_label, label)\n",
        "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "metadata": {
        "id": "4WgmkNZWurC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3. Running an experiment \n",
        "\n",
        "### Adjusting the learning rate\n",
        "\n",
        "The code below uses a *scheduler*: *torch.optim.lr_scheduler* provides several methods to adjust the learning rate based on the number of epochs.\n",
        "\n",
        "Learning rate scheduling should be applied after optimizer’s update.\n",
        "\n",
        "* torch.optim.lr_scheduler.StepLR: Decays the learning rate of each parameter group by gamma every step_size epochs.\n",
        "\n",
        "https://pytorch.org/docs/stable/optim.html"
      ],
      "metadata": {
        "id": "_6zahomkrWMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emb_dim = 50\n",
        "hid_dim = 64\n",
        "model = FeedforwardNeuralNetModel(vocab_size, emb_dim, hid_dim, num_class).to(device)"
      ],
      "metadata": {
        "id": "A7le5MIHuq7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1) # <----\n",
        "total_accu = None\n",
        "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(dev_iter, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_iter, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val: # <----\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "metadata": {
        "id": "_Oo69gIdv4Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "metadata": {
        "id": "UZq7hmXOv4t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶▶ **Make some additional experiments varying**: \n",
        "* the size of the embeddings, e.g. 300, 3000\n",
        "* the size of the hidden layer, \n",
        "* the activation function, \n",
        "* the number of iterations."
      ],
      "metadata": {
        "id": "2RNZAiGwBn-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RzGpFg5vBoOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Using pretrained embeddings\n",
        "\n",
        "Upload the file *cc.fr.300.10000.vec*: the first 10000 lines of the FastText embeddings for French, https://fasttext.cc/docs/en/crawl-vectors.html."
      ],
      "metadata": {
        "id": "UDlM7OZq56HO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the vectors\n",
        "\n",
        "▶▶ **Write a function that load the vectors**, i.e. a dictionary mapping a word to its vector, as defined in the fasttext file. \n",
        "\n",
        "▶▶ **Print the vocabulary and the size of the embeddings.**"
      ],
      "metadata": {
        "id": "RX2DkAqws1gU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yd2EEjECv4vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the weight matrix\n",
        "\n",
        "NOw we build a matrix over the dataset associating each word to its vector. For each word in dataset’s vocabulary, we check if it is on FastText’s vocabulary:\n",
        "* if yes: load its pre-trained word vector. \n",
        "* else: we initialize a random vector."
      ],
      "metadata": {
        "id": "GTA0vXeevSuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_dim = 300\n",
        "matrix_len = len(vocab)\n",
        "weights_matrix = np.zeros((matrix_len, emb_dim))\n",
        "words_found = 0\n",
        "\n",
        "for i in range(0, len(vocab)):\n",
        "    word = vocab.lookup_token(i)\n",
        "    try: \n",
        "        weights_matrix[i] = vectors[word]\n",
        "        words_found += 1\n",
        "    except KeyError:\n",
        "        weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
        "weights_matrix = torch.from_numpy(weights_matrix)\n",
        "print( weights_matrix.size())"
      ],
      "metadata": {
        "id": "4XXFTaRxvRNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding layer\n",
        "\n",
        "The code below defines a function that builds the embedding layer using the pretrained embeddings."
      ],
      "metadata": {
        "id": "EACvg0Uw7qje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "    num_embeddings, embedding_dim = weights_matrix.size()\n",
        "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    emb_layer.load_state_dict({'weight': weights_matrix}) # <----\n",
        "    if non_trainable:\n",
        "        emb_layer.weight.requires_grad = False\n",
        "\n",
        "    return emb_layer, num_embeddings, embedding_dim"
      ],
      "metadata": {
        "id": "e-miRfkYvZpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model definition\n",
        "\n",
        "Now we can modify our model to add this embedding layer. \n",
        "\n",
        "Note that the embedding bag now takes pre initialized weights. "
      ],
      "metadata": {
        "id": "VcLWQgu877rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedforwardNeuralNetModel2(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim, output_dim, weights_matrix):\n",
        "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
        "        # just always do it in an nn.Module\n",
        "        super(FeedforwardNeuralNetModel2, self).__init__()\n",
        "\n",
        "        # Define the parameters that you will need. \n",
        "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
        "\n",
        "        self.embedding_sum = nn.EmbeddingBag.from_pretrained(  self.embedding.weight, mode='sum')\n",
        "\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "\n",
        "        # Non-linearity\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "\n",
        "        embedded = self.embedding_sum(text, offsets)\n",
        "\n",
        "        # Linear function  # LINEAR\n",
        "        out = self.fc1(embedded)\n",
        "\n",
        "        # Non-linearity  # NON-LINEAR\n",
        "        out = self.sigmoid(out) \n",
        "\n",
        "        # Linear function (readout)  # LINEAR\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "fXOPuCv_vZrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "emb_dim = 300\n",
        "hidden_dim = 128\n",
        "\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "model = FeedforwardNeuralNetModel2(emb_dim, hidden_dim, num_class, weights_matrix).to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1) # <----\n",
        "total_accu = None\n",
        "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(dev_iter, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_iter, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val: # <----\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "metadata": {
        "id": "qgyfNh2UvZuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.embedding.weight[vocab.lookup_indices(['mauvais'])])"
      ],
      "metadata": {
        "id": "fSg2Lzr-vZwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to save your results and print some vizualizations:\n",
        "\n",
        "* Plot the cost function during training with different values for eg the learning rate \n",
        "* Plot the accuracy wrt the size of the embedding layer\n",
        "* Plot the accuracy wrt to the number of training examples"
      ],
      "metadata": {
        "id": "4DLHIDmCEQ-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5rF7DeeWERJp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}