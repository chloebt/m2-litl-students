{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP5_masterLiTL_2122.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGmy-dtuOtiw"
      },
      "source": [
        "# TP5: RNNs\n",
        "\n",
        "In this practical session, we will explore the modifications needed to use a RNN for text classification. We will compare a FFNN with an LSTM, using either BoW or continuous representations. We will also see how to use mini-batches for both classification algorithms.\n",
        "\n",
        "The first part of the code is based on previous sessions:\n",
        "* Part 1: using BoW representations with FFNN or LSTM\n",
        "* Part 2: using continuous representations with FFNN or LSTM\n",
        "* Part 3: using mini-batches requires padding with LSTMs\n",
        "* Part 4: trying other architectures\n",
        "* Part 5: sequence tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pydK_h3QLZfO"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "## 1- Define the device to be used\n",
        "\n",
        "# CUDA for PyTorch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdSyhJqpVczO"
      },
      "source": [
        "# PART1: BoW Representation\n",
        "\n",
        "## 1.1 Read and load the data\n",
        "\n",
        "The code below is exactly the same as in TP2, building BoW representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoVJ18s_oxkn"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import sklearn\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "train_path = \"allocine_train.tsv\"\n",
        "dev_path = \"allocine_dev.tsv\"\n",
        "test_path = \"allocine_test.tsv\"\n",
        "\n",
        "# This will be the size of the vectors reprensenting the input\n",
        "MAX_FEATURES = 5000 \n",
        "\n",
        "# Load train, dev and test set\n",
        "train_df = pd.read_csv(train_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "dev_df = pd.read_csv(dev_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "test_df = pd.read_csv(test_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "print(\"Creating features from bag of words...\")\n",
        "vectorizer = CountVectorizer(analyzer = \"word\", max_features = MAX_FEATURES) \n",
        "train_data_features = vectorizer.fit_transform(train_df[\"review\"])\n",
        "x_train = train_data_features.toarray()\n",
        "y_train = np.asarray(train_df[\"sentiment\"])\n",
        "print( \"TRAIN:\", x_train.shape )\n",
        "count_train = x_train.shape[0]\n",
        "# -- DEV\n",
        "dev_data_features = vectorizer.transform(dev_df[\"review\"])\n",
        "x_dev = dev_data_features.toarray()\n",
        "y_dev = np.asarray(dev_df[\"sentiment\"])\n",
        "print( \"DEV:\", x_dev.shape )\n",
        "# -- TEST\n",
        "test_data_features = vectorizer.transform(test_df[\"review\"])\n",
        "x_test = test_data_features.toarray()\n",
        "y_test = np.asarray(test_df[\"sentiment\"])\n",
        "print( \"TEST:\", x_test.shape )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Building models\n",
        "\n",
        "### 1.2.1 Using a FFNN\n",
        "\n",
        "The code below defines a FFNN, taking as input a BoW representation (no embedding layer)."
      ],
      "metadata": {
        "id": "Q5cYtN3Q6t-b"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuV1OjAdMHOX"
      },
      "source": [
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function ==> W1\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Non-linearity ==> g\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Linear function (readout) ==> W2\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        y = g(x.W1+b).W2\n",
        "        '''\n",
        "        # Linear function  # LINEAR ==> x.W1+b\n",
        "        out = self.fc1(x)\n",
        "\n",
        "        # Non-linearity  # NON-LINEAR ==> h1 = g(x.W1+b)\n",
        "        out = self.sigmoid(out) \n",
        "\n",
        "        # Linear function (readout)  # LINEAR ==> y = h1.W2\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 Using an LSTM\n",
        "\n",
        "The code below defines an LSTM model, taking also a Bow representation.\n",
        "\n",
        "As you can see, we have now:\n",
        "* an LSTM layer that will transform our input into a vector representation with the size hidden_dim\n",
        "* in the forward pass, we need to reshape the data using:\n",
        "```\n",
        "x = x.view(len(x), 1, -1)\n",
        "```\n",
        "\n",
        "We need to reshape our input data before passing it to the LSTM layer, because it takes a 3D tensor with (Sequence lenght, Batch size, Input size). This is done with the 'view' method, the pytorch 'reshape' function for tensors. (there's also a format with batch size first, more easy to understand)"
      ],
      "metadata": {
        "id": "nGipZB6P67x-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM( input_size=input_dim, \n",
        "                            hidden_size=hidden_dim, \n",
        "                            bidirectional=False)\n",
        "\n",
        "        # Linear function (readout) ==> W2\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "\n",
        "    def forward(self, x):\n",
        "        # The view function is meant to reshape the tensor, keeping the \n",
        "        # same number of elements\n",
        "        # e.g. try a = torch.range(1, 16) and a = a.view(4, 4)\n",
        "        # When you don t know how many elements you want for one dimension,\n",
        "        # you can use -1\n",
        "        # Here, an LSTM wants as input a 3D tensor with:\n",
        "        # Sequence lenght, Batch size, Input size\n",
        "        x = x.view(len(x), 1, -1)\n",
        "        out, (ht, ct) = self.lstm( x )\n",
        "        y = self.fc2(ht[-1])\n",
        "        return y\n",
        "        "
      ],
      "metadata": {
        "id": "553lGX8njqKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training and evaluation functions are given below. "
      ],
      "metadata": {
        "id": "4DvcPpqTkCuQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANibLgnhL9jU"
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "def train( model, train_loader, optimizer, num_epochs=5, trace=False ):\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, total_acc, total_count = 0, 0, 0\n",
        "        for input, label in train_loader:\n",
        "            input = input.to(device)\n",
        "            label = label.to(device)\n",
        "            # Step1. Clearing the accumulated gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Step 2. Forward pass to get output/logits\n",
        "            outputs = model( input )\n",
        "            if trace:\n",
        "              print(input) # <---- call with trace=True to 'see' the input\n",
        "            # Step 3. Compute the loss, gradients, and update the parameters by\n",
        "            # calling optimizer.step()\n",
        "            # - Calculate Loss: softmax --> cross entropy loss\n",
        "            loss = criterion(outputs, label)\n",
        "            # - Getting gradients w.r.t. parameters\n",
        "            loss.backward()\n",
        "            # - Updating parameters\n",
        "            optimizer.step()\n",
        "            # Accumulating the loss over time\n",
        "            train_loss += loss.item()\n",
        "            total_acc += (outputs.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "        # Compute accuracy on train set at each epoch\n",
        "        print('Epoch: {}. Loss: {}. ACC {} '.format(epoch, train_loss/count_train, total_acc/count_train))\n",
        "        total_acc, total_count = 0, 0\n",
        "        train_loss = 0\n",
        "\n",
        "def evaluate( model, dev_loader ):\n",
        "    predictions = []\n",
        "    gold = []\n",
        "    with torch.no_grad():\n",
        "        for input, label in dev_loader:\n",
        "            input = input.to(device)\n",
        "            label = label.to(device)\n",
        "            probs = model(input)\n",
        "            predictions.append( torch.argmax(probs, dim=1).cpu().numpy()[0] )\n",
        "            gold.append(int(label))\n",
        "    print(classification_report(gold, predictions))\n",
        "    return gold, predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Run an experiment\n",
        "\n",
        "### 1.3.1 Test with a FFNN\n",
        "\n",
        "The code below will launch an experiment with a simple Feed Forward Neural Network.\n",
        "\n",
        "We load the data with a batch size of 1."
      ],
      "metadata": {
        "id": "zKA97IBlpkQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# create Tensor dataset\n",
        "train_data = TensorDataset(torch.from_numpy(x_train).to(torch.float), torch.from_numpy(y_train))\n",
        "batch_size = 1 #no batch, or batch = 1\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size )\n",
        "# Load dev data\n",
        "dev_data = TensorDataset(torch.from_numpy(x_dev).to(torch.float), torch.from_numpy(y_dev))\n",
        "dev_loader = DataLoader(dev_data, shuffle=True )"
      ],
      "metadata": {
        "id": "VN3e9f7CnDBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcGyjXbUoxx9"
      },
      "source": [
        "# Set the value of the hyper-parameters\n",
        "VOCAB_SIZE = MAX_FEATURES # here BoW representation\n",
        "input_dim = VOCAB_SIZE \n",
        "hidden_dim = 4\n",
        "output_dim = 2\n",
        "learning_rate = 0.1\n",
        "num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model_ffnn = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.SGD(model_ffnn.parameters(), lr=learning_rate)\n",
        "model_ffnn = model_ffnn.to(device)\n",
        "# Train the model\n",
        "train( model_ffnn, train_loader, optimizer, num_epochs=5 )\n",
        "# Evaluate on dev\n",
        "gold, pred = evaluate( model_ffnn, dev_loader )"
      ],
      "metadata": {
        "id": "CouamGG9jpA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.2 Test with LSTM \n",
        "\n",
        "The code below will launch the experiment with the LSTM model."
      ],
      "metadata": {
        "id": "b-w_NPoObyMY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7mmAyoPMziY"
      },
      "source": [
        "# Initialization of the model\n",
        "model_lstm = LSTMModel(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.SGD(model_lstm.parameters(), lr=learning_rate)\n",
        "model = model_lstm.to(device)\n",
        "\n",
        "# Train the model\n",
        "train( model_lstm, train_loader, optimizer, num_epochs=5 )\n",
        "\n",
        "# Evaluate on dev\n",
        "gold, pred = evaluate( model_lstm, dev_loader )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an additional exercize, you can vary some hyper-parameters and compute the final scores of both models on the test dataset."
      ],
      "metadata": {
        "id": "iK6YvaCX8nNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART2: continuous representation\n",
        "\n",
        "Now we will go back to the continuous representation, i.e. randomly initialized real-valued vectors. \n",
        "\n",
        "## 2.1 Load the data\n",
        "\n",
        "The code below is the same as in TP4: we tokenize the data, extract the vocabulary, and build pipelines to process the data. We also define the function 'collate_batch' that is used to process batches of data. For now, we keep batch_size=1. "
      ],
      "metadata": {
        "id": "vH5PWlKbb2H_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# splits the string sentence by space.\n",
        "tokenizer = get_tokenizer( None ) \n",
        "train_iter = []\n",
        "for i in train_df.index:\n",
        "    train_iter.append( tuple( [train_df[\"sentiment\"][i], train_df[\"review\"][i]] ) )\n",
        "dev_iter = []\n",
        "for i in dev_df.index:\n",
        "    dev_iter.append( tuple( [dev_df[\"sentiment\"][i], dev_df[\"review\"][i]] ) )\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) #simple mapping to self\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label.to(device), text_list.to(device), offsets.to(device)\n"
      ],
      "metadata": {
        "id": "U2dZLp8FrSNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Define the models\n",
        "\n",
        "The code below defines a FFNN with an embedding layer that transforms our input words to vectors of size 'embed_dim' and performs an operation on these vectors to build a representaton for each document (default=mean).\n",
        "\n",
        "Remember that we need the 'offsets' here to retrieve the batches (each document is concatenated to the others in a batch, the offsets are used to retrieve the separate documents)."
      ],
      "metadata": {
        "id": "7PRplEud9XHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedforwardNeuralNetModel2(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel2, self).__init__()\n",
        "\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        # Linear function ==> W1\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "\n",
        "        # Non-linearity ==> g\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Linear function (readout) ==> W2\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        # Linear function  # LINEAR ==> x.W1+b\n",
        "        out = self.fc1(embedded)\n",
        "\n",
        "        # Non-linearity  # NON-LINEAR ==> h1 = g(x.W1+b)\n",
        "        out = self.sigmoid(out) \n",
        "\n",
        "        # Linear function (readout)  # LINEAR ==> y = h1.W2\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "CVWapsW2sQ2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below defines an architecture using an LSTM which is fed with continuous representations. The embedding layer transforms our words into continuous vectors that are the inputs of our LSTM (that is thus a replacement of the 'embedding bag'). "
      ],
      "metadata": {
        "id": "j_4UxQJX92so"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel2(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(LSTMModel2, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM( input_size=embedding_dim, \n",
        "                            hidden_size=hidden_dim, \n",
        "                            bidirectional=False)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "\n",
        "    def forward(self, text):\n",
        "        embeds = self.embedding(text)\n",
        "        x = embeds.view(len(text), 1, -1)\n",
        "        out, (ht, ct) = self.lstm( x )\n",
        "        y = self.fc2(ht[-1])\n",
        "        return y"
      ],
      "metadata": {
        "id": "wxTCWm7CtpvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and evaluation\n",
        "\n",
        "To use the offsets, we need to modify the train and evaluation procedures."
      ],
      "metadata": {
        "id": "UsXmIGqApbxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_woffset( model, train_loader, optimizer, num_epochs=5 ):\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, total_acc, total_count = 0, 0, 0\n",
        "        for label, input, offsets in train_loader:\n",
        "            input = input.to(device)\n",
        "            label = label.to(device)\n",
        "            # Step1. Clearing the accumulated gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Step 2. Forward pass to get output/logits\n",
        "            outputs = model( input, offsets ) # <-----\n",
        "            # Step 3. Compute the loss, gradients, and update the parameters by\n",
        "            # calling optimizer.step()\n",
        "            # - Calculate Loss: softmax --> cross entropy loss\n",
        "            loss = criterion(outputs, label)\n",
        "            # - Getting gradients w.r.t. parameters\n",
        "            loss.backward()\n",
        "            # - Updating parameters\n",
        "            optimizer.step()\n",
        "            # Accumulating the loss over time\n",
        "            train_loss += loss.item()\n",
        "            total_acc += (outputs.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "        # Compute accuracy on train set at each epoch\n",
        "        print('Epoch: {}. Loss: {}. ACC {} '.format(epoch, train_loss/count_train, total_acc/count_train))\n",
        "        total_acc, total_count = 0, 0\n",
        "        train_loss = 0\n",
        "\n",
        "def evaluate_woffset( model, dev_loader ):\n",
        "    predictions = []\n",
        "    gold = []\n",
        "    with torch.no_grad():\n",
        "        for label, input, offsets in dev_loader:\n",
        "            input = input.to(device)\n",
        "            label = label.to(device)\n",
        "            probs = model(input, offsets) # <-----\n",
        "            predictions.append( torch.argmax(probs, dim=1).cpu().numpy()[0] )\n",
        "            gold.append(int(label))\n",
        "    print(classification_report(gold, predictions))\n",
        "    return gold, predictions"
      ],
      "metadata": {
        "id": "US_0JmN5phqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Run an experiments\n",
        "\n",
        "### Test with a FFNN\n",
        "\n",
        "The code below uses the FFNN with continuous representations."
      ],
      "metadata": {
        "id": "NC2VtTmv-Q_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "batch_size = 1\n",
        "train_loader = DataLoader(train_iter, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "dev_loader = DataLoader(dev_iter, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "TBB9S0NBqNLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the values of the hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "emb_dim = 300\n",
        "hidden_dim = 4\n",
        "output_dim = 2\n",
        "learning_rate = 0.1\n",
        "num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Jod8FnWPs_Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model_ffnn2 = FeedforwardNeuralNetModel2(vocab_size, emb_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.SGD(model_ffnn2.parameters(), lr=learning_rate)\n",
        "model_ffnn2 = model_ffnn2.to(device)\n",
        "# Train the model\n",
        "train_woffset( model_ffnn2, train_loader, optimizer, num_epochs=5 )\n",
        "# Evaluate on dev\n",
        "gold, pred = evaluate_woffset( model_ffnn2, dev_loader )"
      ],
      "metadata": {
        "id": "1Xug7ygbpAhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test with LSTM\n",
        "\n",
        "The code below will laucnh an experiment with the LSTM archtecture and a continuous representation. \n",
        "\n",
        "We don't need offsets with LSTM, since we do not embed directly each sequence using EmbeddingBag. Below is thus a version of collate_batch that does not return the offsets. In this case, we can use the train / evaluation functions defined before.\n",
        "\n",
        "\n",
        "Note: be careful, the collate_batch function below return (input, label) while the previous one returns (label, input, offsets), thus in one train function we have *for input, label in train* while in the other we have *for label, input in train* (should be modified in further versions)"
      ],
      "metadata": {
        "id": "x1H86MYF-lph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch2(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return text_list.to(device), label.to(device)"
      ],
      "metadata": {
        "id": "DL53tkChstGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "batch_size = 1\n",
        "train_loader = DataLoader(train_iter, batch_size=batch_size, shuffle=True, collate_fn=collate_batch2)\n",
        "dev_loader = DataLoader(dev_iter, batch_size=batch_size, shuffle=True, collate_fn=collate_batch2)"
      ],
      "metadata": {
        "id": "lQ-GND1UsOc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the values for the hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "emb_dim = 300\n",
        "hidden_dim = 32\n",
        "output_dim = 2\n",
        "learning_rate = 0.1\n",
        "num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Ll4Q4w3ouyEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model_lstm2 = LSTMModel2(vocab_size, emb_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.SGD(model_lstm2.parameters(), lr=learning_rate)\n",
        "model_lstm2 = model_lstm2.to(device)\n",
        "\n",
        "# Train the model\n",
        "train( model_lstm2, train_loader, optimizer, num_epochs=5 )\n",
        "# Evaluate on dev\n",
        "gold, pred = evaluate( model_lstm2, dev_loader )"
      ],
      "metadata": {
        "id": "ROOOLRLasTtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an additional exercize, vary the hyper-parameters values and evaluate the models on the test set. "
      ],
      "metadata": {
        "id": "4WKeIN7Q-zG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART3: using mini-batches\n",
        "\n",
        "## With FFNN\n",
        "We have the code required to use mini-batches with FFNN. The function 'collate_batch' defined earlier makes a concatenation of the input data, and the offsets are used to retrieve the separate documents to be embeddded. \n",
        "\n",
        "▶▶ **Load the data with a batch of size 2 and run a FFNN.**"
      ],
      "metadata": {
        "id": "qRBQjE9Yz4tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n"
      ],
      "metadata": {
        "id": "naR8bdAb_MXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "vocab_size = len(vocab)\n",
        "emb_dim = 300\n",
        "hidden_dim = 32\n",
        "output_dim = 2\n",
        "learning_rate = 0.1\n",
        "num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Q-Ll7nT4ulx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "\n",
        "# Train the model\n",
        "\n",
        "# Evaluate on dev\n"
      ],
      "metadata": {
        "id": "Ev3AikiQusA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With LSTM\n",
        "\n",
        "When using LSTMs, we need a bit more work: the problem is that all the documents in a batch need to have the same length, because the size of the input defines the size of the network (each xi is associated with a state si). \n",
        "\n",
        "The solution is called **padding**: we add zeros at the end of the sequences that are shorter than the max length. \n",
        "\n",
        "The easiest solution to do so is to pad the sequences using *torch.nn.utils.rnn.pad_sequence* as done below within the *collate_batch_pad* function. This function returns a tensor of padded sequences, that can be directly used as input of our model.\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html"
      ],
      "metadata": {
        "id": "PMW11fIi_RPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_batch_pad(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    #text_list = torch.cat(text_list) # Instead of concatenating, we use padding\n",
        "    text_list = pad_sequence(text_list, padding_value=0) # <-------\n",
        "    return text_list.to(device), label.to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "3U9zTPil09aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We slightly modify our model, just to take into account a custom batch size. See in the forward pass:\n",
        "* the *view* method now has, as a 2nd argument, the batch size (while it was previously set to 1)"
      ],
      "metadata": {
        "id": "Jz6ONIX1AmDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel3(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, batch_size):\n",
        "        super(LSTMModel3, self).__init__()\n",
        "        self.trace = True\n",
        "        self.batch_size = batch_size # <------\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM( input_size=embedding_dim, \n",
        "                            hidden_size=hidden_dim, \n",
        "                            bidirectional=False)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "\n",
        "    def forward(self, text):\n",
        "        embeds = self.embedding(text)\n",
        "        if self.trace:\n",
        "          print( len(text), self.batch_size, embeds.shape)\n",
        "          self.trace = False\n",
        "        x = embeds.view(len(text), self.batch_size, -1) # <------\n",
        "        out, (ht, ct) = self.lstm( x )\n",
        "        y = self.fc2(ht[-1])\n",
        "        return y"
      ],
      "metadata": {
        "id": "tk2xBsPQz6ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now run an experiment with a batch size of 2.\n",
        "\n",
        "Note that we have another modification here in the Dataloader:\n",
        "* drop_last=True: drop the last incomplete batch \n",
        "\n",
        "▶▶ **Uncomment the 'print' in the forward function above and in the train loop to see what the data looks like (stop training when a few tensors are printed).**"
      ],
      "metadata": {
        "id": "EaODIVCQA05f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "batch_size = 2\n",
        "train_loader = DataLoader(train_iter, batch_size=batch_size, shuffle=True, \n",
        "                          collate_fn=collate_batch_pad, drop_last=True)\n",
        "dev_loader = DataLoader(dev_iter, shuffle=True, batch_size=2, \n",
        "                        collate_fn=collate_batch_pad, drop_last=True)"
      ],
      "metadata": {
        "id": "Dr42-G6ZvkU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "vocab_size = len(vocab)\n",
        "emb_dim = 300\n",
        "hidden_dim = 32\n",
        "output_dim = 2\n",
        "learning_rate = 0.1\n",
        "num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "J1vCbZ_3vsNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model_lstm3 = LSTMModel3( vocab_size, emb_dim, hidden_dim, output_dim, batch_size )\n",
        "optimizer = torch.optim.SGD(model_lstm3.parameters(), lr=learning_rate)\n",
        "model_lstm3 = model_lstm3.to(device)\n",
        "# Train the model\n",
        "train( model_lstm3, train_loader, optimizer, num_epochs=5 )"
      ],
      "metadata": {
        "id": "dA8gDgLtz8w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also modify the evaluation function to take batches as input."
      ],
      "metadata": {
        "id": "hJ9e0yn8Fgr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_batch( model, dev_loader ):\n",
        "    predictions = []\n",
        "    gold = []\n",
        "    with torch.no_grad():\n",
        "        for input, label in dev_loader:\n",
        "            input = input.to(device)\n",
        "            label = label.to(device)\n",
        "            probs = model(input)\n",
        "            # print( probs)\n",
        "            # print( torch.argmax(probs, dim=1).cpu().numpy())\n",
        "            # predictions.append( torch.argmax(probs, dim=1).cpu().numpy()[0] )\n",
        "            predictions.extend( torch.argmax(probs, dim=1).cpu().numpy() ) # <-----\n",
        "            # gold.append( int(label) )\n",
        "            gold.extend([int(l) for l in label])  # <-----\n",
        "    print(classification_report(gold, predictions))\n",
        "    return gold, predictions\n",
        "\n",
        "\n",
        "# Evaluate on dev\n",
        "gold, pred = evaluate_batch( model_lstm3, dev_loader )"
      ],
      "metadata": {
        "id": "LgfhrZ7L08RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART4: trying other architectures\n",
        "\n",
        "* Try to implement a GRU instead of an LSTM\n",
        "* Try to implement a bi-GRU\n",
        "\n",
        "As an additional exercize:\n",
        "* Try with multiple GRU layers\n",
        "* Try to add an hidden layer over the RNN\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.GRU.html"
      ],
      "metadata": {
        "id": "SaLSbelaE-Yl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Using a GRU\n",
        "\n",
        "▶▶ **Modify the code to use a GRU instead of an LSTM**"
      ],
      "metadata": {
        "id": "Luq11i8ThV6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, batch_size):\n",
        "          \n",
        "\n",
        "    def forward(self, text):\n",
        "        "
      ],
      "metadata": {
        "id": "dGJI32ljE-rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=2\n",
        "\n",
        "dataloader = DataLoader(train_iter, batch_size=batch_size, shuffle=False, \n",
        "                        collate_fn=collate_batch_pad, drop_last=True)"
      ],
      "metadata": {
        "id": "NIBwzJdYFaJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "vocab_size = len(vocab)\n",
        "emb_dim = 300\n",
        "hidden_dim = 32\n",
        "output_dim = 2\n",
        "\n",
        "learning_rate = 0.1\n",
        "num_epochs = 5\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "psUh9r5n0MM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_gru = GRUModel(vocab_size, emb_dim, hidden_dim, output_dim, batch_size)\n",
        "optimizer = torch.optim.SGD(model_gru.parameters(), lr=learning_rate)\n",
        "model_gru = model_gru.to(device)"
      ],
      "metadata": {
        "id": "EjXEj1cm0RnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, total_acc, total_count = 0, 0, 0\n",
        "    for text, label in dataloader:\n",
        "        text = text.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model_gru( text )\n",
        "        #print(text)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, label)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulating the loss over time\n",
        "        train_loss += loss.item()\n",
        "        total_acc += (outputs.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "\n",
        "    # Compute accuracy on train set at each epoch\n",
        "    print('Epoch: {}. Loss: {}. ACC {} '.format(epoch, train_loss/count_train, total_acc/count_train))\n",
        "        \n",
        "    total_acc, total_count = 0, 0\n",
        "    train_loss = 0"
      ],
      "metadata": {
        "id": "hvblSz53FdCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 bi-GRU\n",
        "\n",
        "▶▶ **Modify the code to implement a bi-directional GRU. Hint: what is the size of the output of a bi-RNN? what should be used for predictions?**"
      ],
      "metadata": {
        "id": "IyQiRBv_3_Y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiGRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, batch_size):\n",
        " \n",
        "\n",
        "    def forward(self, text):\n"
      ],
      "metadata": {
        "id": "n6AqVPfA4A2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=2\n",
        "\n",
        "dataloader = DataLoader(train_iter, batch_size=batch_size, shuffle=False, \n",
        "                        collate_fn=collate_batch_pad, drop_last=True)"
      ],
      "metadata": {
        "id": "MnP_uLrv4GZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "vocab_size = len(vocab)\n",
        "emb_dim = 300\n",
        "hidden_dim = 32\n",
        "output_dim = 2\n",
        "\n",
        "learning_rate = 0.1\n",
        "num_epochs = 5\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "rqOmLUQm4IgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_bigru = BiGRUModel(vocab_size, emb_dim, hidden_dim, output_dim, batch_size)\n",
        "optimizer = torch.optim.SGD(model_bigru.parameters(), lr=learning_rate)\n",
        "model_bigru = model_bigru.to(device)"
      ],
      "metadata": {
        "id": "rzHXdyVo4KkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, total_acc, total_count = 0, 0, 0\n",
        "    for text, label in dataloader:\n",
        "        text = text.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model_bigru( text )\n",
        "        #print(text)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, label)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulating the loss over time\n",
        "        train_loss += loss.item()\n",
        "        total_acc += (outputs.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "\n",
        "    # Compute accuracy on train set at each epoch\n",
        "    print('Epoch: {}. Loss: {}. ACC {} '.format(epoch, train_loss/count_train, total_acc/count_train))\n",
        "        \n",
        "    total_acc, total_count = 0, 0\n",
        "    train_loss = 0"
      ],
      "metadata": {
        "id": "cxg_n6fw4N3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART5: Sequence tagging\n",
        "\n",
        "## POS Tagging\n",
        "\n",
        "From: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html "
      ],
      "metadata": {
        "id": "rTsfKWeP7Jdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "training_data = [\n",
        "    # Tags are: DET - determiner; NN - noun; V - verb\n",
        "    # For example, the word \"The\" is a determiner\n",
        "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
        "]\n",
        "word_to_ix = {}\n",
        "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
        "for sent, tags in training_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
        "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
        "print(word_to_ix)\n",
        "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}  # Assign each tag with a unique index\n",
        "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "\n",
        "# These will usually be more like 32 or 64 dimensional.\n",
        "# We will keep them small, so we can see how the weights change as we train.\n",
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6"
      ],
      "metadata": {
        "id": "6A5Z-EsT7w_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        #print('embeds.shape', embeds.shape)\n",
        "        #print(embeds.view(len(sentence), 1, -1).shape)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1)) # the whole output, vs output[-1] for classif\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1) # required with nn.NLLLoss()\n",
        "        return tag_scores"
      ],
      "metadata": {
        "id": "7O_41eHi71SO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function = nn.NLLLoss() # does not include the softmax\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# See what the scores are before training\n",
        "# Note that element i,j of the output is the score for tag j for word i.\n",
        "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "    print(tag_scores)\n",
        "\n",
        "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    for sentence, tags in training_data:\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
        "        # Tensors of word indices.\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = prepare_sequence(tags, tag_to_ix)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        tag_scores = model(sentence_in)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        #  calling optimizer.step()\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "jiSYatB48Brn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See what the scores are after training\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "    predictions = torch.argmax(tag_scores, dim=1).cpu().numpy()\n",
        "    print(tag_scores)\n",
        "    print(predictions)\n",
        "    print(training_data[0][0])\n",
        "    print( [ix_to_tag[p] for p in predictions])\n",
        "\n",
        "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
        "    # for word i. The predicted tag is the maximum scoring tag.\n",
        "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
        "    # since 0 is index of the maximum value of row 1,\n",
        "    # 1 is the index of maximum value of row 2, etc.\n",
        "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
        "    #print(tag_scores)"
      ],
      "metadata": {
        "id": "Q_xgiu1MBAnB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}