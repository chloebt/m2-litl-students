{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TP1_masterLiTL_2122.ipynb","provenance":[{"file_id":"1M2CpVop1hZP6tgXcyLLnqiQwCqD0VCiH","timestamp":1636406475085}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lmulT50Qopks"},"source":["# TP1: Machine learning (reminder)\n","Master LiTL - 2021-2022\n","\n","## Requirements\n","In this practical session, we will explore machine learning models for NLP applications ; specifically, we will train a classifier for sentiment analysis on a French dataset of movie reviews. \n","For these exercises, we will make use of Python (v3.*), and a number of modules for data processing and machine learning: *numpy*, *scipy*, *scikit-learn*, *pandas* and *spacy* . \n","If  you  want  to  use  your  own  computer  you  will  need  to  make  sure  these  are  installed  (e.g.  using  the command *pip*). If you’re using *Miniconda*, you can use the command\n","```\n","conda install <modulename>\n","```\n","\n","\n","First,  download  the  archive  for  the  practical  session  from  the  course  page  to  an  appropriate working directory, and unzip it. Under linux, you can issue the following commands :\n","```\n","$ unzip tp3.zip \n","$ cd tp3\n","```\n"]},{"cell_type":"code","metadata":{"id":"wf3lJWgabkfB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636533585208,"user_tz":-60,"elapsed":34614,"user":{"displayName":"Ludovic Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04032220066414696094"}},"outputId":"e9f5d3db-5bb7-47a4-df44-182aa314ee7d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"D4dW8cDBpG2v"},"source":["## Task and dataset\n","\n","We’ll go through the following stages of an NLP machine learning pipeline, using sentiment classification as an application:\n","* data preprocessing (tokenization) \n","* feature extraction\n","* model training\n","* evaluation\n","\n","As a dataset, we’ll be using a set of reviews for television series in French, extracted from the website allocine.fr. \n","The dataset consists of the text of the review, as well as a sentiment label (positive or negative).\n","\n","The training set is divided into a training part (for training, 5576 reviews, ± 90%) and test part (for evaluation, 544 reviews, ± 10%). \n","The dataset is balanced, which means positive and negative instances are evenly distributed. \n","Additionally, training and test set contain reviews about different TV series (in order to avoid possible bias when evaluating)."]},{"cell_type":"markdown","metadata":{"id":"PKlLeW1Rp3Hl"},"source":["## Exercise 1: Preprocessing (code given)\n","\n","First, we’ll load the training set and axplore the dataset.\n","\n"]},{"cell_type":"code","metadata":{"id":"vjWd_ZWYj9i0","colab":{"base_uri":"https://localhost:8080/","height":465},"executionInfo":{"status":"error","timestamp":1636533638844,"user_tz":-60,"elapsed":336,"user":{"displayName":"Ludovic Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04032220066414696094"}},"outputId":"1f09e222-470c-4131-fd4d-69e189c87b71"},"source":["import pandas as pd\n","\n","train_path = \"allocine_train.tsv\"\n","dev_path = \"allocine_dev.tsv\"\n","test_path = \"allocine_test.tsv\"\n","\n","train = pd.read_csv(train_path, header=0, delimiter='\\t', quoting=3) #'allocine_train.tsv'\n","print(\"TRAIN:\", train.shape)\n","print(train.columns.values)\n","print(train['sentiment'][0], train['review'][0])\n","print()\n","\n","dev = pd.read_csv(dev_path, header=0, delimiter='\\t', quoting=3) \n","print(\"DEV:\", dev.shape)\n","print()\n","\n","test = pd.read_csv(test_path, header=0, delimiter='\\t', quoting=3) \n","print(\"TEST:\", test.shape)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-51e09c83b62c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Work/Teaching/M2-LITL/M2-LiTL-Students/TP1/data/allocine_test.tsv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#'allocine_train.tsv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TRAIN:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Work/Teaching/M2-LITL/M2-LiTL-Students/TP1/data/allocine_train.tsv'"]}]},{"cell_type":"markdown","metadata":{"id":"smVT_Vt1qA-W"},"source":["We need to preprocess the dataset to be able to properly extract features from it.\n","In order to do so, we’ll create a function that makes use of spacy’s preprocessing pipeline. \n","\n","You need to import the Spacy model for French. Two options (second not tested): \n","\n","```\n","!python -m spacy download en_core_web_lg\n","# or\n","import spacy.cli\n","spacy.cli.download(\"fr_core_news_sm\")\n","```"]},{"cell_type":"code","metadata":{"id":"fJJpf5la8cFd"},"source":["import spacy.cli\n","spacy.cli.download(\"fr_core_news_sm\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vANVT9eOqBMz"},"source":["# Preprocessing = tokenize data\n","import spacy\n","nlp = spacy.load('fr_core_news_sm', disable=['tagger', 'parser', 'ner'])\n","\n","\n","def preprocess_data( dataset ):\n","  num_reviews = dataset['review'].size\n","  print(\"#Reviews =\", num_reviews)\n","  dataset_tok = []\n","  for i in range(num_reviews):\n","      clean_review = review_to_tokens(dataset['review'][i])\n","      dataset_tok.append(clean_review)\n","  for i, r in enumerate(dataset_tok[:2]):\n","      print('\\n', i, r) \n","  return dataset_tok\n","\n","def review_to_tokens(raw_review):\n","    doc = nlp(raw_review)\n","    tokenList = [token.text for token in doc]\n","    tokenized_string = ' '.join(tokenList)\n","    tokenized_string_lowercase = tokenized_string.lower()\n","    return tokenized_string_lowercase\n","\n","print(\"-- Preprocess TRAIN:\")\n","train_tok = preprocess_data( train )\n","\n","print(\"\\n-- Preprocess DEV:\")\n","dev_tok = preprocess_data( dev )\n","\n","print(\"\\n-- Preprocess TEST:\")\n","test_tok = preprocess_data( test )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"90iwyLkc9hw8"},"source":["### Exercise 2: Feature extraction \n","\n","Now it’s time to decide which features to use in our classifier. We’ll start with simple bag of words features.\n","\n","▶▶ **TODO: write the code to vectorize the dev set.**"]},{"cell_type":"code","metadata":{"id":"ZGnHRsH_9jtk"},"source":["# Vectorizing data: BOW representation\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer_bow = CountVectorizer( analyzer = 'word', max_features = 500 )\n","\n","print(\"Vectorize TRAIN:\")\n","train_feats_bow = vectorizer_bow.fit_transform( train_tok )\n","print(train_feats_bow.shape)\n","\n","vocab = vectorizer_bow.get_feature_names()\n","print(\"Vocabulary:\", vocab[:20])\n","\n","# --------------------------------------------------------\n","# TODO: Write the code to vectorize the DEV set\n","# --------------------------------------------------------\n","print(\"\\nVectorize DEV:\")\n","\n","\n","# Vocabulary should remain the same!\n","print(\"Vocabulary:\", vocab[:20])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u3VCUNqZ9ohj"},"source":["### Exercise 3: Classification (Code given)\n","\n","We’ll start with the simplest classifier, yet often performing well: Naive Bayes.\n","Train the classifier et report its performance on the dev set.\n","\n"]},{"cell_type":"code","metadata":{"id":"CssLBO2SBPMN"},"source":["## Classification with NAIVE BAYES\n","\n","# Train the classifier\n","from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n","classifier = MultinomialNB()\n","classifier.fit(train_feats_bow, train['sentiment'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKuY2GrsKv3R"},"source":["# Compute the performance on the dev set\n","score = classifier.score( dev_feats_bow, dev['sentiment'] )\n","print(score)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0KdSuShTBQ00"},"source":["#### Exercise 3-b (code given)\n","* What does the score represent ?\n","* Look at the instances that were classified badly. Do you see why the review was misclassified ? "]},{"cell_type":"code","metadata":{"id":"0fek-bze96bp"},"source":["## Look at misclassified instances\n","pred = classifier.predict( dev_feats_bow )\n","print(pred) # = matrix, illisible\n","#print(test['sentiment']) #gold - y_test\n","print()\n","\n","print('Misclassified examples: ')\n","count_err = 0\n","for i in range(len(pred)):\n","    if pred[i] != dev['sentiment'][i]:\n","        print( \"\\nGOLD=\", dev['sentiment'][i], \"PRED=\",pred[i] , i, dev['review'][i])\n","        count_err += 1\n","        \n","print( \"CHECK: \", \"#Total=\", len(pred), \"#Errors=\", count_err, \"Acc=\", (len(pred)-count_err)/len(pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LBe-8okB-9oc"},"source":["### Exercise 4: Experiment with different feature sets.\n","\n","Here, we'll just try bi-grams.\n","\n","▶▶ **TODO: write the code to vectorize the data into bigrams. Keep 'max_features = 500'. Then retrain and evaluate the classifier.**\n","\n","We could also have tried to:\n","  * Exclude a list of stopwords (high-frequency words that are considered too general to be meaningful, such as une or le)\n","  * Experiment with n-grams with n>2 \n","  * Combine features (e.g. BOW + bi-grams)\n","  * Can you think of other features to include?"]},{"cell_type":"code","metadata":{"id":"SkyrhwGx-jYz"},"source":["# --------------------------------------------------------\n","# TODO: Write the code to vectorize to extract bigrams\n","# --------------------------------------------------------\n","\n","#vectorizer_big = ...\n","\n","# --------------------------------------------------------\n","# TODO: Vectorize the train and dev sets\n","# --------------------------------------------------------\n","print(\"Vectorize TRAIN:\")\n","\n","\n","print(\"\\nVectorize DEV:\")\n","\n","\n","vocab = vectorizer_big.get_feature_names()\n","print(\"\\nVocabulary:\", vocab[:20])\n","\n","# --------------------------------------------------------\n","# TODO: Train a Naive Bayes classifier and evaluate on dev\n","# --------------------------------------------------------\n","print(\"\\nTraining classifier\")\n","#classifier_big = ...\n","\n","# Compute the performance on the dev set\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-rFwwod5_gXI"},"source":["### Exercise 5\n","\n","Experiment with different classifiers, compare:\n","* Naive Bayes \n","* MaxEnt\n","\n","▶▶ **Compare the results obtained with NB to the ones obtained with MaxEnt.**\n","\n","▶▶ **When you're done, try what happens if you remove 'max_features = 500'. What do you conclude?**"]},{"cell_type":"code","metadata":{"id":"kHrZ3vI-_9zE"},"source":["from sklearn.linear_model import LogisticRegression\n","\n","# --------------------------------------------------------\n","# TODO: Train a MaxEnt classifier and evaluate on dev, using the best features\n","# --------------------------------------------------------\n","\n","# Train a model with MaxEnt\n","\n","\n","# Compute the performance on the dev set\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BEMs98mR_-AG"},"source":["### Exercise 6\n","\n","You’ve determined the best feature set and classification algorithm (missing: the best set of hyper-parameters). \n","\n","▶▶ **compute the performance on the test set**."]},{"cell_type":"code","metadata":{"id":"YJYV84q6_-o0"},"source":["# --------------------------------------------------------\n","# TODO: Compute the final results on the TEST set\n","# --------------------------------------------------------\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y71z29pK-jwS"},"source":["## Intrinsic model evaluation (code given)\n","\n","Some models allow us to look at the most informative features. \n","\n","▶▶ **Examine both the top and the bottom of the list. Which features are most informative ?**"]},{"cell_type":"code","metadata":{"id":"U4mRULuT-TQi"},"source":["classifier_lr_bow = LogisticRegression()\n","classifier_lr_bow.fit(train_feats_bow, train['sentiment'])\n","\n","vocab = vectorizer_bow.get_feature_names()\n","\n","allCoefficients = [(classifier_lr_bow.coef_[0,i], vocab[i]) for i in range(len(vocab))]\n","allCoefficients.sort()\n","allCoefficients.reverse()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"in907YQ4-oXv"},"source":["print(\"Top features for positive class:\")\n","print( '\\n'.join( [ f+':\\t'+str((round(w,3))) for (w,f) in allCoefficients[:50]] ) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AdrYsRJ7-rMf"},"source":["print(\"Top features for negative class:\")\n","print( '\\n'.join( [ f+':'+str((round(w,3))) for (w,f) in allCoefficients[-50:]] ) )"],"execution_count":null,"outputs":[]}]}