{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP4_PART1_masterLiTL_2122_WSOL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCHhtzOXQ2po"
      },
      "source": [
        "# TP 4: Training a Feedforward neural network\n",
        "Master LiTL - 2021-2022\n",
        "\n",
        "## Requirements\n",
        "In this section, we will investigate variations of the setting and the hyper-parameter values of a feedforward NN, still on sentiment analysis on a French dataset of reviews.\n",
        "Our goal is to find the best model for the task, we will thus make use of the development set this time! \n",
        "\n",
        "We will explore:    \n",
        "* varied architectures\n",
        "* varied optimizers\n",
        "* varied activation functions\n",
        "* varied values for the hyper-parameters\n",
        "\n",
        "And in the part 2:\n",
        "* varied representation: sparse and continuous bag-of-words\n",
        "* Add plots of cost function\n",
        "* Add plots of number of training examples "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nT7n30VpCOzF",
        "outputId": "feb7b465-d461-45af-973b-8809c6438d9f"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If you’re using Colab, allocate a GPU by going to Edit > Notebook Settings.\n",
        "# We move our tensor to the GPU if available\n",
        "if torch.cuda.is_available():\n",
        "  print(f\"GPU ok\")\n",
        "else:\n",
        "  print(\"no gpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdSyhJqpVczO"
      },
      "source": [
        "\n",
        "## 1. Code for running a FFNN\n",
        "\n",
        "### 1.1 Read the data\n",
        "\n",
        "The code below is the same as last time: the input is a BoW representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoVJ18s_oxkn",
        "outputId": "5652c004-c2aa-45eb-f171-aa5082f7733c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import sklearn\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "train_path = \"allocine_train.tsv\"\n",
        "dev_path = \"allocine_dev.tsv\"\n",
        "test_path = \"allocine_test.tsv\"\n",
        "\n",
        "# This will be the size of the vectors reprensenting the input\n",
        "MAX_FEATURES = 5000 \n",
        "\n",
        "# Load train set\n",
        "train_df = pd.read_csv(train_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "    \n",
        "# -- VECTORIZE\n",
        "print(\"Creating features from bag of words...\")  \n",
        "vectorizer = CountVectorizer( analyzer = \"word\", max_features = MAX_FEATURES ) \n",
        "train_data_features = vectorizer.fit_transform(train_df[\"review\"])\n",
        "# -- TO DENSE\n",
        "x_train = train_data_features.toarray()\n",
        "y_train = np.asarray(train_df[\"sentiment\"])\n",
        "print( \"TRAIN:\", x_train.shape )\n",
        "\n",
        "dev_df = pd.read_csv(dev_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "dev_data_features = vectorizer.transform(dev_df[\"review\"])\n",
        "x_dev = dev_data_features.toarray()\n",
        "y_dev = np.asarray(dev_df[\"sentiment\"])\n",
        "print( \"DEV:\", x_dev.shape )\n",
        "\n",
        "test_df = pd.read_csv(test_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "test_data_features = vectorizer.transform(test_df[\"review\"])\n",
        "x_test = test_data_features.toarray()\n",
        "y_test = np.asarray(test_df[\"sentiment\"])\n",
        "print( \"TEST:\", x_test.shape )\n",
        "\n",
        "count_train = x_train.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating features from bag of words...\n",
            "TRAIN: (5027, 5000)\n",
            "DEV: (549, 5000)\n",
            "TEST: (544, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Load the data\n",
        "\n",
        "Note that batch size is chosen here."
      ],
      "metadata": {
        "id": "NKM4vI_bny7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data into TENSORS\n",
        "\n",
        "def load_data( x_train, y_train, x_dev, y_dev, x_test, y_test, batch_size=1 ):\n",
        "    #batch_size = 1 # == no batch\n",
        "    # create Tensor dataset\n",
        "    train_data = TensorDataset(torch.from_numpy(x_train).to(torch.float), torch.from_numpy(y_train))\n",
        "\n",
        "    # dataloaders\n",
        "    # make sure to SHUFFLE your data\n",
        "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "    \n",
        "    # Don t need batch at test time\n",
        "    dev_data = TensorDataset(torch.from_numpy(x_dev).to(torch.float), torch.from_numpy(y_dev))\n",
        "    dev_loader = DataLoader(dev_data, shuffle=True, batch_size=1)\n",
        "\n",
        "    test_data = TensorDataset(torch.from_numpy(x_test).to(torch.float), torch.from_numpy(y_test))\n",
        "    test_loader = DataLoader(test_data, shuffle=True, batch_size=1)\n",
        "\n",
        "    print('BATCH SIZE:', batch_size)\n",
        "\n",
        "    return train_loader, dev_loader, test_loader"
      ],
      "metadata": {
        "id": "VDZsMXoUnyCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOeZCY09o6CV"
      },
      "source": [
        "### 1.3 Neural Network Definition\n",
        "\n",
        "Now we can build our learning model.\n",
        "\n",
        "▶▶**What are the elements that can be changed here?**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOLUTION\n",
        "Note that here you can change: hidden_dim, number of hidden layers, activation function."
      ],
      "metadata": {
        "id": "2ku11PL8payo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvmc-_zqoxvF"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
        "        # just always do it in an nn.Module\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "\n",
        "        # Define the parameters that you will need. \n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Non-linearity\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        print( 'INPUT DIM:', input_dim, 'HIDDEN DIM:', hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Linear function  # LINEAR\n",
        "        out = self.fc1(x)\n",
        "\n",
        "        # Non-linearity  # NON-LINEAR\n",
        "        out = self.sigmoid(out) \n",
        "\n",
        "        # Linear function (readout)  # LINEAR\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Training function\n",
        "\n",
        "Below, the code for a function that trains a model.\n",
        "\n",
        "▶▶**What are the elements that can be changed here?**"
      ],
      "metadata": {
        "id": "DVrIvVbjoxU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOLUTION\n",
        "\n",
        "Here you can change: the total number of epochs, the criterion / loss, the optimizer which also includes the learning rate."
      ],
      "metadata": {
        "id": "JDdIcpBBpdpc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnNx8hZJox3v"
      },
      "source": [
        "# TRAINING\n",
        "def train( model, train_loader, optimizer, num_epochs=5 ):\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, total_acc, total_count = 0, 0, 0\n",
        "        for input, label in train_loader:\n",
        "            # Step1. Clearing the accumulated gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Step 2. Forward pass to get output/logits\n",
        "            outputs = model( input )\n",
        "\n",
        "            # Step 3. Compute the loss, gradients, and update the parameters by\n",
        "            # calling optimizer.step()\n",
        "            # - Calculate Loss: softmax --> cross entropy loss\n",
        "            loss = criterion(outputs, label)\n",
        "            # - Getting gradients w.r.t. parameters\n",
        "            loss.backward()\n",
        "            # - Updating parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulating the loss over time\n",
        "            train_loss += loss.item()\n",
        "            total_acc += (outputs.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "\n",
        "        # Compute accuracy on train set at each epoch\n",
        "        print('Epoch: {}. Loss: {}. ACC {} '.format(epoch, train_loss/count_train, total_acc/count_train))\n",
        "        \n",
        "        total_acc, total_count = 0, 0\n",
        "        train_loss = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQRgrMEael9W"
      },
      "source": [
        "### 1.5 Evaluation\n",
        "\n",
        "Below you have the code for a function that can be used to evaluate the model: it prints the classification report and return the gold and predicted labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldDubAPDox5K"
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "\n",
        "def evaluate( model, dev_loader ):\n",
        "    predictions = []\n",
        "    gold = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input, label in dev_loader:\n",
        "            probs = model(input)\n",
        "            predictions.append( torch.argmax(probs, dim=1).cpu().numpy()[0] )\n",
        "            gold.append(int(label))\n",
        "\n",
        "    print(classification_report(gold, predictions))\n",
        "    return gold, predictions\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Runing an experiment\n",
        "\n",
        "Below a function that could be used to save the results, don't hesitate to write your own or modify it."
      ],
      "metadata": {
        "id": "eGJ2WtzFpKgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Save the scores and settings\n",
        "my_expe = 'scores_ffnn_bow.txt'\n",
        "\n",
        "def write_expe_settings( my_file, batch=1, hidden=1, hsize=4, act='sigmoid', lr=0.1, opt='sgd', epochs=5, score=0. ):\n",
        "  with open( my_file, 'a' ) as f:\n",
        "    f.write( 'batch:{batch}\\thidden:{hidden}\\thsize:{hsize}\\tact:{act}\\tlr:{lr}\\topt:{opt}\\tepochs:{epochs}\\tscore:{score}\\n'.format( \n",
        "        batch=batch, hidden=hidden, hsize=hsize, act=act, lr=lr, opt=opt, epochs=epochs, score=score ) )"
      ],
      "metadata": {
        "id": "EK7KkDp4XXmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E05Ui0G9es1o"
      },
      "source": [
        "### TEST #1\n",
        "\n",
        "Start testing! The first test is the one with the 'default' parameters used in the previous practical session.\n",
        "\n",
        "▶▶**Describe the setting of the 'default' experiment**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOLUTION\n",
        "\n",
        "**BoW, 5000 features, Batch size: 1, 1 hidden layer, hidden size: 4, activation: sigmoid, learning rate: 0.1, optimizer: SGD, max epochs: 5**"
      ],
      "metadata": {
        "id": "2x0xsRY1sZDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶▶ **Run the model and evaluate on dev. Save the score for future comparison.**"
      ],
      "metadata": {
        "id": "RRCdktsEw3AC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_loader, dev_loader, test_loader = load_data( x_train, y_train, x_dev, y_dev, x_test, y_test, batch_size=1 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WV9IEMA8oxlQ",
        "outputId": "5cff42da-0722-4972-92d8-f2d85cdaca4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH SIZE: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcGyjXbUoxx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0456396-2ef3-4c98-a851-2ff898c3707f"
      },
      "source": [
        "# Many choices here!\n",
        "VOCAB_SIZE = MAX_FEATURES\n",
        "input_dim = VOCAB_SIZE \n",
        "hidden_dim = 4\n",
        "output_dim = 2\n",
        "\n",
        "learning_rate = 0.1\n",
        "num_epochs = 5\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "optimizer = torch.optim.SGD( model_bow.parameters(), lr=learning_rate )\n",
        "\n",
        "# Train and evaluate\n",
        "\n",
        "train( model_bow, train_loader, optimizer )\n",
        "\n",
        "gold, pred = evaluate( model_bow, dev_loader )\n",
        "accuracy = accuracy_score( gold, pred )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT DIM: 5000 HIDDEN DIM: 4\n",
            "Epoch: 0. Loss: 0.5356872780121616. ACC 0.7266759498706983 \n",
            "Epoch: 1. Loss: 0.3817175804624497. ACC 0.8283270340163119 \n",
            "Epoch: 2. Loss: 0.3384615813505414. ACC 0.8597573105231748 \n",
            "Epoch: 3. Loss: 0.2843738552404072. ACC 0.8828327034016312 \n",
            "Epoch: 4. Loss: 0.26506259036045465. ACC 0.8905908096280087 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.88      0.79       230\n",
            "           1       0.89      0.74      0.81       319\n",
            "\n",
            "    accuracy                           0.80       549\n",
            "   macro avg       0.80      0.81      0.80       549\n",
            "weighted avg       0.82      0.80      0.80       549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "write_expe_settings( my_expe, batch=1, hidden=1, hsize=4, act='sigmoid', lr=0.1, opt='sgd', epochs=5, score=accuracy )"
      ],
      "metadata": {
        "id": "PwsL7C3XXly9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "169R-kzbTU4E"
      },
      "source": [
        "## 3. Exercises\n",
        "\n",
        "▶▶ **Now, try to change:**\n",
        "1. Batch size \n",
        "2. Max number of epochs (with best batch size)\n",
        "3. Size of the hidden layer\n",
        "4. Activation function\n",
        "5. Optimizer\n",
        "6. Learning rate\n",
        "7. Try with 1 additional layers \n",
        " \n",
        "\n",
        "How does this affect the loss and the performance of the model?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 3.1 Batch size\n",
        "---\n",
        "\n",
        "Let's try with: 1, 10, 100, 1000"
      ],
      "metadata": {
        "id": "ayJIGZltUtlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TEST #2\n",
        "\n",
        "▶▶**Describe the setting of the second experiment**"
      ],
      "metadata": {
        "id": "NTaVQNUXVQcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SOLUTION\n",
        "\n",
        "BoW, 5000 features, **Batch size: 10**, 1 hidden layer, hidden size: 4, activation: sigmoid, learning rate: 0.1, optimizer: SGD, max epochs: 5"
      ],
      "metadata": {
        "id": "OXAqBt-J0u5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "\n",
        "# Load data\n",
        "train_loader, dev_loader, test_loader = load_data( x_train, y_train, x_dev, y_dev, x_test, y_test, batch_size=batch_size )\n",
        "\n",
        "# Hyper-parameters\n",
        "hidden_dim = 4\n",
        "output_dim = 2\n",
        "learning_rate = 0.1\n",
        "num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.SGD( model_bow.parameters(), lr=learning_rate )\n",
        "\n",
        "# Train and evaluate\n",
        "train( model_bow, train_loader, optimizer )\n",
        "gold, pred = evaluate( model_bow, dev_loader )\n",
        "accuracy = accuracy_score( gold, pred )\n",
        "\n",
        "write_expe_settings( my_expe, batch=batch_size, hidden=1, hsize=4, act='sigmoid', lr=0.1, opt='sgd', epochs=5, score=accuracy )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPf2jBZ9VMTa",
        "outputId": "d6e15db5-2684-4f8f-85da-61f29a71c40a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH SIZE: 10\n",
            "INPUT DIM: 5000 HIDDEN DIM: 4\n",
            "Epoch: 0. Loss: 0.0595094999511507. ACC 0.6942510443604536 \n",
            "Epoch: 1. Loss: 0.04239837287291395. ACC 0.821563556793316 \n",
            "Epoch: 2. Loss: 0.032071483498785636. ACC 0.8703003779590213 \n",
            "Epoch: 3. Loss: 0.027037093746738024. ACC 0.8903918838273324 \n",
            "Epoch: 4. Loss: 0.023645152846399065. ACC 0.9063059478814403 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.91      0.81       230\n",
            "           1       0.92      0.76      0.84       319\n",
            "\n",
            "    accuracy                           0.83       549\n",
            "   macro avg       0.83      0.84      0.82       549\n",
            "weighted avg       0.84      0.83      0.83       549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TEST #3\n",
        "\n",
        "BoW, 5000 features, **Batch size: 100**, 1 hidden layer, hidden size: 4, activation: sigmoid, learning rate: 0.1, optimizer: SGD, max epochs: 5"
      ],
      "metadata": {
        "id": "arxB2-EScgCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "\n",
        "# Load data\n",
        "train_loader, dev_loader, test_loader = load_data( x_train, y_train, x_dev, y_dev, x_test, y_test, batch_size=batch_size )\n",
        "\n",
        "# Hyper-parameters\n",
        "hidden_dim = 4\n",
        "output_dim = 2\n",
        "learning_rate = 0.1\n",
        "num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.SGD( model_bow.parameters(), lr=learning_rate )\n",
        "\n",
        "# Train and evaluate\n",
        "train( model_bow, train_loader, optimizer )\n",
        "gold, pred = evaluate( model_bow, dev_loader )\n",
        "accuracy = accuracy_score( gold, pred )\n",
        "\n",
        "write_expe_settings( my_expe, batch=batch_size, hidden=1, hsize=4, act='sigmoid', lr=0.1, opt='sgd', epochs=5, score=accuracy )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FAQEfiMVMWC",
        "outputId": "29f380f8-ac02-4672-85ae-b4fef68e93d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH SIZE: 100\n",
            "INPUT DIM: 5000 HIDDEN DIM: 4\n",
            "Epoch: 0. Loss: 0.006877064195048247. ACC 0.5866321861945495 \n",
            "Epoch: 1. Loss: 0.006576847776352636. ACC 0.7012134473841257 \n",
            "Epoch: 2. Loss: 0.006268150068453441. ACC 0.7370200915058683 \n",
            "Epoch: 3. Loss: 0.0059588611042947735. ACC 0.7525363039586235 \n",
            "Epoch: 4. Loss: 0.005636463612051259. ACC 0.7718321066242292 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.83      0.72       230\n",
            "           1       0.85      0.65      0.74       319\n",
            "\n",
            "    accuracy                           0.73       549\n",
            "   macro avg       0.74      0.74      0.73       549\n",
            "weighted avg       0.76      0.73      0.73       549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TEST #4\n",
        "\n",
        "BoW, 5000 features, **Batch size: 1000**, 1 hidden layer, hidden size: 4, activation: sigmoid, learning rate: 0.1, optimizer: SGD, max epochs: 5"
      ],
      "metadata": {
        "id": "YtQNdnKm2OKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1000\n",
        "\n",
        "# Load data\n",
        "train_loader, dev_loader, test_loader = load_data( x_train, y_train, x_dev, y_dev, x_test, y_test, batch_size=batch_size )\n",
        "\n",
        "# Hyper-parameters\n",
        "hidden_dim = 4\n",
        "output_dim = 2\n",
        "learning_rate = 0.1\n",
        "num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.SGD( model_bow.parameters(), lr=learning_rate )\n",
        "\n",
        "# Train and evaluate\n",
        "train( model_bow, train_loader, optimizer )\n",
        "gold, pred = evaluate( model_bow, dev_loader )\n",
        "accuracy = accuracy_score( gold, pred )\n",
        "\n",
        "write_expe_settings( my_expe, batch=batch_size, hidden=1, hsize=4, act='sigmoid', lr=0.1, opt='sgd', epochs=5, score=accuracy )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCNlyqK22OW_",
        "outputId": "399d880c-7ca7-4d94-c3e3-bb94a6a75c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH SIZE: 1000\n",
            "INPUT DIM: 5000 HIDDEN DIM: 4\n",
            "Epoch: 0. Loss: 0.0008264508640076072. ACC 0.5090511239307738 \n",
            "Epoch: 1. Loss: 0.0008226839856264817. ACC 0.5090511239307738 \n",
            "Epoch: 2. Loss: 0.0008219510275349599. ACC 0.533717923214641 \n",
            "Epoch: 3. Loss: 0.0008196555313685554. ACC 0.6329818977521384 \n",
            "Epoch: 4. Loss: 0.0008168000100217189. ACC 0.6461110005967774 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.41      0.50       230\n",
            "           1       0.66      0.83      0.74       319\n",
            "\n",
            "    accuracy                           0.66       549\n",
            "   macro avg       0.65      0.62      0.62       549\n",
            "weighted avg       0.65      0.66      0.64       549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answers: Batch size\n",
        "\n",
        "Increasing the batch size leads to a faster training a degradation in terms of performance\n",
        "\n",
        "see e.g. (Keskar et al. 2016) https://arxiv.org/abs/1609.04836 :\n",
        "* *It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize ... large-batch methods tend to converge to sharp minimizers of the training and testing functions-and as is well known, sharp minima lead to poorer generalization*\n",
        "\n",
        "Trade-off between faster training and generalization ability.\n",
        "\n",
        "People often test typical values of 32, 64, 128, 256, 512 and 1024 (but it depends also on the size of the training data, here we have a very small dataset).\n"
      ],
      "metadata": {
        "id": "vBQISOcXrzPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 3.2 Number of epochs\n",
        "---\n",
        "\n",
        "Let's try with: 5, 50"
      ],
      "metadata": {
        "id": "XejMdU5ocyBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TEST #5\n",
        "\n",
        "BoW, 5000 features, Batch size: 10, 1 hidden layer, hidden size: 4, activation: sigmoid, learning rate: 0.1, optimizer: SGD, **max epochs: 50**"
      ],
      "metadata": {
        "id": "vE_cC-bmdfVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "max_epochs = 50\n",
        "\n",
        "# Load data\n",
        "train_loader, dev_loader, test_loader = load_data( x_train, y_train, x_dev, y_dev, x_test, y_test, batch_size=batch_size )\n",
        "\n",
        "# Hyper-parameters\n",
        "hidden_dim = 4\n",
        "output_dim = 2\n",
        "learning_rate = 0.1\n",
        "#num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.SGD( model_bow.parameters(), lr=learning_rate )\n",
        "\n",
        "# Train and evaluate\n",
        "train( model_bow, train_loader, optimizer, num_epochs=max_epochs )\n",
        "gold, pred = evaluate( model_bow, dev_loader )\n",
        "accuracy = accuracy_score( gold, pred )\n",
        "\n",
        "write_expe_settings( my_expe, batch=batch_size, hidden=1, hsize=4, act='sigmoid', lr=0.1, opt='sgd', epochs=max_epochs, score=accuracy )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQreSe8pVMYZ",
        "outputId": "b5ca255e-d56a-4bce-f698-df30777a7284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0. Loss: 0.06248715755264589. ACC 0.6560572906305948 \n",
            "Epoch: 1. Loss: 0.043954694392028044. ACC 0.8187785955838472 \n",
            "Epoch: 2. Loss: 0.03317398302700213. ACC 0.8671175651481997 \n",
            "Epoch: 3. Loss: 0.027742600293360578. ACC 0.8931768450368013 \n",
            "Epoch: 4. Loss: 0.023334740119928248. ACC 0.9098866122936144 \n",
            "Epoch: 5. Loss: 0.02063481365438897. ACC 0.922816789337577 \n",
            "Epoch: 6. Loss: 0.01796321026048628. ACC 0.9321663019693655 \n",
            "Epoch: 7. Loss: 0.01650080053342878. ACC 0.9355480405808634 \n",
            "Epoch: 8. Loss: 0.014617698005509348. ACC 0.9478814402227969 \n",
            "Epoch: 9. Loss: 0.013585968076462463. ACC 0.9486771434255022 \n",
            "Epoch: 10. Loss: 0.011394190632573103. ACC 0.9610105430674358 \n",
            "Epoch: 11. Loss: 0.01055024223769336. ACC 0.9626019494728466 \n",
            "Epoch: 12. Loss: 0.009543743463070452. ACC 0.9689675750944897 \n",
            "Epoch: 13. Loss: 0.007826015936159433. ACC 0.9747364233141038 \n",
            "Epoch: 14. Loss: 0.007118049319019281. ACC 0.9763278297195146 \n",
            "Epoch: 15. Loss: 0.006425816581140528. ACC 0.9811020489357469 \n",
            "Epoch: 16. Loss: 0.005855514550039469. ACC 0.981698826337776 \n",
            "Epoch: 17. Loss: 0.006090072941300238. ACC 0.9805052715337179 \n",
            "Epoch: 18. Loss: 0.005031316356754121. ACC 0.9854784165506266 \n",
            "Epoch: 19. Loss: 0.004489544920905089. ACC 0.9884623035607718 \n",
            "Epoch: 20. Loss: 0.0040233143858605745. ACC 0.9916451163715934 \n",
            "Epoch: 21. Loss: 0.003751913867000641. ACC 0.9910483389695643 \n",
            "Epoch: 22. Loss: 0.0035018635034095775. ACC 0.9920429679729461 \n",
            "Epoch: 23. Loss: 0.0029094781570276375. ACC 0.9956236323851203 \n",
            "Epoch: 24. Loss: 0.002641459213597689. ACC 0.9962204097871494 \n",
            "Epoch: 25. Loss: 0.0024512605310765267. ACC 0.9964193355878257 \n",
            "Epoch: 26. Loss: 0.0023627816014228646. ACC 0.9964193355878257 \n",
            "Epoch: 27. Loss: 0.0021356647074546914. ACC 0.9974139645912075 \n",
            "Epoch: 28. Loss: 0.002047352865056956. ACC 0.9972150387905311 \n",
            "Epoch: 29. Loss: 0.0019240408705525267. ACC 0.9980107419932365 \n",
            "Epoch: 30. Loss: 0.0018321433042780423. ACC 0.9972150387905311 \n",
            "Epoch: 31. Loss: 0.0017038570430720943. ACC 0.9978118161925602 \n",
            "Epoch: 32. Loss: 0.0016746390802681635. ACC 0.9976128903918838 \n",
            "Epoch: 33. Loss: 0.001557909073615996. ACC 0.9982096677939128 \n",
            "Epoch: 34. Loss: 0.0014904898325691148. ACC 0.9978118161925602 \n",
            "Epoch: 35. Loss: 0.0014379579584245942. ACC 0.9982096677939128 \n",
            "Epoch: 36. Loss: 0.001405126767873669. ACC 0.9982096677939128 \n",
            "Epoch: 37. Loss: 0.0013628302004728876. ACC 0.9980107419932365 \n",
            "Epoch: 38. Loss: 0.0013023524946059076. ACC 0.9986075193952656 \n",
            "Epoch: 39. Loss: 0.0012724651037900872. ACC 0.9982096677939128 \n",
            "Epoch: 40. Loss: 0.0014603725609951873. ACC 0.9968171871891784 \n",
            "Epoch: 41. Loss: 0.0013616313778734646. ACC 0.9978118161925602 \n",
            "Epoch: 42. Loss: 0.0011740706784325592. ACC 0.9980107419932365 \n",
            "Epoch: 43. Loss: 0.0011599577394986584. ACC 0.9982096677939128 \n",
            "Epoch: 44. Loss: 0.00112753107812259. ACC 0.9982096677939128 \n",
            "Epoch: 45. Loss: 0.001108333953563876. ACC 0.9980107419932365 \n",
            "Epoch: 46. Loss: 0.001062253781580256. ACC 0.9984085935945892 \n",
            "Epoch: 47. Loss: 0.0010420190941353405. ACC 0.9982096677939128 \n",
            "Epoch: 48. Loss: 0.0010299957002729112. ACC 0.9982096677939128 \n",
            "Epoch: 49. Loss: 0.0009990891739271707. ACC 0.9984085935945892 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.87      0.82       230\n",
            "           1       0.89      0.82      0.86       319\n",
            "\n",
            "    accuracy                           0.84       549\n",
            "   macro avg       0.84      0.84      0.84       549\n",
            "weighted avg       0.85      0.84      0.84       549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ANSWER\n",
        "\n",
        "Not much change after 20 iterations on train: convergence."
      ],
      "metadata": {
        "id": "7aMltDuQ55Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "max_epochs = 10\n",
        "\n",
        "# Load data\n",
        "train_loader, dev_loader, test_loader = load_data( x_train, y_train, x_dev, y_dev, x_test, y_test, batch_size=batch_size )\n",
        "\n",
        "# Hyper-parameters\n",
        "hidden_dim = 4\n",
        "output_dim = 2\n",
        "learning_rate = 0.1\n",
        "#num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.SGD( model_bow.parameters(), lr=learning_rate )\n",
        "\n",
        "# Train and evaluate\n",
        "train( model_bow, train_loader, optimizer, num_epochs=max_epochs )\n",
        "gold, pred = evaluate( model_bow, dev_loader )\n",
        "accuracy = accuracy_score( gold, pred )\n",
        "\n",
        "write_expe_settings( my_expe, batch=batch_size, hidden=1, hsize=4, act='sigmoid', lr=0.1, opt='sgd', epochs=max_epochs, score=accuracy )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlIqww5855hv",
        "outputId": "7ede4573-7445-4568-8bb2-a27e3b9e881f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0. Loss: 0.06291212701456006. ACC 0.6528744778197733 \n",
            "Epoch: 1. Loss: 0.044857184989376006. ACC 0.8082355281480008 \n",
            "Epoch: 2. Loss: 0.03335049673944552. ACC 0.8677143425502287 \n",
            "Epoch: 3. Loss: 0.02790208174216242. ACC 0.8897951064253034 \n",
            "Epoch: 4. Loss: 0.023848686240182167. ACC 0.9076984284861747 \n",
            "Epoch: 5. Loss: 0.020894395210284623. ACC 0.92102645713149 \n",
            "Epoch: 6. Loss: 0.018487477743628012. ACC 0.9313705987666601 \n",
            "Epoch: 7. Loss: 0.016292144128643694. ACC 0.9409190371991247 \n",
            "Epoch: 8. Loss: 0.014915780355308671. ACC 0.9413168888004774 \n",
            "Epoch: 9. Loss: 0.013037947227960511. ACC 0.9508653272329421 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.74      0.81       230\n",
            "           1       0.83      0.93      0.88       319\n",
            "\n",
            "    accuracy                           0.85       549\n",
            "   macro avg       0.86      0.84      0.84       549\n",
            "weighted avg       0.85      0.85      0.85       549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 3.3 Size of the hidden layer\n",
        "---\n",
        "\n",
        "Let's try with: 4, 16, 128, 5000"
      ],
      "metadata": {
        "id": "e-poMh3ndo2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TEST #6\n",
        "\n",
        "BoW, 5000 features, Batch size: 10, 1 hidden layer, **hidden size: 32**, activation: sigmoid, learning rate: 0.1, optimizer: SGD, max epochs: 20"
      ],
      "metadata": {
        "id": "UR5ECuXTd4zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "max_epochs = 5\n",
        "hidden_dim = 64\n",
        "\n",
        "# Load data\n",
        "train_loader, dev_loader, test_loader = load_data( x_train, y_train, x_dev, y_dev, x_test, y_test, batch_size=batch_size )\n",
        "\n",
        "# Hyper-parameters\n",
        "##hidden_dim = 4\n",
        "output_dim = 2\n",
        "learning_rate = 0.1\n",
        "##num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.SGD( model_bow.parameters(), lr=learning_rate )\n",
        "\n",
        "# Train and evaluate\n",
        "train( model_bow, train_loader, optimizer, num_epochs=max_epochs )\n",
        "gold, pred = evaluate( model_bow, dev_loader )\n",
        "accuracy = accuracy_score( gold, pred )\n",
        "\n",
        "write_expe_settings( my_expe, batch=batch_size, hidden=1, hsize=hidden_dim, act='sigmoid', lr=0.1, opt='sgd', epochs=max_epochs, score=accuracy )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLXjvR6QVMa3",
        "outputId": "6171e24c-25e8-4b60-a87c-2ef60741da6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH SIZE: 10\n",
            "INPUT DIM: 5000 HIDDEN DIM: 64\n",
            "Epoch: 0. Loss: 0.06269910253965288. ACC 0.635567933160931 \n",
            "Epoch: 1. Loss: 0.04317359960806021. ACC 0.8040580863337975 \n",
            "Epoch: 2. Loss: 0.03290526890804499. ACC 0.8603540879252038 \n",
            "Epoch: 3. Loss: 0.02682727060934908. ACC 0.8925800676347723 \n",
            "Epoch: 4. Loss: 0.023164639555685547. ACC 0.9067037994827929 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.84      0.81       230\n",
            "           1       0.88      0.84      0.86       319\n",
            "\n",
            "    accuracy                           0.84       549\n",
            "   macro avg       0.83      0.84      0.83       549\n",
            "weighted avg       0.84      0.84      0.84       549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TEST #6\n",
        "\n",
        "BoW, 5000 features, Batch size: XX, 1 hidden layer, **hidden size: 64**, activation: sigmoid, learning rate: 0.1, optimizer: SGD, max epochs: XX"
      ],
      "metadata": {
        "id": "Zv7LnlB-ez9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "max_epochs = 5\n",
        "hidden_dim = 128\n",
        "\n",
        "# Load data\n",
        "train_loader, dev_loader, test_loader = load_data( x_train, y_train, x_dev, y_dev, x_test, y_test, batch_size=batch_size )\n",
        "\n",
        "# Hyper-parameters\n",
        "##hidden_dim = 4\n",
        "output_dim = 2\n",
        "learning_rate = 0.1\n",
        "##num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.SGD( model_bow.parameters(), lr=learning_rate )\n",
        "\n",
        "# Train and evaluate\n",
        "train( model_bow, train_loader, optimizer, num_epochs=max_epochs )\n",
        "gold, pred = evaluate( model_bow, dev_loader )\n",
        "accuracy = accuracy_score( gold, pred )\n",
        "\n",
        "write_expe_settings( my_expe, batch=batch_size, hidden=1, hsize=hidden_dim, act='sigmoid', lr=0.1, opt='sgd', epochs=max_epochs, score=accuracy )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_dYFPV0VMdJ",
        "outputId": "591775ca-0b97-405e-de25-361d7fb7cde5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH SIZE: 10\n",
            "INPUT DIM: 5000 HIDDEN DIM: 128\n",
            "Epoch: 0. Loss: 0.06637925877114853. ACC 0.612691466083151 \n",
            "Epoch: 1. Loss: 0.04425935451514161. ACC 0.7970956833101254 \n",
            "Epoch: 2. Loss: 0.03306083544356604. ACC 0.8677143425502287 \n",
            "Epoch: 3. Loss: 0.028170021697330457. ACC 0.884424109807042 \n",
            "Epoch: 4. Loss: 0.023333700381086737. ACC 0.9031231350706187 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.93      0.80       230\n",
            "           1       0.93      0.71      0.81       319\n",
            "\n",
            "    accuracy                           0.80       549\n",
            "   macro avg       0.82      0.82      0.80       549\n",
            "weighted avg       0.84      0.80      0.80       549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TEST #7\n",
        "\n",
        "BoW, 5000 features, Batch size: XX, 1 hidden layer, **hidden size: 5000**, activation: sigmoid, learning rate: 0.1, optimizer: SGD, max epochs: XX"
      ],
      "metadata": {
        "id": "gQD64xGAfSlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "max_epochs = 5\n",
        "hidden_dim = 5000\n",
        "\n",
        "# Load data\n",
        "train_loader, dev_loader, test_loader = load_data( x_train, y_train, x_dev, y_dev, x_test, y_test, batch_size=batch_size )\n",
        "\n",
        "# Hyper-parameters\n",
        "##hidden_dim = 4\n",
        "output_dim = 2\n",
        "learning_rate = 0.1\n",
        "##num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.SGD( model_bow.parameters(), lr=learning_rate )\n",
        "\n",
        "# Train and evaluate\n",
        "train( model_bow, train_loader, optimizer, num_epochs=max_epochs )\n",
        "gold, pred = evaluate( model_bow, dev_loader )\n",
        "accuracy = accuracy_score( gold, pred )\n",
        "\n",
        "write_expe_settings( my_expe, batch=batch_size, hidden=1, hsize=hidden_dim, act='sigmoid', lr=0.1, opt='sgd', epochs=max_epochs, score=accuracy )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ83eM0eVMf7",
        "outputId": "c682764c-d570-423b-a24f-7bd91134096b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0. Loss: 0.23846807259557165. ACC 0.5100457529341555 \n",
            "Epoch: 1. Loss: 0.15589830538423582. ACC 0.5187984881639148 \n",
            "Epoch: 2. Loss: 0.13582247489722793. ACC 0.4941316888800477 \n",
            "Epoch: 3. Loss: 0.10262338762281428. ACC 0.502287646707778 \n",
            "Epoch: 4. Loss: 0.09239863767191361. ACC 0.4955241694847822 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      1.00      0.59       230\n",
            "           1       0.00      0.00      0.00       319\n",
            "\n",
            "    accuracy                           0.42       549\n",
            "   macro avg       0.21      0.50      0.30       549\n",
            "weighted avg       0.18      0.42      0.25       549\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer: Hidden layer size\n",
        "\n",
        "Increase the performance in terms of macro F1, maybe? Longer training time.\n",
        "\n",
        "No magic number, needs to be optimized. In general, a few typical values are tested:  \n",
        "* *Using too few neurons in the hidden layers will result in something called underfitting. Underfitting occurs when there are too few neurons in the hidden layers to adequately detect the signals in a complicated data set.* \n",
        "* *too many neurons in the hidden layers may result in overfitting. Overfitting occurs when the neural network has so much information processing capacity that the limited amount of information contained in the training set is not enough to train all of the neurons in the hidden layers. A second problem [is increasing] the time it takes to train the network.*\n",
        "* \"Rules\"\n",
        "  * *The number of hidden neurons should be between the size of the input layer and the size of the output layer.* \n",
        "  * *The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.* \n",
        "  * *The number of hidden neurons should be less than twice the size of the input layer.*"
      ],
      "metadata": {
        "id": "hhcgD3ajsL4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 3.4 Activation function\n",
        "---\n",
        "Try with Sigmoid, Tanh, ReLU\n",
        "\n",
        "https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity"
      ],
      "metadata": {
        "id": "gpSfhvM8hMht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## WITH SIGMOID \n",
        "batch_size = 200\n",
        "max_epochs = 10\n",
        "hidden_dim = 16\n",
        "\n",
        "# Load data\n",
        "train_loader, dev_loader, test_loader = load_data( x_train, y_train, x_dev, y_dev, x_test, y_test, batch_size=batch_size )\n",
        "\n",
        "# Hyper-parameters\n",
        "##hidden_dim = 4\n",
        "output_dim = 2\n",
        "learning_rate = 0.1\n",
        "##num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.SGD( model_bow.parameters(), lr=learning_rate )\n",
        "\n",
        "# Train and evaluate\n",
        "train( model_bow, train_loader, optimizer, num_epochs=max_epochs )\n",
        "gold, pred = evaluate( model_bow, dev_loader )\n",
        "accuracy = accuracy_score( gold, pred )\n",
        "\n",
        "write_expe_settings( my_expe, batch=batch_size, hidden=1, hsize=hidden_dim, act='tanh', lr=0.1, opt='sgd', epochs=max_epochs, score=accuracy )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBPtJrSriJt5",
        "outputId": "20a92831-39e3-4c15-e3c6-4ff097412f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH SIZE: 200\n",
            "INPUT DIM: 5000 HIDDEN DIM: 16\n",
            "Epoch: 0. Loss: 0.0035867044483190126. ACC 0.5174060075591804 \n",
            "Epoch: 1. Loss: 0.003538248943054157. ACC 0.585239705589815 \n",
            "Epoch: 2. Loss: 0.003510344151784678. ACC 0.6097075790730058 \n",
            "Epoch: 3. Loss: 0.0034701669692803454. ACC 0.6421324845832505 \n",
            "Epoch: 4. Loss: 0.0034234431407929224. ACC 0.6624229162522379 \n",
            "Epoch: 5. Loss: 0.003360011870708811. ACC 0.6954445991645116 \n",
            "Epoch: 6. Loss: 0.0032856720257924714. ACC 0.6980306345733042 \n",
            "Epoch: 7. Loss: 0.0032111859141369333. ACC 0.7256813208673165 \n",
            "Epoch: 8. Loss: 0.0031274266619556153. ACC 0.7332405012930177 \n",
            "Epoch: 9. Loss: 0.0030350037258520786. ACC 0.7475631589417148 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.56      0.63       230\n",
            "           1       0.73      0.85      0.78       319\n",
            "\n",
            "    accuracy                           0.73       549\n",
            "   macro avg       0.73      0.70      0.71       549\n",
            "weighted avg       0.73      0.73      0.72       549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TEST #7\n",
        "\n",
        "BoW, 5000 features, Batch size: 10, 1 hidden layer, hidden size: 16, activation: tanh, learning rate: 0.1, optimizer: SGD, max epochs: 10"
      ],
      "metadata": {
        "id": "nEdsPL-YhMti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedforwardNeuralNetModel2(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
        "        # just always do it in an nn.Module\n",
        "        super(FeedforwardNeuralNetModel2, self).__init__()\n",
        "\n",
        "        # Define the parameters that you will need. \n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Non-linearity -- to rewrite\n",
        "        self.sigmoid = nn.Tanh()\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Linear function  # LINEAR\n",
        "        out = self.fc1(x)\n",
        "\n",
        "        # Non-linearity  # NON-LINEAR\n",
        "        out = self.sigmoid(out) \n",
        "\n",
        "        # Linear function (readout)  # LINEAR\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "pYDejSy-hNFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 200\n",
        "max_epochs = 10\n",
        "hidden_dim = 16\n",
        "\n",
        "# Load data\n",
        "train_loader, dev_loader, test_loader = load_data( x_train, y_train, x_dev, y_dev, x_test, y_test, batch_size=batch_size )\n",
        "\n",
        "# Hyper-parameters\n",
        "##hidden_dim = 4\n",
        "output_dim = 2\n",
        "learning_rate = 0.1\n",
        "##num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow = FeedforwardNeuralNetModel2(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.SGD( model_bow.parameters(), lr=learning_rate )\n",
        "\n",
        "# Train and evaluate\n",
        "train( model_bow, train_loader, optimizer, num_epochs=max_epochs )\n",
        "gold, pred = evaluate( model_bow, dev_loader )\n",
        "accuracy = accuracy_score( gold, pred )\n",
        "\n",
        "write_expe_settings( my_expe, batch=batch_size, hidden=1, hsize=hidden_dim, act='tanh', lr=0.1, opt='sgd', epochs=max_epochs, score=accuracy )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voWW7OOjhNGm",
        "outputId": "b3767c0e-8b8d-432d-c103-e2f160b662e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH SIZE: 200\n",
            "Epoch: 0. Loss: 0.0033632819330523924. ACC 0.6409389297791923 \n",
            "Epoch: 1. Loss: 0.003020698030117802. ACC 0.7280684304754327 \n",
            "Epoch: 2. Loss: 0.002754686407334339. ACC 0.7660632584046151 \n",
            "Epoch: 3. Loss: 0.002551118700789343. ACC 0.7907300576884821 \n",
            "Epoch: 4. Loss: 0.0023040566241406815. ACC 0.8136065247662622 \n",
            "Epoch: 5. Loss: 0.0022498833355335444. ACC 0.8110204893574696 \n",
            "Epoch: 6. Loss: 0.0020608256072819717. ACC 0.843843246469067 \n",
            "Epoch: 7. Loss: 0.0019477623308632136. ACC 0.8444400238710961 \n",
            "Epoch: 8. Loss: 0.001923756575001052. ACC 0.8408593594589219 \n",
            "Epoch: 9. Loss: 0.0017911832411340912. ACC 0.8539884623035607 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.58      0.70       230\n",
            "           1       0.76      0.95      0.84       319\n",
            "\n",
            "    accuracy                           0.80       549\n",
            "   macro avg       0.83      0.77      0.77       549\n",
            "weighted avg       0.82      0.80      0.79       549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TEST #8\n",
        "\n",
        "BoW, 5000 features, Batch size: 10, 1 hidden layer, hidden size: 16, activation: relu, learning rate: 0.1, optimizer: SGD, max epochs: 10"
      ],
      "metadata": {
        "id": "Z_LDmwqmhNgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedforwardNeuralNetModel3(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
        "        # just always do it in an nn.Module\n",
        "        super(FeedforwardNeuralNetModel3, self).__init__()\n",
        "\n",
        "        # Define the parameters that you will need. \n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Non-linearity\n",
        "        self.sigmoid = nn.ReLU()\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Linear function  # LINEAR\n",
        "        out = self.fc1(x)\n",
        "\n",
        "        # Non-linearity  # NON-LINEAR\n",
        "        out = self.sigmoid(out) \n",
        "\n",
        "        # Linear function (readout)  # LINEAR\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "S1aN5-v7iULZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 200\n",
        "max_epochs = 10\n",
        "hidden_dim = 16\n",
        "\n",
        "# Load data\n",
        "train_loader, dev_loader, test_loader = load_data( x_train, y_train, x_dev, y_dev, x_test, y_test, batch_size=batch_size )\n",
        "\n",
        "# Hyper-parameters\n",
        "##hidden_dim = 4\n",
        "output_dim = 2\n",
        "learning_rate = 0.1\n",
        "##num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow = FeedforwardNeuralNetModel3(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.SGD( model_bow.parameters(), lr=learning_rate )\n",
        "\n",
        "# Train and evaluate\n",
        "train( model_bow, train_loader, optimizer, num_epochs=max_epochs )\n",
        "gold, pred = evaluate( model_bow, dev_loader )\n",
        "accuracy = accuracy_score( gold, pred )\n",
        "\n",
        "write_expe_settings( my_expe, batch=batch_size, hidden=1, hsize=hidden_dim, act='relu', lr=0.1, opt='sgd', epochs=max_epochs, score=accuracy )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANoBMrp9inyK",
        "outputId": "525939e4-0bc8-44ca-e0d6-2df6547db449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH SIZE: 200\n",
            "Epoch: 0. Loss: 0.0035081197851714874. ACC 0.5742987865526159 \n",
            "Epoch: 1. Loss: 0.003222808499731789. ACC 0.6904714541476029 \n",
            "Epoch: 2. Loss: 0.0029210167315790233. ACC 0.7390093495126318 \n",
            "Epoch: 3. Loss: 0.002751473495256884. ACC 0.755719116769445 \n",
            "Epoch: 4. Loss: 0.0027018428168952262. ACC 0.7732245872289636 \n",
            "Epoch: 5. Loss: 0.002733380817764674. ACC 0.7692460712154366 \n",
            "Epoch: 6. Loss: 0.002285557752910513. ACC 0.8207678535906107 \n",
            "Epoch: 7. Loss: 0.0023491217906512537. ACC 0.8132086731649095 \n",
            "Epoch: 8. Loss: 0.002281326107697101. ACC 0.8122140441615278 \n",
            "Epoch: 9. Loss: 0.0021961465469050746. ACC 0.8360851402426894 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.71      0.77       230\n",
            "           1       0.81      0.90      0.85       319\n",
            "\n",
            "    accuracy                           0.82       549\n",
            "   macro avg       0.82      0.80      0.81       549\n",
            "weighted avg       0.82      0.82      0.82       549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 3.5 Optimizer\n",
        "---\n",
        "Try SGD, Adam\n",
        "\n",
        "https://pytorch.org/docs/stable/optim.html#module-torch.optim\n",
        "\n",
        "For a detailed description and comparison of the different optimizers:\n",
        "https://ruder.io/optimizing-gradient-descent/"
      ],
      "metadata": {
        "id": "jqzTB4bfhNqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### WITH SGD  (and ReLU)\n",
        "\n",
        "batch_size = 10\n",
        "max_epochs = 10\n",
        "hidden_dim = 16\n",
        "\n",
        "# Load data\n",
        "train_loader, dev_loader, test_loader = load_data( x_train, y_train, x_dev, y_dev, x_test, y_test, batch_size=batch_size )\n",
        "\n",
        "# Hyper-parameters\n",
        "##hidden_dim = 4\n",
        "output_dim = 2\n",
        "learning_rate = 0.1\n",
        "##num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow = FeedforwardNeuralNetModel3(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.SGD( model_bow.parameters(), lr=learning_rate )\n",
        "\n",
        "# Train and evaluate\n",
        "train( model_bow, train_loader, optimizer, num_epochs=max_epochs )\n",
        "gold, pred = evaluate( model_bow, dev_loader )\n",
        "accuracy = accuracy_score( gold, pred )\n",
        "\n",
        "write_expe_settings( my_expe, batch=batch_size, hidden=1, hsize=hidden_dim, act='relu', lr=0.1, opt='sgd', epochs=max_epochs, score=accuracy )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QQN_REhiy07",
        "outputId": "bf982f0a-b39d-423b-88dc-e545171b9d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0. Loss: 0.05626993505563939. ACC 0.7242888402625821 \n",
            "Epoch: 1. Loss: 0.03875441925931053. ACC 0.8303162920230754 \n",
            "Epoch: 2. Loss: 0.0302670874747944. ACC 0.8754724487766063 \n",
            "Epoch: 3. Loss: 0.02549069301249885. ACC 0.8947682514422121 \n",
            "Epoch: 4. Loss: 0.020774694624361927. ACC 0.9190371991247265 \n",
            "Epoch: 5. Loss: 0.018724975738246812. ACC 0.9289834891585439 \n",
            "Epoch: 6. Loss: 0.01547806462362299. ACC 0.9466878854187388 \n",
            "Epoch: 7. Loss: 0.009750108654659074. ACC 0.9647901332802864 \n",
            "Epoch: 8. Loss: 0.010688928858555264. ACC 0.9651879848816391 \n",
            "Epoch: 9. Loss: 0.014519969835326583. ACC 0.9600159140640541 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.81      0.81       230\n",
            "           1       0.87      0.87      0.87       319\n",
            "\n",
            "    accuracy                           0.85       549\n",
            "   macro avg       0.84      0.84      0.84       549\n",
            "weighted avg       0.85      0.85      0.85       549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TEST #9\n",
        "\n",
        "BoW, 5000 features, Batch size: 10, 1 hidden layer, hidden size: 16, activation: relu, learning rate: 0.1, optimizer: Adam, max epochs: 10"
      ],
      "metadata": {
        "id": "5fO71_wbiylY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "max_epochs = 10\n",
        "hidden_dim = 16\n",
        "\n",
        "# Load data\n",
        "train_loader, dev_loader, test_loader = load_data( x_train, y_train, x_dev, y_dev, x_test, y_test, batch_size=batch_size )\n",
        "\n",
        "# Hyper-parameters\n",
        "##hidden_dim = 4\n",
        "output_dim = 2\n",
        "learning_rate = 0.1\n",
        "##num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow = FeedforwardNeuralNetModel3(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.Adam( model_bow.parameters(), lr=learning_rate ) ## <---\n",
        "\n",
        "# Train and evaluate\n",
        "train( model_bow, train_loader, optimizer, num_epochs=max_epochs )\n",
        "gold, pred = evaluate( model_bow, dev_loader )\n",
        "accuracy = accuracy_score( gold, pred )\n",
        "\n",
        "write_expe_settings( my_expe, batch=batch_size, hidden=1, hsize=hidden_dim, act='relu', lr=0.1, opt='adam', epochs=max_epochs, score=accuracy )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlRWafZviy7S",
        "outputId": "7e187425-e431-4158-a7b4-56b856ef0398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH SIZE: 10\n",
            "Epoch: 0. Loss: 0.07213442693630269. ACC 0.51143823353889 \n",
            "Epoch: 1. Loss: 0.07198922398053843. ACC 0.5390889198329023 \n",
            "Epoch: 2. Loss: 0.06635210800175641. ACC 0.5595782773025662 \n",
            "Epoch: 3. Loss: 0.06072889886439285. ACC 0.594987069822956 \n",
            "Epoch: 4. Loss: 0.061436555608262215. ACC 0.584046150785757 \n",
            "Epoch: 5. Loss: 0.061342544040007406. ACC 0.6210463497115576 \n",
            "Epoch: 6. Loss: 0.05394889997301299. ACC 0.6377561169683708 \n",
            "Epoch: 7. Loss: 0.05296870667108703. ACC 0.6628207678535906 \n",
            "Epoch: 8. Loss: 0.05464682751914154. ACC 0.6574497712353292 \n",
            "Epoch: 9. Loss: 0.051503366374794. ACC 0.6520787746170679 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.23      0.37       230\n",
            "           1       0.64      1.00      0.78       319\n",
            "\n",
            "    accuracy                           0.67       549\n",
            "   macro avg       0.81      0.61      0.57       549\n",
            "weighted avg       0.78      0.67      0.61       549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### More about optimizers\n",
        "\n",
        "Stochastic gradient descent maintains a single learning rate for all weight updates and the learning rate does not change during training.\n",
        "\n",
        "The ADAM algorithms leverages the power of adaptive learning rates methods to find individual learning rates for each parameter.\n",
        "\n",
        "Adaptive Gradient Algorithm (AdaGrad): maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems).\n",
        "\n",
        "Root Mean Square Propagation (RMSProp): also maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing). This means the algorithm does well on online and non-stationary problems (e.g. noisy).\n",
        "Adam realizes the benefits of both AdaGrad and RMSProp.\n",
        "\n",
        "from: https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/ and https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c"
      ],
      "metadata": {
        "id": "U2i-wnLDJWLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 3.6 Learning rate\n",
        "---"
      ],
      "metadata": {
        "id": "SJxu3eDBjg7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TEST #10\n",
        "\n",
        "BoW, 5000 features, Batch size: 10, 1 hidden layer, hidden size: 16, activation: relu, **learning rate: 0.5**, optimizer: Adam, max epochs: 10"
      ],
      "metadata": {
        "id": "43HJoUYIjo5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "max_epochs = 10\n",
        "hidden_dim = 16\n",
        "learning_rate = 0.5\n",
        "\n",
        "# Load data\n",
        "train_loader, dev_loader, test_loader = load_data( x_train, y_train, x_dev, y_dev, x_test, y_test, batch_size=batch_size )\n",
        "\n",
        "# Hyper-parameters\n",
        "##hidden_dim = 4\n",
        "output_dim = 2\n",
        "#learning_rate = 0.1\n",
        "##num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow = FeedforwardNeuralNetModel3(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.Adam( model_bow.parameters(), lr=learning_rate )\n",
        "\n",
        "# Train and evaluate\n",
        "train( model_bow, train_loader, optimizer, num_epochs=max_epochs )\n",
        "gold, pred = evaluate( model_bow, dev_loader )\n",
        "accuracy = accuracy_score( gold, pred )\n",
        "\n",
        "write_expe_settings( my_expe, batch=batch_size, hidden=1, hsize=hidden_dim, act='relu', lr=learning_rate, opt='adam', epochs=max_epochs, score=accuracy )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ9dGVq-iy_q",
        "outputId": "d880e089-a8bd-4b7d-d731-608f438c8aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0. Loss: 0.11304628426330995. ACC 0.5046747563158942 \n",
            "Epoch: 1. Loss: 0.08354000300495056. ACC 0.5040779789138651 \n",
            "Epoch: 2. Loss: 0.07644790185714351. ACC 0.5016908693057489 \n",
            "Epoch: 3. Loss: 0.0714663850258045. ACC 0.5203898945693256 \n",
            "Epoch: 4. Loss: 0.07242015330815661. ACC 0.516610304356475 \n",
            "Epoch: 5. Loss: 0.07200485677777926. ACC 0.5122339367415953 \n",
            "Epoch: 6. Loss: 0.07188668697094239. ACC 0.5020887209071017 \n",
            "Epoch: 7. Loss: 0.07396689143312696. ACC 0.4981102048935747 \n",
            "Epoch: 8. Loss: 0.07325427189413017. ACC 0.49492739208275316 \n",
            "Epoch: 9. Loss: 0.07357010994679368. ACC 0.49074995026854984 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      1.00      0.59       230\n",
            "           1       0.00      0.00      0.00       319\n",
            "\n",
            "    accuracy                           0.42       549\n",
            "   macro avg       0.21      0.50      0.30       549\n",
            "weighted avg       0.18      0.42      0.25       549\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TEST #11\n",
        "\n",
        "BoW, 5000 features, Batch size: 10, 1 hidden layer, hidden size: 16, activation: relu, **learning rate: 0.001**, optimizer: Adam, max epochs: 10"
      ],
      "metadata": {
        "id": "XgwsJMXUj6IA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "max_epochs = 10\n",
        "hidden_dim = 16\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load data\n",
        "train_loader, dev_loader, test_loader = load_data( x_train, y_train, x_dev, y_dev, x_test, y_test, batch_size=batch_size )\n",
        "\n",
        "# Hyper-parameters\n",
        "##hidden_dim = 4\n",
        "output_dim = 2\n",
        "#learning_rate = 0.1\n",
        "##num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow = FeedforwardNeuralNetModel3(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.Adam( model_bow.parameters(), lr=learning_rate )\n",
        "\n",
        "# Train and evaluate\n",
        "train( model_bow, train_loader, optimizer, num_epochs=max_epochs )\n",
        "gold, pred = evaluate( model_bow, dev_loader )\n",
        "accuracy = accuracy_score( gold, pred )\n",
        "\n",
        "write_expe_settings( my_expe, batch=batch_size, hidden=1, hsize=hidden_dim, act='relu', lr=learning_rate, opt='adam', epochs=max_epochs, score=accuracy )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFELWLJXkBbx",
        "outputId": "17fcde48-a8e7-42eb-8866-340a69045c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0. Loss: 0.04085205458378801. ACC 0.8344937338372787 \n",
            "Epoch: 1. Loss: 0.01722472314045696. ACC 0.9476825144221206 \n",
            "Epoch: 2. Loss: 0.010114406593955768. ACC 0.9677740202904317 \n",
            "Epoch: 3. Loss: 0.006535387269435412. ACC 0.9820966779391287 \n",
            "Epoch: 4. Loss: 0.004442139458302994. ACC 0.9898547841655063 \n",
            "Epoch: 5. Loss: 0.002959595974119527. ACC 0.9930375969763279 \n",
            "Epoch: 6. Loss: 0.0021469107372873615. ACC 0.9958225581857967 \n",
            "Epoch: 7. Loss: 0.0013920834114515715. ACC 0.9980107419932365 \n",
            "Epoch: 8. Loss: 0.0011005863557361237. ACC 0.9986075193952656 \n",
            "Epoch: 9. Loss: 0.0014661112178039036. ACC 0.9964193355878257 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.80      0.80       230\n",
            "           1       0.86      0.85      0.85       319\n",
            "\n",
            "    accuracy                           0.83       549\n",
            "   macro avg       0.83      0.83      0.83       549\n",
            "weighted avg       0.83      0.83      0.83       549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 3.7 Number of hidden layers\n",
        "---"
      ],
      "metadata": {
        "id": "IlyioqnpfeHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TEST #12\n",
        "\n",
        "BoW, 5000 features, Batch size: 10, 2 hidden layer, hidden size: 16, activation: sigmoid, learning rate: 0.01, optimizer: adam, max epochs: 10"
      ],
      "metadata": {
        "id": "ONYtNURqfjp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeedforwardNeuralNetModel2(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, hidden_dim2, output_dim):\n",
        "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
        "        # just always do it in an nn.Module\n",
        "        super(FeedforwardNeuralNetModel2, self).__init__()\n",
        "\n",
        "        # Define the parameters that you will need. \n",
        "        # -- LAYER 1\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear( input_dim, hidden_dim )\n",
        "        ## -- LAYER 2\n",
        "        self.fc3 = nn.Linear( hidden_dim, hidden_dim2 )\n",
        "\n",
        "        # Non-linearity\n",
        "        self.sigmoid = nn.ReLU()\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim2, output_dim)  \n",
        "\n",
        "    def forward(self, x):\n",
        "        # -- LAYER 1\n",
        "        # Linear function  # LINEAR\n",
        "        out = self.fc1(x)\n",
        "        # Non-linearity  # NON-LINEAR\n",
        "        out = self.sigmoid(out) \n",
        "\n",
        "        # -- LAYER 2\n",
        "        out = self.fc3(out)\n",
        "        out = self.sigmoid(out)\n",
        "\n",
        "        # Linear function (readout)  # LINEAR\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "rAqZLBbof2_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "max_epochs = 10\n",
        "hidden_dim = 16\n",
        "hidden_dim2 = 32\n",
        "\n",
        "# Load data\n",
        "train_loader, dev_loader, test_loader = load_data( x_train, y_train, x_dev, y_dev, x_test, y_test, batch_size=batch_size )\n",
        "\n",
        "# Hyper-parameters\n",
        "##hidden_dim = 4\n",
        "output_dim = 2\n",
        "learning_rate = 0.001\n",
        "##num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the model\n",
        "model_bow = FeedforwardNeuralNetModel2(input_dim, hidden_dim, hidden_dim2, output_dim)\n",
        "optimizer = torch.optim.Adam( model_bow.parameters(), lr=learning_rate )\n",
        "\n",
        "# Train and evaluate\n",
        "train( model_bow, train_loader, optimizer, num_epochs=max_epochs )\n",
        "gold, pred = evaluate( model_bow, dev_loader )\n",
        "accuracy = accuracy_score( gold, pred )\n",
        "\n",
        "write_expe_settings( my_expe, batch=batch_size, hidden=2, hsize=str(hidden_dim)+';'+str(hidden_dim2), act='relu', lr=0.1, opt='adam', epochs=max_epochs, score=accuracy )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSxX6ecaVMiL",
        "outputId": "ac84b86a-01c9-4a20-b962-f26d9b7929c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0. Loss: 0.04112395134785789. ACC 0.8191764471851999 \n",
            "Epoch: 1. Loss: 0.015822856690344087. ACC 0.9435050726079173 \n",
            "Epoch: 2. Loss: 0.009069929567643376. ACC 0.9681718718917843 \n",
            "Epoch: 3. Loss: 0.004802363096739822. ACC 0.9858762681519793 \n",
            "Epoch: 4. Loss: 0.002587109142221498. ACC 0.9914461905709171 \n",
            "Epoch: 5. Loss: 0.004202911742800717. ACC 0.9884623035607718 \n",
            "Epoch: 6. Loss: 0.0019092409496743834. ACC 0.9942311517803859 \n",
            "Epoch: 7. Loss: 0.0007101426964987467. ACC 0.9978118161925602 \n",
            "Epoch: 8. Loss: 0.0005763179586664687. ACC 0.9984085935945892 \n",
            "Epoch: 9. Loss: 0.0005077990493367739. ACC 0.9986075193952656 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.85      0.82       230\n",
            "           1       0.89      0.84      0.86       319\n",
            "\n",
            "    accuracy                           0.85       549\n",
            "   macro avg       0.84      0.85      0.84       549\n",
            "weighted avg       0.85      0.85      0.85       549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "X3wTELxBf1OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Kt_GiQA-f1QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gxCLOfAHf1Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PGkg9bFtf1Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fClo0juef1YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mcFn_6iKf1cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VTQF90MxVMj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3OwrZGFPVMn0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}